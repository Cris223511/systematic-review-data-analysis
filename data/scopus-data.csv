"Authors","Author full names","Author(s) ID","Title","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","Cited by","DOI","Link","Affiliations","Authors with affiliations","Abstract","Author Keywords","Index Keywords","Molecular Sequence Numbers","Chemicals/CAS","Funding Texts","References","Editors","Publisher","Sponsors","Conference name","Conference date","Conference location","Conference code","ISSN","ISBN","CODEN","PubMed ID","Language of Original Document","Document Type","Publication Stage","Open Access","Source","EID"
"Bae, S.; Hong, J.; Ha, S.; Moon, J.; Yu, J.; Choi, H.; Lee, J.; Do, R.; Sim, H.; Kim, H.; Lim, H.; Park, M.-H.; Ko, E.; Yang, C.-M.; Lee, D.; Yoo, H.; Lee, Y.; Bong, G.; Kim, J.I.; Sung, H.; Kim, H.-W.; Jung, E.; Chung, S.; Son, J.-W.; Yoo, J.H.; Jeon, S.; Kim, H.; Kim, B.-N.; Cheon, K.-A.","Bae, Soo-kyung (58091045900); Hong, Junho (60058629600); Ha, Sungji (57216947678); Moon, Jiwoo (60058179500); Yu, Jaeeun (57203308915); Choi, Hangnyoung (58571005700); Lee, Junghan (56957279400); Do, Ryemi (57211475464); Sim, Hewoen (59713269600); Kim, Hanna (60058336100); Lim, Hyojeong (60058629700); Park, Min-hyeon (25723693600); Ko, Eunseol (60058479000); Yang, Chanmo (57218423289); Lee, Dongho (59943122700); Yoo, Heejeong (8863687400); Lee, Yoojeong (57573060000); Bong, Guiyoung (55990225500); Kim, Johanna-inhyang (56177337000); Sung, Haneul (60058479100); Kim, Hyo-won (13402798900); Jung, Eunji (58669437800); Chung, Seungwon (57218775913); Son, Jeong-woo (24477952200); Yoo, Jaehyun (56089180700); Jeon, Sekye (57222727129); Kim, Hwiyoung (57203411277); Kim, Bung-nyun (59181463500); Cheon, Keun-ah (7003802865)","58091045900; 60058629600; 57216947678; 60058179500; 57203308915; 58571005700; 56957279400; 57211475464; 59713269600; 60058336100; 60058629700; 25723693600; 60058479000; 57218423289; 59943122700; 8863687400; 57573060000; 55990225500; 56177337000; 60058479100; 13402798900; 58669437800; 57218775913; 24477952200; 56089180700; 57222727129; 57203411277; 59181463500; 7003802865","Multimodal AI for risk stratification in autism spectrum disorder: integrating voice and screening tools","2025","npj Digital Medicine","8","1","538","","","0","0","10.1038/s41746-025-01914-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013848588&doi=10.1038%2Fs41746-025-01914-6&partnerID=40&md5=301b589b5205d15394c2b467a1d30bdf","Department of Integrative Medicine, Yonsei University College of Medicine, Seoul, South Korea; Department of Artificial Intelligence, Yonsei University, Seoul, South Korea; Department of Psychiatry, Yonsei University College of Medicine, Seoul, South Korea; Division of Mechanical and Biomedical Engineering, Ewha Womans University, Seoul, South Korea; Department of Child and Adolescent Psychiatry, Severance Hospital, Seoul, South Korea; Biomedical Research Institute, Seoul, South Korea; Department of Psychiatry, The Catholic University of Korea Eunpyeong St. Mary’s Hospital, Seoul, South Korea; Department of Psychiatry, The Catholic University of Korea Eunpyeong St. Mary’s Hospital, Seoul, South Korea; Department of Psychiatry, Wonkwang Medical Center, Iksan, South Korea; Department of Psychiatry, Wonkwang University (WKU), School of Medicine, Iksan, South Korea; Department of Psychiatry, Seoul National University Bundang Hospital, Seongnam, South Korea; Department of Psychiatry, Seoul National University College of Medicine, Seoul, South Korea; Department of Psychiatry, Hanyang University College of Medicine, Seoul, South Korea; Institute of Mental Health, Hanyang University, Seoul, South Korea; Department of Psychiatry, Asan Medical Center, Seoul, South Korea; Yonsei Clinic of Psychiatry, Seoul, South Korea; Department of Psychiatry, Chungbuk National University Hospital, Cheongju-si, South Korea; Department of Psychiatry, The Catholic University of Korea Seoul St. Mary's Hospital, Seoul, South Korea; Department of Neurosurgery, Yonsei University College of Medicine, Seoul, South Korea; Department of Psychiatry, Seoul National University Hospital, Seoul, South Korea","Bae, Soo-kyung, Department of Integrative Medicine, Yonsei University College of Medicine, Seoul, South Korea; Hong, Junho, Department of Artificial Intelligence, Yonsei University, Seoul, South Korea; Ha, Sungji, Department of Psychiatry, Yonsei University College of Medicine, Seoul, South Korea; Moon, Jiwoo, Division of Mechanical and Biomedical Engineering, Ewha Womans University, Seoul, South Korea; Yu, Jaeeun, Department of Psychiatry, Yonsei University College of Medicine, Seoul, South Korea; Choi, Hangnyoung, Department of Child and Adolescent Psychiatry, Severance Hospital, Seoul, South Korea; Lee, Junghan, Department of Child and Adolescent Psychiatry, Severance Hospital, Seoul, South Korea; Do, Ryemi, Biomedical Research Institute, Seoul, South Korea; Sim, Hewoen, Biomedical Research Institute, Seoul, South Korea; Kim, Hanna, Biomedical Research Institute, Seoul, South Korea; Lim, Hyojeong, Biomedical Research Institute, Seoul, South Korea; Park, Min-hyeon, Department of Psychiatry, The Catholic University of Korea Eunpyeong St. Mary’s Hospital, Seoul, South Korea; Ko, Eunseol, Department of Psychiatry, The Catholic University of Korea Eunpyeong St. Mary’s Hospital, Seoul, South Korea; Yang, Chanmo, Department of Psychiatry, Wonkwang Medical Center, Iksan, South Korea, Department of Psychiatry, Wonkwang University (WKU), School of Medicine, Iksan, South Korea; Lee, Dongho, Department of Psychiatry, Wonkwang Medical Center, Iksan, South Korea; Yoo, Heejeong, Department of Psychiatry, Seoul National University Bundang Hospital, Seongnam, South Korea, Department of Psychiatry, Seoul National University College of Medicine, Seoul, South Korea; Lee, Yoojeong, Department of Psychiatry, Seoul National University Bundang Hospital, Seongnam, South Korea; Bong, Guiyoung, Department of Psychiatry, Seoul National University Bundang Hospital, Seongnam, South Korea; Kim, Johanna-inhyang, Department of Psychiatry, Hanyang University College of Medicine, Seoul, South Korea; Sung, Haneul, Institute of Mental Health, Hanyang University, Seoul, South Korea; Kim, Hyo-won, Department of Psychiatry, Asan Medical Center, Seoul, South Korea; Jung, Eunji, Yonsei Clinic of Psychiatry, Seoul, South Korea; Chung, Seungwon, Department of Psychiatry, Chungbuk National University Hospital, Cheongju-si, South Korea; Son, Jeong-woo, Department of Psychiatry, Chungbuk National University Hospital, Cheongju-si, South Korea; Yoo, Jaehyun, Department of Psychiatry, The Catholic University of Korea Seoul St. Mary's Hospital, Seoul, South Korea; Jeon, Sekye, Department of Psychiatry, The Catholic University of Korea Seoul St. Mary's Hospital, Seoul, South Korea; Kim, Hwiyoung, Department of Artificial Intelligence, Yonsei University, Seoul, South Korea, Department of Neurosurgery, Yonsei University College of Medicine, Seoul, South Korea; Kim, Bung-nyun, Department of Psychiatry, Seoul National University Hospital, Seoul, South Korea; Cheon, Keun-ah, Department of Child and Adolescent Psychiatry, Severance Hospital, Seoul, South Korea","Early Autism Spectrum Disorder (ASD) identification is crucial but resource-intensive. This study evaluated a novel two-stage multimodal AI framework for scalable ASD screening using data from 1242 children (18–48 months). A mobile application collected parent-child interaction audio and screening tool data (MCHAT, SCQ-L, SRS). Stage 1 differentiated typically developing from high-risk/ASD children, integrating MCHAT/SCQ-L text with audio features (AUROC 0.942). Stage 2 distinguished high-risk from ASD children by combining task success data with SRS text (AUROC 0.914, Accuracy 0.852). The model’s predicted risk categories strongly agreed with gold-standard ADOS-2 assessments (79.59% accuracy) and correlated significantly (Pearson r = 0.830, p < 0.001). Leveraging mobile data and deep learning, this framework demonstrates potential for accurate, scalable early ASD screening and risk stratification, supporting timely interventions. © 2025 Elsevier B.V., All rights reserved.","","Deep learning; Diagnosis; Risk assessment; Audio features; Audio tools; Autism spectrum disorders; Gold standards; Mobile applications; Multi-modal; Parent-child interactions; Risk categories; Risk stratification; Screening tool; Diseases; adaptive behavior; adult; analytical error; Article; artificial intelligence; autism; child; child parent relation; deep learning; diagnostic accuracy; diagnostic test accuracy study; disease severity; false positive result; female; follow up; high risk patient; human; language delay; major clinical study; male; prediction; screening; self care; symptom; videorecording; voice","","","This study was supported by funding from the National Center for Mental Health (grant number: MHER22A01) and the Digital Healthcare Center at Severance Hospital, Yonsei University College of Medicine. We thank the clinicians and research staff at the nine participating hospitals for their assistance with data collection and validation. We are also grateful to the children and families who participated in this study.","Perochon, Sam, Early detection of autism using digital behavioral phenotyping, Nature Medicine, 29, 10, pp. 2489-2497, (2023); Stevens, Elizabeth, Identification and analysis of behavioral phenotypes in autism spectrum disorder via unsupervised machine learning, International Journal of Medical Informatics, 129, pp. 29-36, (2019); Qin, Lei, New advances in the diagnosis and treatment of autism spectrum disorders, European Journal of Medical Research, 29, 1, (2024); IEEE Transactions on Computational Social Systems, (2023); Korean Childhood Autism Rating Scale 2nd Edn K Cars 2, (2020); Christiansz, Jessica A., Autism Spectrum Disorder in the DSM-5: Diagnostic Sensitivity and Specificity in Early Childhood, Journal of Autism and Developmental Disorders, 46, 6, pp. 2054-2063, (2016); Kohli, Manu, The Role of Intelligent Technologies in Early Detection of Autism Spectrum Disorder (ASD): A Scoping Review, IEEE Access, 10, pp. 104887-104913, (2022); Zhu, Fenglei, A multimodal machine learning system in early screening for toddlers with autism spectrum disorders based on the response to name, Frontiers in Psychiatry, 14, (2023); Anagnostopoulou, Panagiota, Artificial intelligence in autism assessment, International Journal of Emerging Technologies in Learning, 15, 6, pp. 95-107, (2020); Megerian, Jonathan Thomas, Evaluation of an artificial intelligence-based medical device for diagnosis of autism spectrum disorder, npj Digital Medicine, 5, 1, (2022)","","Nature Research","","","","","","23986352","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-105013848588"
"Shahini, A.; Kamath, A.P.; Sharma, E.; Salvi, M.; Tan, R.-S.; Siuly, S.; Seoni, S.; Ganguly, R.; Devi, A.; Deo, R.; Barua, P.D.; Acharya, U.R.","Shahini, Alen (59002287600); Kamath, Aditya Prabhakara (57271228000); Sharma, Ekta (57212448573); Salvi, Massimo (57191596088); Tan, Rusan (59811412000); Siuly, Siuly (54382473200); Seoni, Silvia (57213608081); Ganguly, Rahul (56868302700); Devi, Aruna (57752817100); Deo, Ravinesh (8630380500); Barua, Prabal Datta (36993665100); Acharya, Rajendra U. (7004510847)","59002287600; 57271228000; 57212448573; 57191596088; 59811412000; 54382473200; 57213608081; 56868302700; 57752817100; 8630380500; 36993665100; 7004510847","A systematic review for artificial intelligence-driven assistive technologies to support children with neurodevelopmental disorders","2025","Information Fusion","124","","103441","","","0","0","10.1016/j.inffus.2025.103441","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008545443&doi=10.1016%2Fj.inffus.2025.103441&partnerID=40&md5=42c52f37d39fa93768bd51adb4dd8080","Biolab, Politecnico di Torino, Turin, Italy; School of Engineering, Providence, United States; Physics and Computing, University of Southern Queensland, Toowoomba, Australia; National Heart Centre Singapore, Singapore City, Singapore; Duke-NUS Medical School, Singapore City, Singapore; Victoria University Melbourne, Institute of Sustainable Industries and Liveable Cities, Melbourne, Australia; School of Education, Charles Sturt University, Bathurst, Australia; School of Education and Tertiary Access, University of the Sunshine Coast, Sippy Downs, Australia; School of Business (Information System), University of Southern Queensland, Toowoomba, Australia; Centre for Health Research, University of Southern Queensland, Toowoomba, Australia","Shahini, Alen, Biolab, Politecnico di Torino, Turin, Italy; Kamath, Aditya Prabhakara, School of Engineering, Providence, United States; Sharma, Ekta, Physics and Computing, University of Southern Queensland, Toowoomba, Australia; Salvi, Massimo, Biolab, Politecnico di Torino, Turin, Italy; Tan, Rusan, National Heart Centre Singapore, Singapore City, Singapore, Duke-NUS Medical School, Singapore City, Singapore; Siuly, Siuly, Victoria University Melbourne, Institute of Sustainable Industries and Liveable Cities, Melbourne, Australia; Seoni, Silvia, Biolab, Politecnico di Torino, Turin, Italy; Ganguly, Rahul, School of Education, Charles Sturt University, Bathurst, Australia; Devi, Aruna, School of Education and Tertiary Access, University of the Sunshine Coast, Sippy Downs, Australia; Deo, Ravinesh, Physics and Computing, University of Southern Queensland, Toowoomba, Australia; Barua, Prabal Datta, School of Business (Information System), University of Southern Queensland, Toowoomba, Australia; Acharya, Rajendra U., National Heart Centre Singapore, Singapore City, Singapore, Centre for Health Research, University of Southern Queensland, Toowoomba, Australia","This systematic review examines AI-powered assistive technologies for children with neurodevelopmental disorders, with a focus on dyslexia (DYS), attention-deficit hyperactivity disorder (ADHD), and autism spectrum disorder (ASD). Our analysis of 84 studies from 2018 to 2024 provides the first thorough cross-disorder comparison of AI implementation patterns. According to our data, each condition has different success rates and technological preferences. AI applications are expanding quickly, especially in research on ASD (56 % of studies), followed by ADHD (36 %), and DYS (8 %). In almost half of the reviewed studies, computer-assisted technologies, which have demonstrated encouraging results in terms of treatment support and diagnostic accuracy, became the main mode of intervention. Despite high accuracy in controlled settings, the implementation of these technologies in clinical practice faces significant challenges. While human oversight remains essential in clinical applications, future advancements should prioritize privacy protection and the ability to assess tools longitudinally. Notably, multimodal approaches that integrate various data types have improved diagnostic accuracy; recent research has shown that they can detect ASD with up to 99.8 % accuracy and ADHD with up to 97.4 % accuracy. A promising trend is the combination of mobile applications and wearable technology, especially for real-time monitoring and intervention. This review highlights the potential and current limitations of AI-driven tools in supporting children with neurodevelopmental disorders. Future development should focus not on replacing clinical expertise, but on augmenting it. Research efforts should aim at creating tools that enhance professional judgment while preserving the essential human components of assessment and intervention. © 2025 Elsevier B.V., All rights reserved.","Artificial intelligence; Assistive technology; Deep learning; Machine learning; Neurodevelopmental disorders; Pediatric care","Assistive technology; Clinical research; Computer aided diagnosis; Diseases; Learning systems; Wearable computers; Attention deficit hyperactivity disorder; Autism spectrum disorders; Condition; Deep learning; Diagnostic accuracy; Machine-learning; Neurodevelopmental disorder; Pediatric care; Systematic Review","","","","Thapar, Anita K., Neurodevelopmental disorders, The Lancet Psychiatry, 4, 4, pp. 339-346, (2017); Dsm 5 Guidebook the Essential Companion to the Diagnostic and Statistical Manual of Mental Disorders, (2014); Wilens, Timothy Edwin, The Impact of Pharmacotherapy of Childhood-Onset Psychiatric Disorders on the Development of Substance Use Disorders, Journal of Child and Adolescent Psychopharmacology, 32, 4, pp. 200-214, (2022); Schmitt, Andrea, The impact of environmental factors in severe psychiatric disorders, Frontiers in Neuroscience, 8 FEB, (2014); Barua, Prabal Datta, Artificial Intelligence Enabled Personalised Assistive Tools to Enhance Education of Children with Neurodevelopmental Disorders—A Review, International Journal of Environmental Research and Public Health, 19, 3, (2022); Alkahtani, Keetam D.F., Empowering Teachers With Low-Intensity Interventions: Using the Caught Being Good Game to Promote Positive Behavior Among Students With ADHD, Behavioral Disorders, 49, 3, pp. 173-188, (2024); Yoo, Jae-hyun, Development of an innovative approach using portable eye tracking to assist ADHD screening: a machine learning study, Frontiers in Psychiatry, 15, (2024); Kim, Sunhae, Can the mmpi predict adult adhd? An approach using machine learning methods, Diagnostics, 11, 6, (2021); Friedman, Lisa A., Brain development in ADHD, Current Opinion in Neurobiology, 30, pp. 106-111, (2015); Lohani, Dhruv Chandra, ADHD diagnosis using structural brain MRI and personal characteristic data with machine learning framework, Psychiatry Research - Neuroimaging, 334, (2023)","","Elsevier B.V.","","","","","","15662535","","","","English","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-105008545443"
"Joseph, C.; Maheswari, P.U.","Joseph, C. (59908037800); Maheswari, P. Uma (59226762900)","59908037800; 59226762900","Facial emotion based smartphone addiction detection and prevention using deep learning and video based learning","2025","Scientific Reports","15","1","18025","","","0","1","10.1038/s41598-025-99681-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005775694&doi=10.1038%2Fs41598-025-99681-7&partnerID=40&md5=ba6fd1a9a36c9059f40bac23ea5d7ed9","Department of Media Sciences, Anna University, Chennai, India","Joseph, C., Department of Media Sciences, Anna University, Chennai, India; Maheswari, P. Uma, Department of Media Sciences, Anna University, Chennai, India","Smartphone addiction among students has emerged as a critical issue, negatively impacting their academic performance, emotional well-being, and social behavior. This paper introduces the Theory of Mind integrated with Video Modelling (TMVM) framework, a novel deep learning-based approach aimed at recognizing and mitigating smartphone addiction. The TMVM framework leverages Theory of Mind AI to analyze students’ facial emotions via smartphone cameras while watching videos. Based on detected emotions such as happiness, sadness, or anger, the system dynamically shuffles motivational videos using advanced algorithms like Fisher-Yates and Durstenfeld shuffling techniques to promote behavioral change. The framework also incorporates Behavior Parameters (BHP) evaluation, grounded in the Social Identity Model of Deindividuation Effects (SIDE) theory, to assess key behavioral metrics such as social identity, self-awareness, anonymity, responsibility, and accountability. Additionally, face emotion detection algorithms tuned with MnasNet-Teaching Learning Based Optimization (TLBO) and Convolution Neural Networks (CNN)-Cuckoo Search Optimization (CSO) are employed for accurate emotion recognition. Experimental results demonstrate significant improvements in students’ behavior and reductions in smartphone usage post-intervention. The TMVM system achieves high accuracy in emotion detection and behavioral outcome prediction while fostering engagement in school and social activities. TMVM method is tested in 750 students with low BHP and evaluated the behavioural parameters. After the intervention of TMVM the students showed more than 90% improvement in their BHP parameters. A paired sample t-test revealed notable reductions in mean scores from pre- to post-intervention across all measured dimensions. Social identity decreased from 4.07 to 2.21 (t(55) = 16.125, p < 0.001), anonymity from 4.11 to 2.01 (t(55) = 15.699, p < 0.001), self-awareness from 3.95 to 1.93 (t(55) = 15.103, p < 0.001), loss of individuality from 4.04 to 2.07 (t(55) = 13.364, p < 0.001), while sense of responsibility and accountability improved with mean differences of 1.18 and 2.0, respectively, both statistically significant at p < 0.001.The results showed 85% improvement in students’ knowledge and attitudes. © 2025 Elsevier B.V., All rights reserved.","Adolescents; Intervention; Smartphone addiction; Social Media; Video Modelling (VM)","addiction; adolescent; algorithm; artificial neural network; deep learning; diagnosis; emotion; facial expression; female; human; internet addiction; male; prevention and control; psychology; smartphone; student; videorecording; young adult; Adolescent; Algorithms; Behavior, Addictive; Deep Learning; Emotions; Facial Expression; Female; Humans; Internet Addiction Disorder; Male; Neural Networks, Computer; Smartphone; Students; Video Recording; Young Adult","","","","Thomas, Patricia A., Family Relationships and Well-Being, Innovation in Aging, 1, 3, (2017); Journal of Advances in Medical Education Professionalism, (2017); Feng, Guiyun, Research on learning behavior patterns from the perspective of educational data mining: Evaluation, prediction and visualization, Expert Systems with Applications, 237, (2024); Darling-Hammond, Linda, Implications for educational practice of the science of learning and development, Applied Developmental Science, 24, 2, pp. 97-140, (2020); Lavy, Shiri, A Review of Character Strengths Interventions in Twenty-First-Century Schools: their Importance and How they can be Fostered, Applied Research in Quality of Life, 15, 2, pp. 573-596, (2020); Anser, Muhammad Khalid, Dynamic linkages between poverty, inequality, crime, and social expenditures in a panel of 16 countries: two-step GMM estimates, Journal of Economic Structures, 9, 1, (2020); Vezzali, Loris, Using intercultural videos of direct contact to implement vicarious contact: A school-based intervention that improves intergroup attitudes, Group Processes and Intergroup Relations, 22, 7, pp. 1059-1076, (2019); A Study to Examine the Impact of the Paideia Seminar Reading Intervention Program at A School in Connecticut, (2020); Li, Qingfeng, Hypergraph-enhanced multi-interest learning for multi-behavior sequential recommendation, Expert Systems with Applications, 255, (2024); Zurqoni, Zurqoni, Strategy and implementation of character education in senior high schools and vocational high schools, Journal of Social Studies Education Research, 9, 3, pp. 370-397, (2018)","","Nature Research","","","","","","20452322","","","40410532","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-105005775694"
"Tan, C.W.; Du, T.; Teo, J.C.; Chan, D.X.H.; Kong, W.M.; Sng, B.L.","Tan, C. W. (57201466832); Du, Tiehua (57942858700); Teo, Jing Chun (57199274406); Chan, Diana Xin Hui (56784996500); Kong, Waiming (16402227400); Sng, Ban Leong (24825547700)","57201466832; 57942858700; 57199274406; 56784996500; 16402227400; 24825547700","Automated pain detection using facial expression in adult patients with a customized spatial temporal attention long short-term memory (STA-LSTM) network","2025","Scientific Reports","15","1","13429","","","0","0","10.1038/s41598-025-97885-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003401918&doi=10.1038%2Fs41598-025-97885-5&partnerID=40&md5=30b2eb83c7b923a3d7ea1eeddb31af5f","Department of Women's Anesthesia, KK Women's And Children's Hospital, Singapore City, Singapore; Duke-NUS Medical School, Singapore City, Singapore; Biomedical Engineering and Materials Group, Nanyang Polytechnic, Singapore City, Singapore; Digital Integration Medical Innovation and Care Transformation, KK Women's And Children's Hospital, Singapore City, Singapore; Department of Anesthesiology, Singapore Health Services, Singapore City, Singapore","Tan, C. W., Department of Women's Anesthesia, KK Women's And Children's Hospital, Singapore City, Singapore, Duke-NUS Medical School, Singapore City, Singapore; Du, Tiehua, Biomedical Engineering and Materials Group, Nanyang Polytechnic, Singapore City, Singapore; Teo, Jing Chun, Digital Integration Medical Innovation and Care Transformation, KK Women's And Children's Hospital, Singapore City, Singapore; Chan, Diana Xin Hui, Duke-NUS Medical School, Singapore City, Singapore, Department of Anesthesiology, Singapore Health Services, Singapore City, Singapore; Kong, Waiming, Biomedical Engineering and Materials Group, Nanyang Polytechnic, Singapore City, Singapore; Sng, Ban Leong, Department of Women's Anesthesia, KK Women's And Children's Hospital, Singapore City, Singapore, Duke-NUS Medical School, Singapore City, Singapore","Self-reported pain scores are often used for pain assessments and require effective communication. On the other hand, observer-based assessments are resource-intensive and require training. We developed an automated system to assess pain intensity in adult patients based on changes in facial expression. We recruited adult patients undergoing surgery or interventional pain procedures in two public healthcare institutions in Singapore. The patients’ facial expressions were videotaped from a frontal view with varying body poses using a customized mobile application. The collected videos were trimmed into multiple 1 s clips and categorized into three levels of pain: no pain, mild pain, or significant pain. A total of 468 facial key points were extracted from each video frame. A customized spatial temporal attention long short-term memory (STA-LSTM) deep learning network was trained and validated using the extracted keypoints to detect pain levels by analyzing facial expressions in both the spatial and temporal domains. Model performance was evaluated using accuracy, sensitivity, recall, and F1-score. Two hundred patients were recruited, with 2008 videos collected for further clipping into 10,274 1 s clips. Videos from 160 patients (7599 clips) were used for STA-LSTM training, while the remaining 40 patients’ videos (2675 clips) were set aside for validation. By differentiating the polychromous levels of pain (no pain versus mild pain versus significant pain requiring clinical intervention), we reported the optimal performance of STA-LSTM model, with accuracy, sensitivity, recall, and F1-score all at 0.8660. Our proposed solution has the potential to facilitate objective pain assessment in clinical settings through the developed STA-LSTM model, enabling healthcare professionals and caregivers to perform pain assessments effectively in both inpatient and outpatient settings. © 2025 Elsevier B.V., All rights reserved.","Artificial intelligence; Deep learning; Facial; Machine learning; Pain assessment","adult; aged; attention; deep learning; diagnosis; facial expression; female; human; long term memory; male; middle aged; pain; pain measurement; procedures; short term memory; videorecording; Adult; Aged; Attention; Deep Learning; Facial Expression; Female; Humans; Male; Memory, Long-Term; Memory, Short-Term; Middle Aged; Pain; Pain Measurement; Video Recording","","","The study was supported by Singapore Ministry of Health (MOH) Health Innovation Fund (Reference number MH 110:12/12-30). The funders had no roles in the study design, data collection and analysis, interpretation of data and the manuscript writing.","LeResche, Linda A., Facial expression in pain: A study of candid photographs, Journal of Nonverbal Behavior, 7, 1, pp. 46-56, (1982); Skrobik, Yoanna K., Pain, Analgesic Effectiveness, and Long-Term Opioid Dependency, Lessons from the ICU, pp. 213-222, (2020); Thong, Ivan Shin Kai, The validity of pain intensity measures: What do the NRS, VAS, VRS, and FPS-R measure?, Scandinavian Journal of Pain, 18, 1, pp. 99-107, (2018); Handbook of Pain Assessment, (2011); Manocha, Sachin, Assessment of paediatric pain: A critical review, Journal of Basic and Clinical Physiology and Pharmacology, 27, 4, pp. 323-331, (2016); Kunz, Miriam, Facial muscle movements encoding pain - A systematic review, Pain, 160, 3, pp. 535-549, (2019); Dildine, Troy Christopher, The need for diversity in research on facial expressions of pain, Pain, 160, 8, pp. 1901-1902, (2019); Automated Pain Detection from Facial Expressions Using Facs A Review, (2018); Atee, Mustafa, Pain assessment in dementia: Evaluation of a point-of-care technological solution, Journal of Alzheimer's Disease, 60, 1, pp. 137-150, (2017); Hoti, Kreshnik, Clinimetric properties of the electronic pain assessment tool (ePAT) for aged-care residents with moderate to severe dementia, Journal of Pain Research, 11, pp. 1037-1044, (2018)","","Nature Research","","","","","","20452322","","","40251301","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-105003401918"
"Guo, W.-H.; Yang, X.-D.; Ruan, Z.; X.; D.-Z.; Song, S.-C.; Chen, Y.-Q.; Chan, P.","Guo, Weihang (57423552800); Yang, Xiaodong (57198984204); Ruan, Zheng (59236244500); Song, Shuchao (58847700500); Chen, Yiqiang (35408792800); Chan, Piu (7403497663)","57423552800; 57198984204; 59236244500; 60097187300; 60097222500; 58847700500; 35408792800; 7403497663","Early detection of Parkinson's disease: Machine learning-based prediction of UPDRS Part III scores in de novo patients using smartphone assessments","2025","Journal of Parkinson's Disease","15","6","","1099","1110","0","0","10.1177/1877718X251359494","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015627717&doi=10.1177%2F1877718X251359494&partnerID=40&md5=a4d6388a0a9bd7d9b3489455660c5f54","Department of Neurology and Neurobiology, Xuanwu Hospital, Capital Medical University, Beijing, China; Institute of Computing Technology Chinese Academy of Sciences, Beijing, China; School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Xuanwu Hospital, Capital Medical University, Beijing, China; Xuanwu Hospital, Capital Medical University, Beijing, China; Innovation Center for Neurological Disorders, Xuanwu Hospital, Capital Medical University, Beijing, China","Guo, Weihang, Department of Neurology and Neurobiology, Xuanwu Hospital, Capital Medical University, Beijing, China; Yang, Xiaodong, Institute of Computing Technology Chinese Academy of Sciences, Beijing, China, School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China; Ruan, Zheng, Department of Neurology and Neurobiology, Xuanwu Hospital, Capital Medical University, Beijing, China; null, null, University of Chinese Academy of Sciences, Beijing, China; null, null, Institute of Computing Technology Chinese Academy of Sciences, Beijing, China, School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China; Song, Shuchao, Institute of Computing Technology Chinese Academy of Sciences, Beijing, China, School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China; Chen, Yiqiang, Institute of Computing Technology Chinese Academy of Sciences, Beijing, China, School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China; Chan, Piu, Department of Neurology and Neurobiology, Xuanwu Hospital, Capital Medical University, Beijing, China, Xuanwu Hospital, Capital Medical University, Beijing, China, Xuanwu Hospital, Capital Medical University, Beijing, China, Innovation Center for Neurological Disorders, Xuanwu Hospital, Capital Medical University, Beijing, China","BackgroundDetecting motor symptoms in Parkinson's disease (PD) at home, especially in the prodromal, is crucial for disease-modifying therapies.ObjectiveTo evaluate the effectiveness of machine learning models using smartphone-based assessments in predicting motor symptoms in untreated de novo PD.MethodsUsing a clinical trial in early de novo patients with PD, the PDAssist smartphone application and machine learning models were investigated for eight motor tasks: resting tremor, postural tremor, finger tapping, facial expressions, rigidity, speech, walking, and pronation/supination to predict motor symptoms of PD as comparing with UPDRS Part III scores.ResultsOur prediction model demonstrated acceptable performance in detecting PD mild symptoms, with accuracy ranging from 0.87 to 0.93 for resting tremor, postural tremor, finger tapping, facial expressions and postural stability, while the rigidity model achieved 0.81 accuracy with a Kappa of 0.74, and the speech model showed 0.79 accuracy and 0.61 Kappa, emphasizing its potential for detecting subtle motor deficits and remote monitoring. External validation confirmed the model's robustness, with significantly higher predicted scores (all tasks) for PD patients (9.45 ± 3.08) compared to healthy controls (3.79 ± 1.99, t = -14.27, p < 0.001), validating its ability to differentiate between the two groups.ConclusionsSmartphone-based assessments effectively discriminate de novo PD patients from controls and monitor motor symptoms in prodromal and early PD patients. Future work will involve expanding patient cohorts and refining algorithms for better generalizability and reliability of self-collected data in home settings.; UPDRS Part III score has been suggested a sensitive measure for detecting early motor symptoms of Parkinson's disease (PD), but it is difficult to apply at home setting which is important for early detection and intervention. Smartphone apps and machine learning models may provide the alternative. Taking the advantage of a clinical trial in early de novo PD patients, we used a smartphone app, PDAssist, to investigate the machine learning models on various motor symptoms including tremors, finger tapping, facial expressions, rigidity, speech, walking, and pronation/supination as compared with their UPDRS Part III score. The PDAssist apps performed well in identifying mild motor symptoms like tremors finger tapping and facial expression with high accuracy in measurements. These results suggest that smartphone-based assessments can be useful tools for identifying early motor deficits in de novo PD patients and monitoring of PD motor symptoms from home. This record is sourced from MEDLINE/PubMed, a database of the U.S. National Library of Medicine","machine learning; motor symptoms; Parkinson’s disease; smartphone assessment; untreated de novo patients; UPDRS part III","aged; complication; diagnosis; early diagnosis; etiology; female; human; machine learning; male; middle aged; mobile application; Parkinson disease; pathophysiology; severity of illness index; smartphone; tremor; Aged; Early Diagnosis; Female; Humans; Machine Learning; Male; Middle Aged; Mobile Applications; Parkinson Disease; Severity of Illness Index; Smartphone; Tremor","","","","","","","","","","","","18777171; 1877718X","","","40725991","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-105015627717"
"Cronin, P.; Collins, L.M.; Sullivan, A.M.","Cronin, Padraig (58992837900); Collins, Lucy M. (58295887200); Sullivan, Aideen M. (7202864584)","58992837900; 58295887200; 7202864584","Commercially available products for the digital tracking of biomarkers in Parkinson's Disease","2025","Aging and Health Research","5","3","100240","","","0","0","10.1016/j.ahr.2025.100240","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007092215&doi=10.1016%2Fj.ahr.2025.100240&partnerID=40&md5=456e78c598ffadd674cc14ee82236ac2","Department of Pharmacology and Therapeutics, University College Cork, Cork, Ireland; Department of Anatomy and Neuroscience, School of Medicine, Cork, Ireland; University College Cork, Cork, Ireland","Cronin, Padraig, Department of Pharmacology and Therapeutics, University College Cork, Cork, Ireland, Department of Anatomy and Neuroscience, School of Medicine, Cork, Ireland; Collins, Lucy M., Department of Anatomy and Neuroscience, School of Medicine, Cork, Ireland, University College Cork, Cork, Ireland; Sullivan, Aideen M., Department of Pharmacology and Therapeutics, University College Cork, Cork, Ireland, Department of Anatomy and Neuroscience, School of Medicine, Cork, Ireland, University College Cork, Cork, Ireland","Background: Parkinson's Disease (PD) is a debilitating neurological disorder which affects 8.5 million people globally. Diagnosis of PD is made upon presentation of motor symptoms. However, there is a well-recognised prodromal phase of PD, when patients experience non-motor symptoms, and subtle motor symptoms, before the onset of the cardinal motor symptoms. Biomarkers of this prodromal phase can provide a diagnostic window into early disease processes, assisting with the differential diagnosis of PD and enabling earlier treatment. Due to increased availability of commercially-available products, both wearable devices and smartphone applications are being explored for potential to identify PD biomarkers. Such products can provide clinicians with early warning of disease progression, and supply researchers with tools for monitoring PD outside of laboratory settings. Methods: This systematic review critically examined the academic literature published in the English language to identify currently-available products designed to track biomarkers of PD across 6 databases between January 2000 and March 2025. Results: 27 papers were identified which captured physiological biomarkers in PD patients using commercially-available products. Current products emphasise the capture of early motor dysfunction through both upper limb and eye movements. There is a lack of literature on the validation of commercially-available products for the detection of PD, despite an increase in advanced data analysis algorithms. Conclusion: There is a critical need for validation of devices for the tracking of biomarkers of PD, which may be utilised for detection during the prodromal phase. © 2025 Elsevier B.V., All rights reserved.","Detection; Parkinson's disease; Technology; Wearable devices","biological marker; bradykinesia; commercial phenomena; controlled study; deep learning; diagnostic test accuracy study; disease exacerbation; disease severity; entropy; eye movement; facial expression; gyroscope sensor; Hoehn and Yahr scale; human; MDS-Unified Parkinson Disease Rating Scale; motor dysfunction; motor performance; outcome assessment; Parkinson disease; patient monitoring; Personality Disorder Questionnaire; photoelectric plethysmography; predictive value; quality control; receiver operating characteristic; remote sensing; Review; sensitivity and specificity; stride time; systematic review; test retest reliability; Unified Parkinson Disease Rating Scale; upper limb","","","","Jankovic, Joseph J., Parkinson's disease: Clinical features and diagnosis, Journal of Neurology, Neurosurgery and Psychiatry, 79, 4, pp. 368-376, (2008); undefined, (2023); Dorsey, Earl Ray, The emerging evidence of the Parkinson pandemic, Journal of Parkinson's Disease, 8, s1, pp. S3-S8, (2018); MacPhee, Graeme J.A., Parkinson's disease, Reviews in Clinical Gerontology, 11, 1, pp. 33-49, (2001); Braak, Heiko, Staging of brain pathology related to sporadic Parkinson's disease, Neurobiology of Aging, 24, 2, pp. 197-211, (2003); Dickson, Dennis W., Neuropathology of non-motor features of Parkinson disease, Parkinsonism and Related Disorders, 15, SUPPL. 3, pp. S1-S5, (2009); Braak, Heiko, Neuropathological Staging of Brain Pathology in Sporadic Parkinson's disease: Separating the Wheat from the Chaff, Journal of Parkinson's Disease, 7, s1, pp. S73-S87, (2017); Simon, David K., Parkinson Disease Epidemiology, Pathology, Genetics, and Pathophysiology, Clinics in Geriatric Medicine, 36, 1, pp. 1-12, (2020); Brooks, David J., Imaging approaches to Parkinson disease, Journal of Nuclear Medicine, 51, 4, pp. 596-609, (2010); Kordower, Jeffrey H., Disease duration and the integrity of the nigrostriatal system in Parkinson's disease, Brain, 136, 8, pp. 2419-2431, (2013)","","Elsevier B.V.","","","","","","26670321","","","","English","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-105007092215"
"Sadashiva, S.; Manjunath, K.G.","Sadashiva, Shwetha (60090253100); Manjunath, K. G. (55751854400)","60090253100; 55751854400","Multimodal Deep Learning Framework Using Transformer and LSTM Models for Suicide Risk Detection from Social Media and Voice Data","2025","International Journal of Safety and Security Engineering","15","7","","1445","1459","0","0","10.18280/ijsse.150712","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015357113&doi=10.18280%2Fijsse.150712&partnerID=40&md5=b7e3dc91e49968b5e8b08f2c2e0ba42e","Department of Computer Science and Engineering, Siddaganga Institute of Technology, Tumkur, India","Sadashiva, Shwetha, Department of Computer Science and Engineering, Siddaganga Institute of Technology, Tumkur, India; Manjunath, K. G., Department of Computer Science and Engineering, Siddaganga Institute of Technology, Tumkur, India","Mental issues in the younger generation have become a growing concern worldwide with suicidal ideation and behaviors creating serious but mostly ignored risks until the point where consequences are irreversible. Conventional detection methods in the form of clinical interviews, psychological tests, and self-report surveys are plagued by limitations such as high subjectivity, poor scalability, and delayed response. This study suggests a modern multimodal framework based on advanced deep learning to identify suicidal inclinations through analysis of various digital indicators in the form of social media text, audio speech, and facial expressions. The system combines various top-performing models—Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM), and Transformer models—with Natural Language Processing (NLP) methods to identify and analyze temporal and contextual patterns that are suicidal. Comparative evaluation based on the Social Media Suicide Risk Dataset, with both linguistic and acoustic features, was performed. Of the various models evaluated, the best-performing model was the Transformer model, showing an accuracy of 94.2% with good precision, recall, and F1-Score values. This reflects the model's efficacy in recognizing weak, high-risk behavioral indicators in various modalities. Through real-time, customized digital intervention, this system provides an efficient and scalable solution to be employed by mental professionals, educational institutions, and social networks to identify suicide risk in advance and act towards saving those in danger, especially from the younger generation. © 2025 Elsevier B.V., All rights reserved.","mental health monitoring; multimodal fusion; personalized intervention; social media analysis; suicide risk prediction; transformer models","","","","","Wang, Huaqiang, Deep-Learning-Based Multiview RGBD Sensor System for 3-D Face Point Cloud Registration, IEEE Sensors Letters, 7, 5, pp. 1-4, (2023); Tan, Phan Xuan, Attention-Based Grasp Detection With Monocular Depth Estimation, IEEE Access, 12, pp. 65041-65057, (2024); Venkatesh, Divyashree Yamadur, Efficient reconfigurable parallel switching for low-density parity-check encoding and decoding, IAES International Journal of Artificial Intelligence, 14, 1, pp. 260-269, (2025); Laga, Hamid, A Survey on Deep Learning Techniques for Stereo-Based Depth Estimation, IEEE Transactions on Pattern Analysis and Machine Intelligence, 44, 4, pp. 1738-1764, (2022); Shankara, Kavitha Hosakote, An efficient load-balancing in machine learning-based DC-DC conversion using renewable energy resources, IAES International Journal of Artificial Intelligence, 14, 1, pp. 307-316, (2025); Honnegowda, Jyothi, Efficient reduction of computational complexity in video surveillance using hybrid machine learning for event recognition, IAES International Journal of Artificial Intelligence, 14, 1, pp. 317-326, (2025); Poornima, Madaraje Urs, An Innovative IoT Framework using Machine Learning for Predicting Information Loss at the Data Link Layer in Smart Networks, Engineering, Technology and Applied Science Research, 15, 2, pp. 20904-20911, (2025); Hu, Junjie, Deep Depth Completion From Extremely Sparse Data: A Survey, IEEE Transactions on Pattern Analysis and Machine Intelligence, 45, 7, pp. 8244-8264, (2023); Ren, Liangliang, Uniform and variational deep learning for RGB-D object recognition and person re-identification, IEEE Transactions on Image Processing, 28, 10, pp. 4970-4983, (2019); Han, Zhi, Depth Selection for Deep ReLU Nets in Feature Extraction and Generalization, IEEE Transactions on Pattern Analysis and Machine Intelligence, 44, 4, pp. 1853-1868, (2022)","","International Information and Engineering Technology Association","","","","","","20419031; 2041904X","","","","English","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-105015357113"
"la Fauci de Leo, A.; Bagheri-Zadeh, P.; Voderhobli, K.; Sheikh-Akbari, A.","la Fauci de Leo, Andrea (60016935100); Bagheri-Zadeh, Pooneh (22137238400); Voderhobli, Kiran (9333838200); Sheikh-Akbari, Akbar (24469966400)","60016935100; 22137238400; 9333838200; 24469966400","A New AI Framework to Support Social-Emotional Skills and Emotion Awareness in Children with Autism Spectrum Disorder","2025","Computers","14","7","292","","","0","0","10.3390/computers14070292","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011834407&doi=10.3390%2Fcomputers14070292&partnerID=40&md5=6796fedfcf8ecf1ed3b6b34805e0b839","Leeds Beckett University, Leeds, United Kingdom","la Fauci de Leo, Andrea, Leeds Beckett University, Leeds, United Kingdom; Bagheri-Zadeh, Pooneh, Leeds Beckett University, Leeds, United Kingdom; Voderhobli, Kiran, Leeds Beckett University, Leeds, United Kingdom; Sheikh-Akbari, Akbar, Leeds Beckett University, Leeds, United Kingdom","This research highlights the importance of Emotion Aware Technologies (EAT) and their implementation in serious games to assist children with Autism Spectrum Disorder (ASD) in developing social-emotional skills. As AI is gaining popularity, such tools can be used in mobile applications as invaluable teaching tools. In this paper, a new AI framework application is discussed that will help children with ASD develop efficient social-emotional skills. It uses the Jetpack Compose framework and Google Cloud Vision API as emotion-aware technology. The framework is developed with two main features designed to help children reflect on their emotions, internalise them, and train them how to express these emotions. Each activity is based on similar features from literature with enhanced functionalities. A diary feature allows children to take pictures of themselves, and the application categorises their facial expressions, saving the picture in the appropriate space. The three-level minigame consists of a series of prompts depicting a specific emotion that children have to match. The results of the framework offer a good starting point for similar applications to be developed further, especially by training custom models to be used with ML Kit. © 2025 Elsevier B.V., All rights reserved.","AI; Android; Autism Spectrum Disorder; Cloud Vision; Serious Games","Android (operating system); Application programming interfaces (API); Artificial intelligence; Assistive technology; Behavioral research; Emotion Recognition; Fighter aircraft; Android; Autism spectrum disorders; Children with autisms; Cloud vision; Custom models; Facial Expressions; Google+; Mobile applications; Teaching tools; Three-level; Serious games","","","A similar functionality is seen in another application called Emoface, sponsored by the National Centre for Scientific Research in France (CNRS). However, such an application does not use emotion-aware technology, as the user would manually add their pictures to the list.","Wu, Xuesen, Global trends and hotspots in the digital therapeutics of autism spectrum disorders: a bibliometric analysis from 2002 to 2022, Frontiers in Psychiatry, 14, (2023); Hus, Yvette, Challenges surrounding the diagnosis of autism in children, Neuropsychiatric Disease and Treatment, 17, pp. 3509-3529, (2021); O'Nions, Elizabeth J.P., Autism in England: assessing underdiagnosis in a population-based cohort study of prospectively collected primary care data, The Lancet Regional Health - Europe, 29, (2023); American Psychiatric Association, (2022); International Journal of Academic Research in Business and Social Sciences, (2018); Tahiru, Fati, AI in education: A systematic literature review, Journal of Cases on Information Technology, 23, 1, pp. 1-20, (2021); Rusia, Mayank Kumar, A comprehensive survey on techniques to handle face identity threats: challenges and opportunities, Multimedia Tools and Applications, 82, 2, pp. 1669-1748, (2023); Andrejevic, Mark, Facial recognition technology in schools: critical questions and concerns, Learning, Media and Technology, 45, 2, pp. 115-128, (2020); Zakari, Hanan Makki, A review of serious games for children with autism spectrum disorders (ASD), Lecture Notes in Computer Science, 8778, pp. 93-106, (2014); Grossard, Charline, Serious games to teach social interactions and emotions to individuals with autism spectrum disorders (ASD), Computers and Education, 113, pp. 195-211, (2017)","","Multidisciplinary Digital Publishing Institute (MDPI)","","","","","","2073431X","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-105011834407"
"Faria, D.R.; da Silva Ayrosa, P.P.","Faria, Diego R. (26031469100); da Silva Ayrosa, Pedro Paulo (59661488800)","26031469100; 59661488800","Adaptive Neuro-Affective Engagement via Bayesian Feedback Learning in Serious Games for Neurodivergent Children","2025","Applied Sciences (Switzerland)","15","13","7532","","","0","0","10.3390/app15137532","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010338609&doi=10.3390%2Fapp15137532&partnerID=40&md5=ab061b5a3e72ab15c300ddb1c70daafb","School of Sciences, Loughborough University, Loughborough, United Kingdom; Computer Science Deapartment, Universidade Estadual de Londrina, Londrina, Brazil","Faria, Diego R., School of Sciences, Loughborough University, Loughborough, United Kingdom; da Silva Ayrosa, Pedro Paulo, Computer Science Deapartment, Universidade Estadual de Londrina, Londrina, Brazil","Neuro-Affective Intelligence (NAI) integrates neuroscience, psychology, and artificial intelligence to support neurodivergent children through personalized Child–Machine Interaction (CMI). This paper presents an adaptive neuro-affective system designed to enhance engagement in children with neurodevelopmental disorders through serious games. The proposed framework incorporates real-time biophysical signals—including EEG-based concentration, facial expressions, and in-game performance—to compute a personalized engagement score. We introduce a novel mechanism, Bayesian Immediate Feedback Learning (BIFL), which dynamically selects visual, auditory, or textual stimuli based on real-time neuro-affective feedback. A multimodal CNN-based classifier detects mental states, while a probabilistic ensemble merges affective state classifications derived from facial expressions. A multimodal weighted engagement function continuously updates stimulus–response expectations. The system adapts in real time by selecting the most appropriate cue to support the child’s cognitive and emotional state. Experimental validation with 40 children (ages 6–10) diagnosed with Autism Spectrum Disorder (ASD) and Attention Deficit Hyperactivity Disorder (ADHD) demonstrates the system’s effectiveness in sustaining attention, improving emotional regulation, and increasing overall game engagement. The proposed framework—combining neuro-affective state recognition, multimodal engagement scoring, and BIFL—significantly improved cognitive and emotional outcomes: concentration increased by 22.4%, emotional engagement by 24.8%, and game performance by 32.1%. Statistical analysis confirmed the significance of these improvements ((Formula presented.), Cohen’s (Formula presented.)). These findings demonstrate the feasibility and impact of probabilistic, multimodal, and neuro-adaptive AI systems in therapeutic and educational applications. © 2025 Elsevier B.V., All rights reserved.","Bayesian feedback learning; educational AI; EEG; emotion recognition; serious games","Artificial intelligence; Biomedical signal processing; Cognitive systems; Electroencephalography; Emotion Recognition; Feedback; Human computer interaction; Interactive computer systems; Iodine compounds; Psychology computing; Bayesian; Bayesian feedback learning; Educational AI; Emotion recognition; Facial Expressions; Feedback learning; Immediate feedbacks; Multi-modal; Performance; Real- time; Serious games","","","This research did not receive direct external funding. However, the first author was supported by mobility grants for conducting pilot studies in Brazil, funded by the Newton Fund (UK), CONFAP (Brazil), and IEEE RAS-SIGHT.","Autism Spectrum Disorders and Other Developmental Disorders Fact Sheet, (2023); Francillette, Yannick, Serious games for people with mental disorders: State of the art of practices to maintain engagement and accessibility, Entertainment Computing, 37, (2021); Grossard, Charline, Serious games to teach social interactions and emotions to individuals with autism spectrum disorders (ASD), Computers and Education, 113, pp. 195-211, (2017); Hassan, Ahmed, Serious games to improve social and emotional intelligence in children with autism, Entertainment Computing, 38, (2021); Saleme, Pamela, Design of a digital game intervention to promote socio-emotional skills and prosocial behavior in children, Multimodal Technologies and Interaction, 5, 10, (2021); Moreno, Gustavo, Recommendations for the design of inclusive apps for the treatment of autism : An approach to design focused on inclusive users, Iberian Conference on Information Systems and Technologies, CISTI, 2020-June, (2020); Alabdulkreem, Eatedal A., Computer-assisted learning for improving ADHD individuals’ executive functions through gamified interventions: A review, Entertainment Computing, 33, (2020); Friedrich, Elisabeth V.C., Brain-computer interface game applications for combined neurofeedback and biofeedback treatment for children on the autism spectrum, Frontiers in Neuroengineering, 7, JUL, (2014); Liarokapis, Fotis, Comparing interaction techniques for serious games through brain-computer interfaces: A user perception evaluation study, Entertainment Computing, 5, 4, pp. 391-399, (2014); Rajabi, Souran, Effect of combined neurofeedback and game-based cognitive training on the treatment of ADHD: A randomized controlled study, Applied Neuropsychology: Child, 9, 3, pp. 193-205, (2020)","","Multidisciplinary Digital Publishing Institute (MDPI)","","","","","","20763417","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-105010338609"
"Resano-Zuazu, M.; Carmona, J.U.; Argüelles, D.","Resano-Zuazu, María (57220865421); Carmona, J. U. (29067507900); Argüelles, David (23982100800)","57220865421; 29067507900; 23982100800","Short-Term Impact of Dry Needling Treatment for Myofascial Pain on Equine Biomechanics Through Artificial Intelligence-Based Gait Analysis","2025","Animals","15","11","1517","","","0","1","10.3390/ani15111517","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007679130&doi=10.3390%2Fani15111517&partnerID=40&md5=26f558320576119526eeefde03d54424","Department of Animal Medicine and Surgery, Universidad de Córdoba, Cordoba, Spain; Departamento de Salud Animal, Universidad de Caldas, Manizales, Colombia","Resano-Zuazu, María, Department of Animal Medicine and Surgery, Universidad de Córdoba, Cordoba, Spain; Carmona, J. U., Departamento de Salud Animal, Universidad de Caldas, Manizales, Colombia; Argüelles, David, Department of Animal Medicine and Surgery, Universidad de Córdoba, Cordoba, Spain","Myofascial pain syndrome (MPS) is a common source of musculoskeletal pain, characterized by trigger points (TrPs). In horses, MPS is frequently underdiagnosed, and evidence on DN effectiveness is limited. This study investigated whether DN can improve the biomechanics in horses using an artificial intelligence (AI)-based markerless smartphone application (app). Fourteen horses participated, including nine used in assisted therapy, four leisure horses, and one with mixed use. The presence of TrPs was evaluated in six muscles through manual palpation: brachiocephalicus, trapezius, gluteus medius, biceps femoris, semitendinosus, and quadriceps femoris. The horses were divided into a treatment group (TG) (n = 7) and control group (CG) (n = 7). Biomechanical data were recorded in a straight line at a trot before the treatment (T0), immediately after the treatment (T1), and 72 h post-treatment (T72). The stride frequency (SF) was significantly lower (p < 0.05) at 72 h compared with both before and immediately after the treatment. The SF of the TG at 72 h was significantly lower than the SF of the CG at T1 (p < 0.05). Non-significant differences were observed for both the asymmetry push-off and impact phase variables, except for the forelimb head range of motion (FHROM) severity, which was significantly (p < 0.05) greater in the CG than in the TG. This study suggests that DN may enhance the gait quality in horses with MPS. © 2025 Elsevier B.V., All rights reserved.","horse; locomotion analysis; machine learning; physiotherapy; trigger points","animal experiment; Article; artificial intelligence; biceps femoris muscle; biomechanics; brachiocephalic trunk; dry needling; facial expression; forelimb; gait; gluteus medius muscle; informed consent; leisure; musculoskeletal pain; myofascial pain; nonhuman; palpation; physiotherapy; quadriceps femoris muscle; range of motion; semitendinous muscle; short term memory; trapezius muscle; trigger point","","","","Lawin, Felix Jaremo, Is Markerless More or Less? Comparing a Smartphone Computer Vision Method for Equine Lameness Assessment to Multi-Camera Motion Capture, Animals, 13, 3, (2023); Diagnosis and Treatment of Equine Musculo Skeletal Pain the Role of Complementary Modalities Acupuncture and Chiropractic, (2005); Janssens, Luc August A., Trigger Points in 48 Dogs with Myofascial Pain Syndromes, Veterinary Surgery, 20, 4, pp. 274-278, (1991); Janssens, L. A., Trigger point therapy., 4, 1, pp. 117-124, (1992); Frank, Elizabeth M., Myofascial trigger point diagnostic criteria in the dog, Journal of Musculoskeletal Pain, 7, 1-2, pp. 231-237, (1999); Ridgway, K., Acupuncture as a treatment modality for back problems., Veterinary Clinics of North America - Equine Practice, 15, 1, pp. 211-221, (1999); Am J Tradit Chin Vet Med, (2012); Wall, Richard L., Introduction to myofascial trigger points in dogs, Topics in Companion Animal Medicine, 29, 2, pp. 43-48, (2014); Bowen, Annette Gwyneth, Investigation of Myofascial Trigger Points in Equine Pectoral Muscles and Girth-Aversion Behavior, Journal of Equine Veterinary Science, 48, pp. 154-160.e1, (2017); Sato, Nadia Yuri Shimosaka, Myofascial pain syndrome, myofascial trigger points and trigger points in veterinary medicine: A review, Brazilian Journal of Veterinary Research and Animal Science, 57, 2, pp. 1-9, (2020)","","Multidisciplinary Digital Publishing Institute (MDPI)","","","","","","20762615","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-105007679130"
"Viljoen, M.; Seris, N.; Shabalala, N.; Ndlovu, M.; de Vries, P.J.; Franz, L.","Viljoen, Marisa (57193718497); Seris, Noleen (57204331595); Shabalala, Nokuthula (6503908791); Ndlovu, Minkateko (57224443379); de Vries, Petrus J. (23766164600); Franz, Lauren (26221735600)","57193718497; 57204331595; 6503908791; 57224443379; 23766164600; 26221735600","Adapting an early autism caregiver coaching intervention for telehealth delivery in low-resource settings: A South African study of the ‘what’ and the ‘why’","2025","Autism","29","5","","1246","1262","0","1","10.1177/13623613241300774","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211507813&doi=10.1177%2F13623613241300774&partnerID=40&md5=eb01b09d757ff7feef2e7ee4548194aa","Department of Child and Adolescent Psychiatry, University of Cape Town, Cape Town, South Africa; Duke University, Durham, United States; Duke University School of Medicine, Durham, United States","Viljoen, Marisa, Department of Child and Adolescent Psychiatry, University of Cape Town, Cape Town, South Africa; Seris, Noleen, Department of Child and Adolescent Psychiatry, University of Cape Town, Cape Town, South Africa; Shabalala, Nokuthula, Department of Child and Adolescent Psychiatry, University of Cape Town, Cape Town, South Africa; Ndlovu, Minkateko, Department of Child and Adolescent Psychiatry, University of Cape Town, Cape Town, South Africa; de Vries, Petrus J., Department of Child and Adolescent Psychiatry, University of Cape Town, Cape Town, South Africa; Franz, Lauren, Department of Child and Adolescent Psychiatry, University of Cape Town, Cape Town, South Africa, Duke University, Durham, United States, Duke University School of Medicine, Durham, United States","The COVID-19 pandemic required in-person interventions to be adapted for remote delivery all over the globe. In South Africa, an in-person cascaded task-sharing naturalistic developmental behavioural intervention was adapted for telehealth delivery in a low-resource context. Here we describe the adaptations made (the ‘what’) and reasons for adaptations (the ‘why’). The Framework for Modification and Adaptations (FRAME) was used to document the ‘what’, and the Exploration, Preparation, Implementation, Sustainment (EPIS) framework to describe the ‘why’. Systematic member-checking ensured robustness of results. The ‘what’ included 10 adaptations: selecting WhatsApp as delivery platform, developing images with simple text to communicate intervention concepts, modifying session structure for hybrid delivery, including a caregiver self-reflection checklist, utilizing online practitioner training, supervision, assessment and consent procedures, developing session recording procedures, distributing session materials electronically, and developing caregiver–child interaction recording and uploading protocols. The ‘why’ included three outer contextual factors (the digital divide, WhatsApp security/privacy policy, and COVID-19 restrictions), three inner contextual factors (characteristics of caregivers and practitioners, ethics board guidance, and school leadership and organizational characteristics) and one innovation factor (support from intervention co-developers). Adaptations were made in response to unchangeable outer contextual factors and through identification of malleable inner contextual factors. Lay abstract: We were busy with an early autism caregiver-coaching programme in South Africa, when COVID-19 stopped all in-person work. We changed the programme so it could be done using computers and/or phones. Here, we describe programme changes (which we call the ‘what’) and the reasons for those changes (which we call the ‘why’). We used a tool called the Framework for Modification and Adaptations (FRAME) to describe the ‘what’, and the Exploration, Preparation, Implementation, Sustainment (EPIS) framework to describe the ‘why’ of our programme changes. The team members who helped make these changes checked that the changes described were correct. We made 10 changes in total: we used WhatsApp to deliver the programme, made simple pictures with words as visual tools for the programme, changed some session activities, changed a self-reflection checklist, provided all activities online, changed the way assessment and consent was done, made a session recording guide, sent things needed for sessions by email and WhatsApp, and made a caregiver–child play recording guide. The reasons for changes (the ‘why’) were about factors outside schools (the types of phones and data people had, WhatsApp security rules, COVID-19 rules), things inside schools/workplace (about the caregivers and nonspecialists themselves, ethics boards, things about the school itself), and support from people who developed the programme. Changes were made by working with things inside schools/workplace that could change. Identifying what could change helped focus and guide which changes were made to a programme. © 2025 Elsevier B.V., All rights reserved.","adaptation; caregiver coaching; exploration; implementation; Naturalistic Developmental Behavioural Intervention; preparation; sustainment (EPIS) framework; telehealth; The Framework for Modification and Adaptations (FRAME)","accuracy; adaptation; Article; artificial intelligence; autism; caregiver; checklist; child; clinical trial; coaching; community; conceptual framework; coronavirus disease 2019; data analysis; data extraction; data source; digital divide; human; intervention study; leadership; physician; privacy; qualitative research; self-reflection; social media; South Africa; South African; stakeholder participation; task sharing; telehealth; workplace; education; female; male; mentoring; mobile application; preschool child; procedures; psychology; Severe acute respiratory syndrome coronavirus 2; telemedicine; therapy; Autistic Disorder; Caregivers; Child; Child, Preschool; COVID-19; Female; Humans; Male; Mentoring; Mobile Applications; SARS-CoV-2; Telemedicine","","","The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: This work was supported by the National Institutes of Mental Health (K01 MH104370; R21 MH120696; R01 MH127573).","Aarons, Gregory A., Advancing a conceptual model of evidence-based practice implementation in public service sectors, Administration and Policy in Mental Health and Mental Health Services Research, 38, 1, pp. 4-23, (2011); Acharibasam, Jeremiah Wezanamo, Telemental health in low- And middle-income countries: A systematic review, International Journal of Telemedicine and Applications, 2018, (2018); Amaral, David G., COVID-19 and Autism Research: Perspectives from Around the Globe, Autism Research, 13, 6, pp. 844-869, (2020); Ameis, Stephanie H., Coping, fostering resilience, and driving care innovation for autistic people and their families during the COVID-19 pandemic and beyond, Molecular Autism, 11, 1, (2020); El Ansari, W. A., Quality of research on community partnerships: Developing the evidence base, Health Education Research, 21, 2, pp. 175-180, (2006); Bauer, Mark S., An introduction to implementation science for the non-specialist, BMC Psychology, 3, 1, (2015); Becan, Jennifer Edwards, A model for rigorously applying the Exploration, Preparation, Implementation, Sustainment (EPIS) framework in the design and measurement of a large scale collaborative multi-site study, Health and Justice, 6, 1, (2018); Bernal, Guillermo, Ecological validity and cultural sensitivity for outcome research: Issues for the cultural adaptation and development of psychosocial treatments with Hispanics, Journal of Abnormal Child Psychology, 23, 1, pp. 67-82, (1995); Boisvert, Michelle, Telepractice in the assessment and treatment of individuals with autism spectrum disorders: A systematic review, Developmental Neurorehabilitation, 13, 6, pp. 423-432, (2010); Brookman-Frazee, Lauren I., Characterizing Shared and Unique Implementation Influences in Two Community Services Systems for Autism: Applying the EPIS Framework to Two Large-Scale Autism Intervention Community Effectiveness Trials, Administration and Policy in Mental Health and Mental Health Services Research, 47, 2, pp. 176-187, (2020)","","SAGE Publications Ltd","","","","","","13623613; 14617005","","AUTIF","39655488","English","Article","Final","All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85211507813"
"Guo, P.; Li, H.; Mo, X.","Guo, Peijun (59916888900); Li, Huan (57203391167); Mo, Xinyue (57200341079)","59916888900; 57203391167; 57200341079","Quantifying Post-Purchase Service Satisfaction: A Topic–Emotion Fusion Approach with Smartphone Data","2025","Big Data and Cognitive Computing","9","5","125","","","0","2","10.3390/bdcc9050125","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006545481&doi=10.3390%2Fbdcc9050125&partnerID=40&md5=0651886085654dab5dbef1d07731c350","School of Cyberspace Security (School of Cryptology), Hainan University, Haikou, China","Guo, Peijun, School of Cyberspace Security (School of Cryptology), Hainan University, Haikou, China; Li, Huan, School of Cyberspace Security (School of Cryptology), Hainan University, Haikou, China; Mo, Xinyue, School of Cyberspace Security (School of Cryptology), Hainan University, Haikou, China","Effectively identifying factors related to user satisfaction is crucial for evaluating customer experience. This study proposes a two-phase analytical framework that combines natural language processing techniques with hierarchical decision-making methods. In Phase 1, an ERNIE-LSTM-based emotion model (ELEM) is used to detect fake reviews from 4016 smartphone evaluations collected from JD.com (accuracy: 84.77%, recall: 84.86%, F1 score: 84.81%). The filtered genuine reviews are then analyzed using Biterm Topic Modeling (BTM) to extract key satisfaction-related topics, which are weighted based on sentiment scores and organized into a multi-criteria evaluation matrix through the Analytic Hierarchy Process (AHP). These topics are further clustered into five major factors: user-centered design (70.8%), core performance (10.0%), imaging features (8.6%), promotional incentives (7.8%), and industrial design (2.8%). This framework is applied to a comparative analysis of two smartphone stores, revealing that Huawei Mate 60 Pro emphasizes performance, while Redmi Note 11 5G focuses on imaging capabilities. Further clustering of user reviews identifies six distinct user groups, all prioritizing user-centered design and core performance, but showing differences in other preferences. In Phase 2, a comparison of word frequencies between product reviews and community Q and A content highlights hidden user concerns often missed by traditional single-source sentiment analysis, such as screen calibration and pixel density. These findings provide insights into how product design influences satisfaction and offer practical guidance for improving product development and marketing strategies. © 2025 Elsevier B.V., All rights reserved.","cross-domain text mining; deep learning; emotion detection; natural language processing; topic modeling; user satisfaction evaluation","Decision making; Integrated circuit layout; Intellectual property core; Sales; User experience; Cross-domain; Cross-domain text mining; Deep learning; Emotion detection; Language processing; Natural language processing; Natural languages; Text-mining; Topic Modeling; User satisfaction evaluations; Emotion Recognition","","","This work is supported by Hainan Provincial Natural Science Foundation of China (Grant number: 623RC455, 623RC457), Scientific Research Fund of Hainan University (Grant number: KYQD (ZR)-22096, KYQD (ZR)-22097).","Vakulenko, Yulia, Online retail experience and customer satisfaction: the mediating role of last mile delivery, International Review of Retail, Distribution and Consumer Research, 29, 3, pp. 306-320, (2019); Rita, Paulo Miguel Ferreira, The impact of e-service quality and customer satisfaction on customer behavior in online shopping, Heliyon, 5, 10, (2019); Chen, Yahui, Research on user generated content in Q&A system and online comments based on text mining, Alexandria Engineering Journal, 61, 10, pp. 7659-7668, (2022); China Bus Mark, (2020); Forestry Economics, (2019); Xu, Xun, Examining the role of emotion in online consumer reviews of various attributes in the surprise box shopping model, Decision Support Systems, 136, (2020); Chen, Tao, The Impact of Online Reviews on Consumers’ Purchasing Decisions: Evidence From an Eye-Tracking Study, Frontiers in Psychology, 13, (2022); Xu, Xun, Time matters: Investigating the asymmetric reflection of online reviews on customer satisfaction and recommendation across temporal lenses, International Journal of Information Management, 75, (2024); Management Review, (2017); Management World, (2009)","","Multidisciplinary Digital Publishing Institute (MDPI)","","","","","","25042289","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-105006545481"
"Stillhart, A.; Häfliger, R.; Takeshita, L.; Stadlinger, B.; Leles, C.R.; Srinivasan, M.","Stillhart, Angela (57202985302); Häfliger, Rahel (59665417000); Takeshita, Lisa (58911216800); Stadlinger, Bernd (23490810700); Leles, Cláudio R. (16202922300); Srinivasan, Murali (10043433800)","57202985302; 59665417000; 58911216800; 23490810700; 16202922300; 10043433800","Screening for dental pain using an automated face coding (AFC) software","2025","Journal of Dentistry","155","","105647","","","0","0","10.1016/j.jdent.2025.105647","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219512752&doi=10.1016%2Fj.jdent.2025.105647&partnerID=40&md5=a11f65b8a32c539169777715c089bae4","Special Care and Geriatric Dentistry, Universität Zürich, Zurich, Switzerland; Clinic of Cranio-Maxillofacial and Oral Surgery, Universität Zürich, Zurich, Switzerland; Universidade Federal de Goiás, Goiania, Brazil","Stillhart, Angela, Special Care and Geriatric Dentistry, Universität Zürich, Zurich, Switzerland; Häfliger, Rahel, Special Care and Geriatric Dentistry, Universität Zürich, Zurich, Switzerland; Takeshita, Lisa, Special Care and Geriatric Dentistry, Universität Zürich, Zurich, Switzerland; Stadlinger, Bernd, Clinic of Cranio-Maxillofacial and Oral Surgery, Universität Zürich, Zurich, Switzerland; Leles, Cláudio R., Special Care and Geriatric Dentistry, Universität Zürich, Zurich, Switzerland, Universidade Federal de Goiás, Goiania, Brazil; Srinivasan, Murali, Special Care and Geriatric Dentistry, Universität Zürich, Zurich, Switzerland","Objectives: This observational study evaluated the effectiveness of an Automated Face Coding (AFC) software in identifying facial expressions related to dental pain. Methods: Fifty-seven participants (49.8 ± 17.1 years) with symptoms of dental pain were recruited. Participants self-reported their pain using a Visual Analog Scale (VAS) score and their faces were filmed using a smartphone. The video clips were exported to an AFC software, which analyzed the facial expressions. The analysis focused on detecting changes in facial expressions and emotional states. The analysis was performed at two timepoints, at baseline (on the first visit), and at post treatment recall when pain was alleviated (self-reported). Non-parametric tests were used for statistical analysis (p < 0.05). Results: Significant reduction in pain levels was observed between the first visit and at the post treatment recall visit (mean VAS: baseline = 5.65 ± 2.08, recall = 0.40 ± 0.80; p < 0.001). No significant gender differences were observed in pain scores (p > 0.05). Significant differences in facial expressions between the two time points was not detected by the software (p > 0.05). Emotional parameters remained stable. Conclusion: The findings of this study concluded that the current capability of the AFC software to detect changes in facial expressions specific to pain alleviation is limited, even though it can provide detailed analysis of facial muscle movements. Further research is needed to enhance the software's sensitivity to pain-related expressions and explore its integration with other diagnostic tools for improved patient care and treatment outcomes. Clinical Significance Statement: The study explored the potential of AFC software in analyzing facial expressions for applications in screening and diagnosis of dental problems especially in non-communicative geriatric patients. While effective in monitoring facial movements, the software's current limitations in detecting pain-specific changes underscore the need for further advancements. © 2025 Elsevier B.V., All rights reserved.","AI radiomics; Artificial intelligence; Automated face coding; Dental pain; Emotions; Facial expressions; Geriatric dentistry; Gerodontology; Special care dentistry","adult; aged; diagnosis; emotion; facial expression; female; human; male; middle aged; pain measurement; procedures; self report; smartphone; software; tooth pain; videorecording; young adult; Adult; Aged; Emotions; Facial Expression; Female; Humans; Male; Middle Aged; Pain Measurement; Self Report; Smartphone; Software; Toothache; Video Recording; Young Adult","","","","Bekendam, Maria T., Facial Expressions of Emotions During Pharmacological and Exercise Stress Testing: the Role of Myocardial Ischemia and Cardiac Symptoms, International Journal of Behavioral Medicine, 28, 6, pp. 692-704, (2021); Arango-De-Montis, Iván, Automatic detection of facial expressions during the Cyberball paradigm in Borderline Personality Disorder: a pilot study, Frontiers in Psychiatry, 15, (2024); Martin, Elizabeth A., Behavioral meaures of psychotic disorders: Using automatic facial coding to detect nonverbal expressions in video, Journal of Psychiatric Research, 176, pp. 9-17, (2024); Burgess, Romana, Quantifying the efficacy of an automated facial coding software using videos of parents, Frontiers in Psychology, 14, (2023); Shepelenko, Anna Yu, The relationship between charitable giving and emotional facial expressions: Results from affective computing, Heliyon, 10, 2, (2024); Zhu, Agnes Q., Defining Standard Values for FaceReader Facial Expression Software Output, Aesthetic Plastic Surgery, 48, 5, pp. 785-792, (2024); de Meulemeester, Celine, Do My Emotions Show or Not? Problems With Transparency Estimation in Women With Borderline Personality Disorder Features, Personality Disorders: Theory, Research, and Treatment, 13, 3, pp. 288-299, (2021); de Meulemeester, Celine, “FEELING INVISIBLE”: INDIVIDUALS WITH BORDERLINE PERSONALITY DISORDER UNDERESTIMATE THE TRANSPARENCY OF THEIR EMOTIONS, Journal of Personality Disorders, 37, 2, pp. 213-232, (2023); Obayashi, Yota, Quantitative evaluation of facial expression in a patient with minimally conscious state after severe traumatic brain injury, Journal of Head Trauma Rehabilitation, 36, 5, pp. E337-E344, (2021); Dupré, Damien, A performance comparison of eight commercially available automatic classifiers for facial affect recognition, PLOS ONE, 15, 4, (2020)","","Elsevier Ltd","","","","","","03005712","","JDENA","39993552","English","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85219512752"
"Zhang, J.-J.; Gao, Y.; Zhang, B.-L.; Wu, D.-D.","Zhang, Jiajin (55763931600); Gao, Yu (59172997900); Zhang, Baolin (56313605100); Wu, Dongdong (24475747200)","55763931600; 59172997900; 56313605100; 24475747200","A deep learning lightweight model for real-time captive macaque facial recognition based on an improved YOLOX model","2025","Zoological Research","46","2","","339","354","0","1","10.24272/j.issn.2095-8137.2024.296","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000689349&doi=10.24272%2Fj.issn.2095-8137.2024.296&partnerID=40&md5=091f019c1666640fd0a07489fd21df87","Key Laboratory of Genetic Evolution & Animal Models, Kunming Institute of Zoology Chinese Academy of Sciences, Kunming, China; Yunnan Agricultural University, Kunming, China; Yunnan Agricultural University, Kunming, China; National Resource Center for Non-Human Primates, Kunming Institute of Zoology Chinese Academy of Sciences, Kunming, China; Yunnan Key Laboratory of Biodiversity Information, Kunming, China; Center for Excellence in Animal Evolution and Genetics, Chinese Academy of Sciences, Beijing, China","Zhang, Jiajin, Key Laboratory of Genetic Evolution & Animal Models, Kunming Institute of Zoology Chinese Academy of Sciences, Kunming, China, Yunnan Agricultural University, Kunming, China; Gao, Yu, Yunnan Agricultural University, Kunming, China; Zhang, Baolin, Key Laboratory of Genetic Evolution & Animal Models, Kunming Institute of Zoology Chinese Academy of Sciences, Kunming, China, National Resource Center for Non-Human Primates, Kunming Institute of Zoology Chinese Academy of Sciences, Kunming, China, Yunnan Key Laboratory of Biodiversity Information, Kunming, China; Wu, Dongdong, Key Laboratory of Genetic Evolution & Animal Models, Kunming Institute of Zoology Chinese Academy of Sciences, Kunming, China, National Resource Center for Non-Human Primates, Kunming Institute of Zoology Chinese Academy of Sciences, Kunming, China, Center for Excellence in Animal Evolution and Genetics, Chinese Academy of Sciences, Beijing, China","自动化监测猕猴行为对推动生物医学研究和提升动物福利具有重要意义。然而，在群体环境中实现对个体猕猴的可靠识别仍然是一个亟待解决的挑战。该研究提出了一种针对圈养猕猴的轻量化面部识别模型——ACE-YOLOX。该模型基于YOLOX框架，融合了高效通道注意力（ECA）、完全交并比损失（CIoU）以及自适应空间特征融合（ASFF）技术，从而有效提升了多尺度特征的捕获能力，同时显著提高了预测精度并降低了计算复杂度。在包含1 196只猕猴的179 400张标注面部图像的数据集上，ACE-YOLOX的表现优于经典目标检测模型，并具备实时处理能力。为验证其实用性，我们进一步开发了一款安卓应用，将ACE-YOLOX部署于智能手机，实现了设备端的实时识别。实验结果表明，ACE-YOLOX作为一种非侵入性猕猴识别工具，具有显著的应用潜力，为未来猕猴面部表情识别、认知心理学和社会行为研究奠定了基础。.; Automated behavior monitoring of macaques offers transformative potential for advancing biomedical research and animal welfare. However, reliably identifying individual macaques in group environments remains a significant challenge. This study introduces ACE-YOLOX, a lightweight facial recognition model tailored for captive macaques. ACE-YOLOX incorporates Efficient Channel Attention (ECA), Complete Intersection over Union loss (CIoU), and Adaptive Spatial Feature Fusion (ASFF) into the YOLOX framework, enhancing prediction accuracy while reducing computational complexity. These integrated approaches enable effective multiscale feature extraction. Using a dataset comprising 179 400 labeled facial images from 1 196 macaques, ACE-YOLOX surpassed the performance of classical object detection models, demonstrating superior accuracy and real-time processing capabilities. An Android application was also developed to deploy ACE-YOLOX on smartphones, enabling on-device, real-time macaque recognition. Our experimental results highlight the potential of ACE-YOLOX as a non-invasive identification tool, offering an important foundation for future studies in macaque facial expression recognition, cognitive psychology, and social behavior. This record is sourced from MEDLINE/PubMed, a database of the U.S. National Library of Medicine","Animal welfare; Facial recognition; Identity recognition; Macaque; YOLOX","animal; deep learning; facial recognition; Macaca; physiology; Animals; Deep Learning; Facial Recognition","","","","","","","","","","","","20958137; 02545853","","","40049662","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-86000689349"
"Nukathati, R.K.; Nagella, U.B.; Siva Kumar, A.P.","Nukathati, Ranjit Kumar (55834787600); Nagella, Uday Bhaskar (59703847800); Siva Kumar, A. P. (57200088190)","55834787600; 59703847800; 57200088190","A Deep Learning Framework with Optimizations for Facial Expression and Emotion Recognition from Videos","2025","International Journal of Electrical and Computer Engineering Systems","16","3","","217","229","0","1","10.32985/ijeces.16.3.3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000627791&doi=10.32985%2Fijeces.16.3.3&partnerID=40&md5=b78bfd13eb6f0b0e5a0386b643bfa101","Department of Computer Science and Engineering, Jawaharlal Nehru Technological University Anantapur, Anantapur, India; Government College, Anantapur, Anantapur, India","Nukathati, Ranjit Kumar, Department of Computer Science and Engineering, Jawaharlal Nehru Technological University Anantapur, Anantapur, India; Nagella, Uday Bhaskar, Government College, Anantapur, Anantapur, India; Siva Kumar, A. P., Department of Computer Science and Engineering, Jawaharlal Nehru Technological University Anantapur, Anantapur, India","Human emotion recognition has many real-time applications in healthcare and psychology domains. Due to the widespread usage of smartphones, large volumes of video content are being produced. A video can have both audio and video frames in the form of images. With the advancements in Artificial Intelligence (AI), there has been significant improvement in the development of computer vision applications.Accuracy in recognizing human emotions from given audio-visual content is a very challenging problem. However, with the improvements in deep learning techniques,analyzing audio-visual content towards emotion recognition is possible. The existing deep learning methods focused on audio content or video frames for emotion recognition. An integrated approach consisting of audio and video frames in a single framework is needed to leverage efficiency. This paper proposes a deep learning framework with specific optimizations for facial expression and emotion recognition from videos. We proposed an algorithm, Learning Human Emotion Recognition (LbHER), which exploits hybrid deep learning models that could process audio and video frames toward emotion recognition. Our empirical study with a benchmark dataset, IEMOCAP, has revealed that the proposed framework and the underlying algorithm could leverage state-of-the-art human emotion recognition. Our experimental results showed that the proposed algorithm outperformed many existing models with the highest average accuracy of 94.66%. Our framework can be integrated into existing computer vision applications to recognize emotions from videos automatically. © 2025 Elsevier B.V., All rights reserved.","Artificial Intelligence; Deep Learning; Emotion Recognition; Hyperparameter Tuning; Spatial Expression Analysis","","","","","Zhang, Jianhua, Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review, Information Fusion, 59, pp. 103-126, (2020); Villegas-Ch, William Eduardo, Identification of Emotions From Facial Gestures in a Teaching Environment With the Use of Machine Learning Techniques, IEEE Access, 11, pp. 38010-38022, (2023); Alvarez Casado, Constantino, Depression Recognition Using Remote Photoplethysmography From Facial Videos, IEEE Transactions on Affective Computing, 14, 4, pp. 3305-3316, (2023); Hassouneh, Aya, Development of a Real-Time Emotion Recognition System Using Facial Expressions and EEG based on machine learning and deep neural network methods, Informatics in Medicine Unlocked, 20, (2020); Pise, Anil Audumbar, Facial emotion recognition using temporal relational network: an application to E-learning, Multimedia Tools and Applications, 81, 19, pp. 26633-26653, (2022); Patel, Keyur, Facial Sentiment Analysis Using AI Techniques: State-of-the-Art, Taxonomies, and Challenges, IEEE Access, 8, pp. 90495-90519, (2020); Bazgir, Omid, Emotion Recognition with Machine Learning Using EEG Signals, (2018); Guo, Jianzhu, Dominant and Complementary Emotion Recognition from Still Images of Faces, IEEE Access, 6, pp. 26391-26403, (2018); Diamantini, Claudia, Automatic annotation of corpora for emotion recognition through facial expressions analysis, Proceedings - International Conference on Pattern Recognition, pp. 5650-5657, (2020); Michael Revina, I., A Survey on Human Face Expression Recognition Techniques, Journal of King Saud University - Computer and Information Sciences, 33, 6, pp. 619-628, (2021)","","J.J. Strossmayer University of Osijek , Faculty of Electrical Engineering, Computer Science and Information Technology","","","","","","18476996; 18477003","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-105000627791"
"Matsouliadis, L.; Siamtanidou, E.; Vryzas, N.; Dimoulas, C.","Matsouliadis, Lazaros (59707426500); Siamtanidou, Eleni (57222623607); Vryzas, Nikolaos (57090797900); Dimoulas, Charalampos A. (57105254400)","59707426500; 57222623607; 57090797900; 57105254400","Speech Emotion Recognition and Serious Games: An Entertaining Approach for Crowdsourcing Annotated Samples","2025","Information (Switzerland)","16","3","238","","","0","0","10.3390/info16030238","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000988164&doi=10.3390%2Finfo16030238&partnerID=40&md5=7b95ba041ffaaf39cbf802c52ede083e","School of Applied Arts and Sustainable Design, Hellenic Open University, Patra, Greece; Multidisciplinary Media & Mediated Communication Research Group (M3C), Aristotle University of Thessaloniki, Thessaloniki, Greece","Matsouliadis, Lazaros, School of Applied Arts and Sustainable Design, Hellenic Open University, Patra, Greece; Siamtanidou, Eleni, Multidisciplinary Media & Mediated Communication Research Group (M3C), Aristotle University of Thessaloniki, Thessaloniki, Greece; Vryzas, Nikolaos, Multidisciplinary Media & Mediated Communication Research Group (M3C), Aristotle University of Thessaloniki, Thessaloniki, Greece; Dimoulas, Charalampos A., Multidisciplinary Media & Mediated Communication Research Group (M3C), Aristotle University of Thessaloniki, Thessaloniki, Greece","Computer games have emerged as valuable tools for education and training. In particular, serious games, which combine learning with entertainment, offer unique potential for engaging users and enhancing knowledge acquisition. This paper presents a case study on the design, development, and evaluation of two serious games, “Silent Kingdom” and “Job Interview Simulator”, created using Unreal Engine 5 and incorporating speech emotion recognition (SER) technology. Through a systematic analysis of the existing research in SER and game development, these games were designed to elicit a wide range of emotion responses from player and collect voice data for the enhancement of SER models. By evaluating player engagement, emotional expression, and overall user experience, this study investigates the effectiveness of serious games in collecting speech data and creating more immersive player experiences. The research also explores the technical limitations of SER integration within game environments in real-time, as well as its impact on player enjoyment. Although there are some technology limitations due to the latency provided for real-time SER analysis, the results reveal that a properly developed game with integrated SER technology could become a more engaging and efficient tool for crowdsourcing speech data. © 2025 Elsevier B.V., All rights reserved.","artificial intelligence; human–computer interaction; semantic analysis and classification of sounds; speech emotion recognition","Emotion Recognition; Game design; Knowledge acquisition; Semantics; Speech enhancement; Speech recognition; Case-studies; Computer interaction; Design development; Education and training; Real- time; Semantic analysis; Semantic analyze and classification of sound; Semantic classification; Speech data; Speech emotion recognition; Crowdsourcing","","","","Hudlická, Eva, To feel or not to feel: The role of affect in human-computer interaction, International Journal of Human Computer Studies, 59, 1-2, pp. 1-32, (2003); Vryzas, Nikolaos, Speech emotion recognition for performance interaction, AES: Journal of the Audio Engineering Society, 66, 6, pp. 457-467, (2018); Vryzas, Nikolaos, Web radio automation for audio stream management in the Era of big data, Information (Switzerland), 11, 4, (2020); Kotsakis, Rigas G., Investigation of spoken-language detection and classification in broadcasted audio content, Information (Switzerland), 11, 4, (2020); Vryzas, Nikolaos, A Prototype Web Application to Support Human-Centered Audiovisual Content Authentication and Crowdsourcing, Future Internet, 14, 3, (2022); Strategy Dev Rev, (2019); Hamari, Juho J., Why do people use gamification services?, International Journal of Information Management, 35, 4, pp. 419-431, (2015); Triantoro, Tamilla Mavlanova, Would you like to play? A comparison of a gamified survey with a traditional online survey method, International Journal of Information Management, 49, pp. 242-252, (2019); Hall, Amanda K., Health Benefits of Digital Videogames for Older Adults: A Systematic Review of the Literature, Games for Health Journal, 1, 6, pp. 402-410, (2012); Res J Appl Sci, (2015)","","Multidisciplinary Digital Publishing Institute (MDPI)","","","","","","20782489","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-105000988164"
"Siamtanidou, E.; Vrysis, L.; Vryzas, N.; Dimoulas, C.","Siamtanidou, Eleni (57222623607); Vrysis, Lazaros (57090900500); Vryzas, Nikolaos (57090797900); Dimoulas, Charalampos A. (57105254400)","57222623607; 57090900500; 57090797900; 57105254400","Gamified Engagement for Data Crowdsourcing and AI Literacy: An Investigation in Affective Communication Through Speech Emotion Recognition","2025","Societies","15","3","54","","","0","1","10.3390/soc15030054","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000981749&doi=10.3390%2Fsoc15030054&partnerID=40&md5=8906da03296d242abe7b1087cb5629dc","Multidisciplinary Media & Mediated Communication Research Group (M3C), Aristotle University of Thessaloniki, Thessaloniki, Greece","Siamtanidou, Eleni, Multidisciplinary Media & Mediated Communication Research Group (M3C), Aristotle University of Thessaloniki, Thessaloniki, Greece; Vrysis, Lazaros, Multidisciplinary Media & Mediated Communication Research Group (M3C), Aristotle University of Thessaloniki, Thessaloniki, Greece; Vryzas, Nikolaos, Multidisciplinary Media & Mediated Communication Research Group (M3C), Aristotle University of Thessaloniki, Thessaloniki, Greece; Dimoulas, Charalampos A., Multidisciplinary Media & Mediated Communication Research Group (M3C), Aristotle University of Thessaloniki, Thessaloniki, Greece","This research investigates the utilization of entertainment approaches, such as serious games and gamification technologies, to address various challenges and implement targeted tasks. Specifically, it details the design and development of an innovative gamified application named “J-Plus”, aimed at both professionals and non-professionals in journalism. This application facilitates the enjoyable, efficient, and high-quality collection of emotionally tagged speech samples, enhancing the performance and robustness of speech emotion recognition (SER) systems. Additionally, these approaches offer significant educational benefits, providing users with knowledge about emotional speech and artificial intelligence (AI) mechanisms while promoting digital skills. This project was evaluated by 48 participants, with 44 engaging in quantitative assessments and 4 forming an expert group for qualitative methodologies. This evaluation validated the research questions and hypotheses, demonstrating the application’s diverse benefits. Key findings indicate that gamified features can effectively support learning and attract users, with approximately 70% of participants agreeing that serious games and gamification could enhance their motivation to practice and improve their emotional speech. Additionally, 50% of participants identified social interaction features, such as collaboration, as most beneficial for fostering motivation and commitment. The integration of these elements supports reliable and extensive data collection and the advancement of AI algorithms while concurrently developing various skills, such as emotional speech articulation and digital literacy. This paper advocates for the creation of collaborative environments and digital communities through crowdsourcing, balancing technological innovation in the SER sector. © 2025 Elsevier B.V., All rights reserved.","crowdsourcing; digital communities; digital literacy; gamification technologies; gamified applications; media literacy; serious games; speech emotion recognition systems","","","","","Impact of Artificial Intelligence in Business and Society, (2023); Tsvetkova, Milena, Understanding human-machine networks: A cross-disciplinary survey, ACM Computing Surveys, 50, 1, (2018); Lillywhite, Aspen, Coverage of artificial intelligence and machine learning within academic literature, canadian newspapers, and twitter tweets: The case of disabled people, Societies, 10, 1, (2020); Wu, Yuchen, Development and Application of Artificial Neural Network, Wireless Personal Communications, 102, 2, pp. 1645-1656, (2018); Understanding AI Technology, (2020); Impact of Artificial Intelligence in Business and Society, (2023); Gruson, Damien, Data science, artificial intelligence, and machine learning: Opportunities for laboratory medicine and the value of positive regulation, Clinical Biochemistry, 69, pp. 1-7, (2019); Hamet, Pavel, Artificial intelligence in medicine, Metabolism: Clinical and Experimental, 69, pp. S36-S40, (2017); Proceedings of the Ground Air Multisensor Interoperability Integration and Networking for Persistent ISR Ix; Lwakatare, Lucy Ellen, Large-scale machine learning systems in real-world industrial settings: A review of challenges and solutions, Information and Software Technology, 127, (2020)","","Multidisciplinary Digital Publishing Institute (MDPI)","","","","","","20754698","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-105000981749"
"Torabgar, M.; Figeys, M.; Esmail, S.; Stroulia, E.; Rios Rincón, A.M.","Torabgar, Melika (59674930600); Figeys, Mathieu (57202647007); Esmail, Shaniff (6602768547); Stroulia, Eleni (6603883706); Rios Rincón, Adriana María (57222010560)","59674930600; 57202647007; 6602768547; 6603883706; 57222010560","Machine Learning Analysis of Engagement Behaviors in Older Adults With Dementia Playing Mobile Games: Exploratory Study","2025","JMIR Serious Games","13","","e54797","","","0","0","10.2196/54797","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000362059&doi=10.2196%2F54797&partnerID=40&md5=5beb2fe7ea41526b179318e2e87e1721","Department of Occupational Therapy, University of Alberta, Edmonton, Canada; University of Alberta, Edmonton, Canada","Torabgar, Melika, Department of Occupational Therapy, University of Alberta, Edmonton, Canada; Figeys, Mathieu, Department of Occupational Therapy, University of Alberta, Edmonton, Canada; Esmail, Shaniff, Department of Occupational Therapy, University of Alberta, Edmonton, Canada; Stroulia, Eleni, University of Alberta, Edmonton, Canada; Rios Rincón, Adriana María, Department of Occupational Therapy, University of Alberta, Edmonton, Canada","Background: The prevalence of dementia is expected to rise with an aging population, necessitating accessible early detection methods. Serious games have emerged as potential cognitive screening tools. They provide not only an engaging platform for assessing cognitive function but also serve as valuable indicators of cognitive health through engagement levels observed during play. Objective: This study aims to examine the differences in engagement-related behaviors between older adults with and without dementia during serious gaming sessions. Further, it seeks to identify the key contributors that enhance the effectiveness of machine learning for dementia classification based on engagement-related behaviors. Methods: This was an exploratory proof-of-concept study. Over 8 weeks, 20 older adults, 6 of whom were living with dementia, were enrolled in a single-case design study. Participants played 1 of 4 “Vibrant Minds” serious games (Bejeweled, Whack-A-Mole, Mah-jong, and Word-Search) over 8 weeks (16 30-min sessions). Throughout the study, sessions were recorded to analyze engagement-related behaviors. This paper reports on the analysis of the engagement-related behaviors of 15 participants. The videos of these 15 participants (10 cognitively intact, 5 with dementia) were analyzed by 2 independent raters, individually annotating engagement-related behaviors at 15-second intervals using a coding system. This analysis resulted in 1774 data points categorized into 47 behavior codes, augmented by 54 additional features including personal characteristics, technical issues, and environmental factors. Each engagement-related behavior was compared between older adults living with dementia and older adults without dementia using the χ2 test with a 2×2 contingency table with a significance level of .05. Codes underwent one-hot encoding and were processed using random forest classifiers to distinguish between participant groups. Results: Significant differences in 64% of engagement-related behaviors were found between groups, notably in torso movements, voice modulation, facial expressions, and concentration. Including engagement-related behaviors, environmental disturbances, technical issues, and personal characteristics resulted in the best model for classifying cases of dementia correctly, achieving an F<inf>1</inf>-score of 0.91 (95% CI 0.851‐0.963) and an area under the receiver operating curve of 0.99 (95% CI 0.984‐1.000). Conclusions: Key features distinguishing between older adults with and without dementia during serious gameplay included torso, voice, facial, and concentration behaviors, as well as age. The best performing machine learning model identified included features of engagement-related behavios, environmental disturbances, technical challenges, and personal attributes. Engagement-related behaviors observed during serious gaming offer crucial markers for identifying dementia. Machine learning models that incorporate these unique behavioral markers present a promising, noninvasive approach for early dementia screening in a variety of settings. © 2025 Elsevier B.V., All rights reserved.","aging; Alzheimer disease; classification; cognition; cognitive; dementia; elderly; engagement; games; gaming; geriatric; gerontology; machine learning; older adult; screening","","","","This research was funded by Aging Gracefully across Environments using Technology to Support Wellness Networks of Centers of Excellence (AGE-WELL), Canada's technology and aging network, under grant AW-PP2019-PP4. The opinions expressed in this publication are those of the authors and do not necessarily reflect those of AGE-WELL.The authors attest that there was no use of generative artificial intelligence technology in generation of text, figures, or other information content in this manuscript.","Global Action Plan on the Public Health Response to Dementia 20172025, (2017); Lancet, (2017); Ford, Elizabeth M., Ethical issues when using digital biomarkers and artificial intelligence for the early detection of dementia, Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 13, 3, (2023); Tolks, Daniel, The Role of AI in Serious Games and Gamification for Health: Scoping Review, JMIR Serious Games, 12, 1, (2024); Bayat, Sayeh, GPS driving: a digital biomarker for preclinical Alzheimer disease, Alzheimer's Research and Therapy, 13, 1, (2021); Predicting Early Indicators of Cognitive Decline from Verbal Utterances, (2020); Mezrar, Samiha, Machine learning and Serious Game for the Early Diagnosis of Alzheimer’s Disease, Simulation and Gaming, 53, 4, pp. 369-387, (2022); Boletsis, Costas, Smartkuber: A Serious Game for Cognitive Health Screening of Elderly Players, Games for Health Journal, 5, 4, pp. 241-251, (2016); Karapapas, Christos, Mild cognitive impairment detection using machine learning models trained on data collected from serious games, Applied Sciences (Switzerland), 11, 17, (2021); Hookham, Geoffrey, A Systematic Review of the Definition and Measurement of Engagement in Serious Games, ACM International Conference Proceeding Series, (2019)","","JMIR Publications Inc.","","","","","","22919279","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-86000362059"
"Riyadi, S.; Rozan, N.; Damarjati, C.","Riyadi, Slamet (6503991450); Rozan, Naufal (59550978800); Damarjati, Cahya (57203037658)","6503991450; 59550978800; 57203037658","Mobile Computer-Assisted Application for Stress Detection Based on Facial Expression Using Modified Convolutional Neural Network","2025","International Journal of Computing and Digital Systems","17","1","","","","0","0","10.12785/ijcds/1571027149","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217484835&doi=10.12785%2Fijcds%2F1571027149&partnerID=40&md5=77fb89d91701f3614024a0aaa126d36d","Department of Information Technology, Universitas Muhammadiyah Yogyakarta, Yogyakarta, Indonesia","Riyadi, Slamet, Department of Information Technology, Universitas Muhammadiyah Yogyakarta, Yogyakarta, Indonesia; Rozan, Naufal, Department of Information Technology, Universitas Muhammadiyah Yogyakarta, Yogyakarta, Indonesia; Damarjati, Cahya, Department of Information Technology, Universitas Muhammadiyah Yogyakarta, Yogyakarta, Indonesia","In this challenging digital era, stress has become an inseparable part of daily life, affecting all ages. Although researchers have discussed stress detection extensively, there are few practical and accessible applications for users. This research aims to develop a mobile application utilizing a modified Convolutional Neural Network (CNN) for stress detection based on facial expression, thereby enabling more effective and efficient stress detection and management. The well-known CNN architectures, i.e., DenseNet201, MobileNetv2, and ResNet50, could have been more optimal for detecting stress from facial expressions. Hence, the CNN architectures are modified to enhance the accuracy of the task by adding dropout layers, Pooling2D, and ReLU Activation. The research was conducted through data collection, image pre-processing, training the model with the modified CNN architectures, and developing a mobile application for stress detection. With the modifications made, this research succeeded in increasing the model’s accuracy in detecting stress from facial expressions, where the modified DenseNet201 achieved the highest accuracy, from 75.90% to 77.83%. The mobile application can detect stress based on facial expression image obtained from file or camera. In addition to its primary functionality, the application also provides practical advice to users on managing stress, depending on their detected stress levels. These features aim to enhance user awareness and promote early stress management. In conclusion, using artificial intelligence technology, especially through modifying the CNN architecture, enhances the accuracy of stress detection from facial expressions, and the developed mobile application offers a practical and scalable solution. © 2025 Elsevier B.V., All rights reserved.","Architecture Modification; Convolutional Neural Network (CNN); Facial Expression; Mobile Application; Stress Detection","","","","I want to express my gratitude to the University of Muhammadiyah Yogyakarta for the financial support that has enabled the execution of this research. The financial assistance from the University of Muhammadiyah Yogyakarta provided essential resources for the research and encouraged me to pursue and achieve higher results. I also thank all co-authors who have contributed valuable suggestions, critiques, and improvements during the research process.","Analisis Hubungan Antara Tingkat Stres Dengan Status Gizi Siswa Sd Di Kota Palu, (2019); Buletin Psikologi, (2016); Jurnal Manajemen Asuhan Keperawatan, (2023); Kesadaran Diri Terhadap Stress Yang Dirasakan, (2020); Extraction of Facial Features as Indicators of Stress and Anxiety, (2015); Stres Kerja, (2018); Institute for Computer Sciences Social Informatics and Telecommunications Engineering Icst, (2014); Stress Detection in Daily Life Scenarios Using Smart Phones and Wearable Sensors A Survey, (2019); Facial Expression Recognition Via Enhanced Stress Convolution Neural Network for Stress Detection, (2022); Knowledge Engineering and Data Science, (2021)","","University of Bahrain","","","","","","2210142X","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85217484835"
"Alhussein, G.; AlKhodari, M.; Khandoker, A.H.; Hadjileontiadis, L.J.","Alhussein, Ghada M. (55636422900); AlKhodari, Mohanad (57195943569); Khandoker, A. H. (23091108200); Hadjileontiadis, Leontios J. (7004037926)","55636422900; 57195943569; 23091108200; 7004037926","Novel Speech-Based Emotion Climate Recognition in Peers' Conversations Incorporating Affect Dynamics and Temporal Convolutional Neural Networks","2025","IEEE Access","13","","","16752","16769","0","6","10.1109/ACCESS.2025.3529125","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215222679&doi=10.1109%2FACCESS.2025.3529125&partnerID=40&md5=b7062349254a3803c43db1cbe62b0fba","University of Oxford Medical Sciences Division, Oxford, United Kingdom; Department of Biomedical Engineering and Biotechnology, Khalifa University of Science and Technology, Abu Dhabi, United Arab Emirates; Department of Electrical & Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece","Alhussein, Ghada M., Department of Biomedical Engineering and Biotechnology, Khalifa University of Science and Technology, Abu Dhabi, United Arab Emirates; AlKhodari, Mohanad, University of Oxford Medical Sciences Division, Oxford, United Kingdom; Khandoker, A. H., Department of Biomedical Engineering and Biotechnology, Khalifa University of Science and Technology, Abu Dhabi, United Arab Emirates; Hadjileontiadis, Leontios J., Department of Biomedical Engineering and Biotechnology, Khalifa University of Science and Technology, Abu Dhabi, United Arab Emirates, Department of Electrical & Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece","Peers' conversation provides a domain of rich emotional information. The latter, apart from facial and gestural expressions, it is also naturally conveyed via peers' speech, contributing to the establishment of a dynamic emotion climate (EC) during their conversational interaction. Recognition of EC could provide an additional source in understating peers' social interaction and behavior on top of peers' actual conversational content.We propose a novel approach for speech-based EC recognition, termed AffECt, which combines peers' complex affect dynamics (AD) with deep features extracted from speech signals using Temporal Convolutional Neural Networks (TCNNs). AffECt was tested and cross-validated on data drawn from there open datasets, i.e., K-EmoCon, IEMOCAP, and SEWA, in terms of EC arousal/valence level classification. The experimental results have shown that AffECt achieves EC classification accuracy up to 83.3% and 80.2% for arousal and valence, respectively, clearly surpassing the results reported in the literature, exhibiting robust performance across different languages. Moreover, there is a distinct improvement when the AD are combined with the TCNN, compared to the baseline deep learning approaches. These results demonstrate the effectiveness of AffECt in speech-based EC recognition, paving the way for many applications, e.g., in patients' group therapy, negotiations, and emotion-aware mobile applications. © 2025 Elsevier B.V., All rights reserved.","AffECt; affect dynamics; arousal/valence; Conversational emotion climate recognition; mel-frequency cepstrum coefficients (MFCCs); temporal convolutional neural networks (TCNN)","Deep learning; Emotion Recognition; Speech communication; Speech recognition; Affect; Affect dynamic; Arousal/valence; Conversational emotion climate recognition; Conversational interaction; Convolutional neural network; Emotional information; Mel frequency cepstrum coefficients; Mel-frequency cepstrum coefficient; Temporal convolutional neural network; Convolutional neural networks","","","This work was supported by Khalifa University, Abu Dhabi, United Arab Emirates, under Award CIRA-2020-031.","Uskul, Ayse K., Emotions and Health, pp. 496-501, (2015); Frijda, Nico H., The laws of emotion, pp. 1-352, (2017); Our World in Data, (2025); Walker, Elizabeth Reisinger, Mortality in mental disorders and global disease burden implications a systematic review and meta-analysis, JAMA Psychiatry, 72, 4, pp. 334-341, (2015); Jacob, Agnes, Modelling speech emotion recognition using logistic regression and decision trees, International Journal of Speech Technology, 20, 4, pp. 897-905, (2017); Li, Pengcheng, An attention pooling based representation learning method for speech emotion recognition, Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH, 2018-September, pp. 3087-3091, (2018); 't Hart, Björn, Emotion in stories: Facial EMG evidence for both mental simulation and moral evaluation, Frontiers in Psychology, 9, APR, (2018); Xiefeng, Cheng, Heart sound signals can be used for emotion recognition, Scientific Reports, 9, 1, (2019); Tully, Phillip J., Negative emotions and quality of life six months after cardiac surgery: The dominant role of depression not anxiety symptoms, Journal of Behavioral Medicine, 32, 6, pp. 510-522, (2009); Landsman-Dijkstra, Jeanet J.A., The long-term lasting effectiveness on self-efficacy, attribution style, expression of emotions and quality of life of a body awareness program for chronic a-specific psychosomatic symptoms, Patient Education and Counseling, 60, 1, pp. 66-79, (2006)","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21693536","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85215222679"
"Najib, F.M.","Najib, Fatma M. (57188809179)","57188809179","Sign language interpretation using machine learning and artificial intelligence","2025","Neural Computing and Applications","37","2","","841","857","0","5","10.1007/s00521-024-10395-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209365434&doi=10.1007%2Fs00521-024-10395-9&partnerID=40&md5=5cebee77c858dce93b67b0f6f5c7732b","Faculty of Computer and Information Sciences, Cairo, Egypt","Najib, Fatma M., Faculty of Computer and Information Sciences, Cairo, Egypt","Sign language is the only way for deaf and mute people to represent their needs and feelings. Most of non-deaf-mute people do not understand sign language, which leads to many difficulties for deaf-mutes' communication in their social life. Sign language interpretation systems and applications get a lot of attention in the recent years. In this paper, we review sign language recognition and interpretation studies based on machine learning, image processing, artificial intelligence, and animation tools. The two reverse processes for sign language interpretation are illustrated. This study discusses the recent research on sign language translation to text and speech with the help of hand gestures, facial expressions interpretation, and lip reading. Also, state of the art in speech to sign language translation is discussed. In addition, some of the popular and highly rated Android and Apple mobile applications that facilitate disabled people communication are presented. This paper clarifies and highlights the recent research and real used applications for deaf-mute people help. This paper tries to provide a link between research proposals and real applications. This link can help covering any gap or non-handled functionalities in the real used applications. Based on our study, we introduce a proposal involves set of functionalities/options that separately introduced and discussed by the recent research studies. These recent research directions should be integrated for achieving more real help. Also, a set of non-addressed research directions are suggested for future focus. © 2025 Elsevier B.V., All rights reserved.","Deaf-mutes; Hand gestures; Lip reading; Sign language animation; Sign language recognition","Gesture recognition; Machine learning; Translation (languages); Deaf-mute; Hand gesture; Language translation; Lip reading; Machine-learning; Recent researches; Sign language; Sign language animation; Sign Language recognition; Social life; Animation","","","","International Journal of Applied Engineering Research, (2018); Mishra, Shubham Kr, Recognition of Hand Gestures and Conversion of Voice for Betterment of Deaf and Mute People, Communications in Computer and Information Science, 1046, pp. 46-57, (2019); Hand Gesture Detection and Conversion to Speech and Text, (2018); Support for Communication with Deaf and Dumb Patients Via Few Shot Machine Learning; Tolentino, Lean Karlo, Static sign language recognition using deep learning, International Journal of Machine Learning and Computing, 9, 6, pp. 821-827, (2019); Saleem, Muhammad Imran, A Novel Machine Learning Based Two-Way Communication System for Deaf and Mute, Applied Sciences (Switzerland), 13, 1, (2023); International Journal of Advanced Trends in Computer Science and Engineering, (2020); International Journal of Innovative Science and Research Technology, (2022); Journal of Pharmaceutical Negative Results, (2023); An Android App for Deaf and Dumb People Sign Language Recognitionsmart Talk App","","Springer Science and Business Media Deutschland GmbH","","","","","","14333058; 09410643","","","","English","Review","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85209365434"
"Özyurt, H.; Özyurt, O.","Özyurt, Hacer (54783311800); Özyurt, Özcan (35614751000)","54783311800; 35614751000","Decoding educational augmented reality research trends: a topic modeling analysis","2025","Education and Information Technologies","30","1","103647","57","87","0","1","10.1007/s10639-024-12943-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200894156&doi=10.1007%2Fs10639-024-12943-1&partnerID=40&md5=de5df1a00073e5c94c19bd208876c5c5","Department of Software Engineering, Karadeniz Technical University, Trabzon, Turkey","Özyurt, Hacer, Department of Software Engineering, Karadeniz Technical University, Trabzon, Turkey; Özyurt, Özcan, Department of Software Engineering, Karadeniz Technical University, Trabzon, Turkey","This study aims to examine the temporal evolution and changes of research interests and trends in the educational augmented reality (AR) literature. To this end, 3718 articles published in the 2003–2022 period and indexed in the Scopus database were analyzed through machine learning-based semantic topic modeling and descriptive analysis. The findings indicate a notable upswing in studies on educational AR, particularly since 2015. The articles were categorized into eleven primary themes through topic modeling analysis. The three most prevalent topics in terms of volume are “Augmented Reality in Education and Cultural Heritage”, “Medical Education and Patient Care”, and “Enhancing Safety and Information in Food Consumption”. Observations across different times reveal that “Augmented Reality in Electrical and Electronic Systems” and “Gesture-Based Instruction and Maintenance” were studied in the initial periods. Since 2015, there has been a notable increase in applications falling under the “Serious Games” category. The least voluminous and slowest-evolving topics are identified as “Serious Games for Children with Autism Spectrum Disorder”, “Augmented Reality in Chemistry and Biology Laboratories”, and “Augmented Reality for Safe and Efficient Driving”. Considering the recent momentum gained by these topics, it is anticipated that they will become popular topics for future studies. This study represents a significant milestone as the first and most comprehensive research using machine learning in its field, not only explaining the current state of the field but also providing valuable information for future research efforts. © 2025 Elsevier B.V., All rights reserved.","Augmented reality; Data science application in education; Machine learning analysis; Temporal evolution of research; Topic modeling","","","","Open access funding provided by the Scientific and Technological Research Council of T\u00FCrkiye (T\u00DCB\u0130TAK).","Akçayir, Murat, Advantages and challenges associated with augmented reality for education: A systematic review of the literature, Educational Research Review, 20, pp. 1-11, (2017); Ali, Imran, Mapping research on healthcare operations and supply chain management: a topic modelling-based literature review, Annals of Operations Research, 315, 1, pp. 29-55, (2022); Arıcı, Faruk, Research trends in the use of augmented reality in science education: Content and bibliometric mapping analysis, Computers and Education, 142, (2019); History of Mobile Augmented Reality, (2015); Journal of Educational Technology and Instruction, (2023); Asmussen, Claus Boye, Smart literature review: a practical topic modelling approach to exploratory literature review, Journal of Big Data, 6, 1, (2019); Contemporary Educational Technology, (2021); Ayaz, Ahmet, Exploring Gamification Research Trends Using Topic Modeling, IEEE Access, 11, pp. 119676-119692, (2023); Azuma, Ronald T., A survey of augmented reality, Presence: Teleoperators and Virtual Environments, 6, 4, pp. 355-385, (1997); Azuma, Ronald T., Recent advances in augmented reality, IEEE Computer Graphics and Applications, 21, 6, pp. 34-47, (2001)","","Springer","","","","","","13602357","","","","English","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85200894156"
"Farzana, W.; Sarker, F.; Chau, T.; Al Mamun, K.A.","Farzana, Walia (57208386534); Sarker, Farhana (24448599500); Chau, Tom (57210396406); Al Mamun, Khandakar Abdulla (57923621700)","57208386534; 24448599500; 57210396406; 57923621700","Technological Evolvement in AAC Modalities to Foster Communications of Verbally Challenged ASD Children: A Systematic Review","2025","IEEE Access","13","","","122943","122959","0","12","10.1109/ACCESS.2021.3055195","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100448012&doi=10.1109%2FACCESS.2021.3055195&partnerID=40&md5=763186767ef7fe4d3548af3472170496","Department of Computer Science and Engineering, United International University, Dhaka, Bangladesh; Department of Computer Science and Engineering, University of Liberal Arts Bangladesh, Dhaka, Bangladesh; Holland Bloorview Kids Rehabilitation Hospital, Toronto, Canada","Farzana, Walia, Department of Computer Science and Engineering, United International University, Dhaka, Bangladesh; Sarker, Farhana, Department of Computer Science and Engineering, University of Liberal Arts Bangladesh, Dhaka, Bangladesh; Chau, Tom, Holland Bloorview Kids Rehabilitation Hospital, Toronto, Canada; Al Mamun, Khandakar Abdulla, Department of Computer Science and Engineering, United International University, Dhaka, Bangladesh","Augmentative and Alternative Communication (AAC) emerged as a combination of methods or strategies that constitute any device, such as Speech Generating Device (SGD), Program (mobile applications), Procedure (PECS, Picture Exchange Communication System), which enhances individual’s communication ability. Autism Spectrum Disorder (ASD) is a spectrum of comprehensive neurodevelopment disorder that leads to speech impairments, repetitive behavior, and social communication difficulties; therefore, it is imperative to underscore that at the core of all impediments are communication impairment. This article represents a systematic review of research initiatives that investigate multi-modal AAC strategies and functionality, features of mobile applications to reinforce communication and communal skills in verbally challenged ASD children because other researches are focused only on low or high-tech AAC or interventions to provide insights on ASD children respond to a particular approach. Following the PRISMA method, a total of 60 (January 2015 to October 2020) research articles were reviewed, indexed by Springer, Science Direct, Scopus, ACM, IEEE databases, and published in the AAC journal. The selected research articles are categorized into different themes where most of them focused on interactive mobile applications to improve emotional, social, learning, and overall communication skills in verbally challenged ASD children. This systematic review provides an outline of the paradigm shift in AAC modalities from PECS to Artificial Intelligence (AI), Machine Learning (ML), and Augmented Reality (AR) based applications. It opens up underline future opportunities to integrate intelligent analytics features in mobile applications to strengthen communication skills in verbally undermined ASD children. © 2025 Elsevier B.V., All rights reserved.","artificial intelligence (AI); Augmentative and alternative communication (AAC); augmented reality (AR); autism spectrum disorder (ASD); machine learning (ML); mobile applications","Application programs; Artificial intelligence; Diseases; Human rehabilitation engineering; Learning systems; Mobile computing; Speech communication; Alternative communication; Augmentative-and-alternative communication; Autism; Autism spectrum disorders; Machine learning; Machine-learning; Mobile applications; Systematic; Augmented reality","","","","Elsabbagh, M., Global Prevalence of Autism and Other Pervasive Developmental Disorders, Autism Research, 5, 3, pp. 160-179, (2012); Chiarotti, Flavia, Epidemiology of autism spectrum disorders: A review of worldwide prevalence estimates since 2014, Brain Sciences, 10, 5, (2020); Hossain, Mohammad Didar, Autism Spectrum disorders (ASD) in South Asia: A systematic review, BMC Psychiatry, 17, 1, (2017); Comprehensive Guide to Autism, (2014); Communication Problems in Children with Autism Spectrum Disorder, (2012); Wodka, Ericka L., Predictors of phrase and fluent speech in children with Autism and severe language delay, Pediatrics, 131, 4, pp. e1128-e1134, (2013); Augmentative and Alternative Communication Knowledge and Skills for Service Delivery, (2002); Procedia Soc Behav Sci, (2014); Shane, Howard C., Applying technology to visually support language and communication in individuals with autism spectrum disorders, Journal of Autism and Developmental Disorders, 42, 6, pp. 1228-1235, (2012); Perspectives on Augmentative and Alternative Communication, (2011)","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21693536","","","","English","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85100448012"
"Çiftçi, S.; Karci, H.; Karaca, N.; Söylemez, B.; Koçakoğlu, H.","Çiftçi, Serdar (57191406554); Karci, Hasari (20433782800); Karaca, Neval (58876439400); Söylemez, Büşra (58656150200); Koçakoğlu, Halil (60086438200)","57191406554; 20433782800; 58876439400; 58656150200; 60086438200","Emotion Analysis of Children’s Drawings","2025","IEEE Access","13","","","159730","159748","0","0","10.1109/ACCESS.2025.3606359","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015203856&doi=10.1109%2FACCESS.2025.3606359&partnerID=40&md5=6ce814b9af41f9f7d7fba839185d3c1b","Department of Computer Engineering, Harran Üniversitesi, Sanliurfa, Turkey; Department of Electrical and Electronic Engineering, Harran Üniversitesi, Sanliurfa, Turkey; Vasıf Çınar Primary School, Izmir, Turkey","Çiftçi, Serdar, Department of Computer Engineering, Harran Üniversitesi, Sanliurfa, Turkey; Karci, Hasari, Department of Electrical and Electronic Engineering, Harran Üniversitesi, Sanliurfa, Turkey; Karaca, Neval, Department of Computer Engineering, Harran Üniversitesi, Sanliurfa, Turkey; Söylemez, Büşra, Department of Computer Engineering, Harran Üniversitesi, Sanliurfa, Turkey; Koçakoğlu, Halil, Vasıf Çınar Primary School, Izmir, Turkey","This study investigates children’s emotional expression through the computational analysis of their drawings. Prior research has frequently relied on proprietary or small-scale datasets, often limited to digital drawings produced on tablets or smartphones, which may not fully capture the natural characteristics of hand-drawn expression. To address these gaps, we introduce KIDO, a novel publicly available dataset of scanned children’s drawings paired with reflections written by the children on whether their drawings represent happiness or sadness. The dataset contains contributions from 5,430 students, with balanced representation across primary and secondary education levels and additional metadata on gender and educational background. For our experiments, we employed CNN-based and Vision Transformer–based models for emotion recognition, obtaining F1-scores of up to 79.03% from drawings, while BERT-based models achieved accuracies of up to 97.47% in classifying children’s written reflections. We further trained a generative model on the KIDO dataset to synthesize emotionally expressive drawings, with subjective evaluations indicating that children often preferred the AI-generated images to those drawn by their peers. KIDO serves as a comprehensive benchmark for advancing research in the fields of computer vision, natural language processing, psychology, and education, and is publicly available at github.com/serdarciftci/KIDO © 2025 Elsevier B.V., All rights reserved.","Children’s emotion analysis; classification of children’s drawings; classification of children’s thoughts; KIDO dataset","Behavioral research; Education computing; Emotion Recognition; Psychology computing; Speech recognition; Child’s emotion analyze; Classification of child’s drawing; Classification of child’s thought; Digital drawings; Emotion analysis; Emotional expressions; Hand-drawn; KIDO dataset; Scanned drawings; Smart phones; Classification (of information)","","","This work was supported in part by the Harran University under Grant 22010. The authors extend their gratitude to the Ministry of National Education of the Republic of T\u00FCrkiye (MEB) and Harran University for their permissions (44451523, 88748, respectively) and valuable contributions in facilitating the collection of children\u2019s drawings and accompanying texts. They would also like to thank Prof. Cahit Kaya for helping translate Turkish texts into English and Berfin Kurto\u011Flu for helping with writing texts. The numerical calculations reported in this article were fully/partially performed at TUBITAK ULAKBIM, High Performance and Grid Computing Center (TRUBA resources).","Wittmann, Barbara, A Neolithic childhood: Children’s drawings as prehistoric sources, Res: Anthropology and Aesthetics, 63-64, pp. 125-142, (2013); Fan, Judith E., Drawing as a versatile cognitive tool, Nature Reviews Psychology, 2, 9, pp. 556-568, (2023); Ballús, Elisabeth, Children’s drawings as a projective tool to explore and prevent experiences of mistreatment and/or sexual abuse, Frontiers in Psychology, 14, (2023); Baird, Sarah J., Identifying psychological trauma among Syrian refugee children for early intervention: Analyzing digitized drawings using machine learning, Journal of Development Economics, 156, (2022); Lev-Ẃiesel, Rachel, Social peer rejection as reflected in drawings and narratives: To what extent does it reflect actual experience? A pilot study, Arts in Psychotherapy, 76, (2021); Bosacki, Sandra Leanne, Voices from the classroom: Pictorial and narrative representations of children's bullying experiences, Journal of Moral Education, 35, 2, pp. 231-245, (2006); Farokhi, Masoumeh, The analysis of children's drawings: Social, emotional, physical, and psychological aspects, Procedia - Social and Behavioral Sciences, 30, pp. 2219-2224, (2011); Lee, Anthony C., Drawing self and others: How do children with autism differ from those with learning difficulties?, British Journal of Developmental Psychology, 24, 3, pp. 547-565, (2006); Schlesier, Juliane, How Do Primary and Early Secondary School Students Report Dealing with Positive and Negative Achievement Emotions in Class? A Mixed-Methods Approach, Education Sciences, 14, 6, (2024); Simoes Lourêiro, Kevin, Analyzing Drawings to Explore children’s Concepts of an Ideal School: Implications for the Improvement of children’s Well-Being at School, Child Indicators Research, 13, 4, pp. 1387-1411, (2020)","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21693536","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-105015203856"
"Keinert, M.; Pistrosch, S.; Mallol-Ragolta, A.; Schuller, B.W.; Berking, M.","Keinert, Marie (57990462700); Pistrosch, Simon (59972448600); Mallol-Ragolta, Adria (57204679167); Schuller, Björn W. (6603767415); Berking, Matthias (10039087200)","57990462700; 59972448600; 57204679167; 6603767415; 10039087200","Facial Emotion Recognition of 16 Distinct Emotions From Smartphone Videos: Comparative Study of Machine Learning and Human Performance","2025","Journal of Medical Internet Research","27","","e68942","","","0","0","10.2196/68942","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009613184&doi=10.2196%2F68942&partnerID=40&md5=9713b488c479ac43eb726aaaf1d94c87","Department of Clinical Psychology and Psychotherapy, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; CHI – Chair of Health Informatics, Technische Universität München, Munich, Germany; Munich Center for Machine Learning, Munich, Germany; Munich Data Science Institute, Munich, Germany; Audio, Imperial College London, London, United Kingdom","Keinert, Marie, Department of Clinical Psychology and Psychotherapy, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Pistrosch, Simon, CHI – Chair of Health Informatics, Technische Universität München, Munich, Germany, Munich Center for Machine Learning, Munich, Germany; Mallol-Ragolta, Adria, CHI – Chair of Health Informatics, Technische Universität München, Munich, Germany, Munich Center for Machine Learning, Munich, Germany; Schuller, Björn W., CHI – Chair of Health Informatics, Technische Universität München, Munich, Germany, Munich Center for Machine Learning, Munich, Germany, Munich Data Science Institute, Munich, Germany, Audio, Imperial College London, London, United Kingdom; Berking, Matthias, Department of Clinical Psychology and Psychotherapy, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany","Background: The development of automatic emotion recognition models from smartphone videos is a crucial step toward the dissemination of psychotherapeutic app interventions that encourage emotional expressions. Existing models focus mainly on the 6 basic emotions while neglecting other therapeutically relevant emotions. To support this research, we introduce the novel Stress Reduction Training Through the Recognition of Emotions Wizard-of-Oz (STREs WoZ) dataset, which contains facial videos of 16 distinct, therapeutically relevant emotions. Objective: This study aimed to develop deep learning–based automatic facial emotion recognition (FER) models for binary (positive vs negative) and multiclass emotion classification tasks, assess the models’ performance, and validate them by comparing the models with human observers. Methods: The STREs WoZ dataset contains 14,412 facial videos of 63 individuals displaying the 16 emotions. The selfie-style videos were recorded during a stress reduction training using front-facing smartphone cameras in a nonconstrained laboratory setting. Automatic FER models using both appearance and deep-learned features for binary and multiclass emotion classification were trained on the STREs WoZ dataset. The appearance features were based on the Facial Action Coding System and extracted with OpenFace. The deep-learned features were obtained through a ResNet50 model. For our deep learning models, we used the appearance features, the deep-learned features, and their concatenation as inputs. We used 3 recurrent neural network (RNN)–based architectures: RNN-convolution, RNN-attention, and RNN-average networks. For validation, 3 human observers were also trained in binary and multiclass emotion recognition. A test set of 3018 facial emotion videos of the 16 emotions was completed by both the automatic FER model and human observers. The performance was assessed with unweighted average recall (UAR) and accuracy. Results: Models using appearance features outperformed those using deep-learned features, as well as models combining both feature types in both tasks, with the attention network using appearance features emerging as the best-performing model. The attention network achieved a UAR of 92.9% in the binary classification task, and accuracy values ranged from 59.0% to 90.0% in the multiclass classification task. Human performance was comparable to that of the automatic FER model in the binary classification task, with a UAR of 91.0%, and superior in the multiclass classification task, with accuracy values ranging from 87.4% to 99.8%. Conclusions: Future studies are needed to enhance the performance of automatic FER models for practical use in psychotherapeutic apps. Nevertheless, this study represents an important first step toward advancing emotion-focused psychotherapeutic interventions via smartphone apps. © 2025 Elsevier B.V., All rights reserved.","16 distinct emotions; deep learning; facial emotion recognition; smartphone video; validation","adult; anger; Article; attention network; binary classification; cognitive restructuring; comparative study; convolutional neural network; courage; deep learning; disgust; emotion; excitement; facial expression; facial recognition; fear; feature extraction; female; gated recurrent unit network; happiness; human; human experiment; love; machine learning; male; multiclass classification; positive valence; residual neural network; sadness; support vector machine; university student; videorecording; young adult; artificial neural network; smartphone; Adult; Deep Learning; Emotions; Facial Expression; Female; Humans; Machine Learning; Male; Neural Networks, Computer; Smartphone; Video Recording","","","The authors extend their sincere thanks to all the participants involved in the data collection, whose contributions were essential to building the dataset. The authors also express their gratitude to Dr Stephanie B\u00F6hme for organizing and supervising the data collection and Dr Stefan Gradl for developing the smartphone app that made it possible. In addition, the authors appreciate the efforts of Verena Butscher, Sanja Kostic, and Mark Ebenhack for their role as human observers in annotating the videos. This work was funded by the Bavarian Ministry of Science and Arts (as part of the Bavarian Research Association on Healthy Use of Digital Technologies and Media [ForDigitHealth]) and the German Research Foundation (SFB 1483\u2014Project-ID 442419336, EmpkinS). The funders were not involved in the study design; the collection, analysis, and interpretation of data; in the writing of the report; and in the decision to submit the manuscript for publication.","Handbook of Personality Theory and Research, (1999); Affect Regulation Training, (2014); Buchanan, Roderick D., Psychotherapy, pp. 468-494, (2019); Peluso, Paul R., Therapist and Client Emotional Expression and Psychotherapy Outcomes: A Meta-Analysis, Psychotherapy, 55, 4, pp. 461-472, (2018); Kohn, Robert, Mental health in the Americas: an overview of the treatment gap, Revista Panamericana de Salud Publica/Pan American Journal of Public Health, 42, (2018); Weisel, Kiona Krueger, Standalone smartphone apps for mental health—a systematic review and meta-analysis, npj Digital Medicine, 2, 1, (2019); Ekman, Paul, Are there basic emotions?, Psychological Review, 99, 3, pp. 550-553, (1992); Russell, James A., A circumplex model of affect, Journal of Personality and Social Psychology, 39, 6, pp. 1161-1178, (1980); Keltner, Dacher J., Emotional Expression: Advances in Basic Emotion Theory, Journal of Nonverbal Behavior, 43, 2, pp. 133-160, (2019); Fitzpatrick, Marilyn R., Integrating Positive Emotions Into Theory, Research, and Practice: A New Challenge for Psychotherapy, Journal of Psychotherapy Integration, 18, 2, pp. 248-258, (2008)","","JMIR Publications Inc.","","","","","","14388871","","","40601921","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-105009613184"
"Videla, R.; Parada, F.J.; Aros, M.B.; Ramírez, P.; Sarzosa, A.; Jorquera, D.; Palma, C.; Trujillo, A.; Ibacache, D.; González, M.J.; Velásquez, A.; Molina, K.; Barraza, M.; Kausel, L.","Videla, Ronnie (58505170000); Parada, Francisco J. (24741159000); Aros, Maybritt (58505195400); Ramírez, Paola Domínguez (57555223200); Sarzosa, Alexis (59955076200); Jorquera, Daniela (59664137600); Palma, Camila (59955654900); Trujillo, Alline (59955368200); Ibacache, David (59665125100); González, María Jesús (59665542100); Velásquez, Andri (59955946800); Molina, Karelia (59955801800); Barraza, Marco (59955655000); Kausel, Leonie K. (57195304437)","58505170000; 24741159000; 58505195400; 57555223200; 59955076200; 59664137600; 59955654900; 59955368200; 59665125100; 59665542100; 59955946800; 59955801800; 59955655000; 57195304437","Breaking barriers in education: leveraging 3E approach and technology to foster inclusion for SEN students","2025","Frontiers in Education","10","","1554314","","","0","0","10.3389/feduc.2025.1554314","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008657126&doi=10.3389%2Ffeduc.2025.1554314&partnerID=40&md5=68e235640b555bc81614de30e010267f","Escuela de Educación Diferencial, Universidad Santo Tomas, Santiago, Santiago, Chile; HUB EDUCA STEAM, La Serena, Chile; Facultad de Psicología, Universidad Diego Portales, Santiago, Chile; Escuela de Educación, Universidad Católica del Norte, Antofagasta, Chile; Facultad de Ciencias Básicas, Universidad Católica del Maule, Talca, Chile; Colegio Antonio Varas, Vicuna, Chile; Carrera de Pedagogía en Educación Física, Universidad Santo Tomas, Santiago, Santiago, Chile","Videla, Ronnie, Escuela de Educación Diferencial, Universidad Santo Tomas, Santiago, Santiago, Chile, HUB EDUCA STEAM, La Serena, Chile; Parada, Francisco J., Facultad de Psicología, Universidad Diego Portales, Santiago, Chile; Aros, Maybritt, Escuela de Educación, Universidad Católica del Norte, Antofagasta, Chile; Ramírez, Paola Domínguez, Facultad de Ciencias Básicas, Universidad Católica del Maule, Talca, Chile; Sarzosa, Alexis, Colegio Antonio Varas, Vicuna, Chile; Jorquera, Daniela, HUB EDUCA STEAM, La Serena, Chile, Escuela de Educación, Universidad Católica del Norte, Antofagasta, Chile; Palma, Camila, Colegio Antonio Varas, Vicuna, Chile; Trujillo, Alline, HUB EDUCA STEAM, La Serena, Chile; Ibacache, David, Escuela de Educación Diferencial, Universidad Santo Tomas, Santiago, Santiago, Chile; González, María Jesús, HUB EDUCA STEAM, La Serena, Chile, Escuela de Educación, Universidad Católica del Norte, Antofagasta, Chile; Velásquez, Andri, Carrera de Pedagogía en Educación Física, Universidad Santo Tomas, Santiago, Santiago, Chile; Molina, Karelia, Colegio Antonio Varas, Vicuna, Chile; Barraza, Marco, HUB EDUCA STEAM, La Serena, Chile; Kausel, Leonie K., Facultad de Psicología, Universidad Diego Portales, Santiago, Chile","Advances in embodied design highlight the interconnection between the brain, body, and environment in shaping learning experiences, emphasizing the potential of multimodal perception in enriched sociomaterial and technological contexts. Despite growing evidence in this area, traditional educational systems remain anchored in transmissive and cognitivist models, perpetuating barriers to equitable learning, particularly for students with special educational needs (SEN), such as Attention Deficit Hyperactivity Disorder (ADHD), Autism Spectrum Disorder (ASD), and Specific Learning Disabilities (SLD). In response to these challenges, post-cognitivist frameworks, including embodied, enactive, and environmentally scaffolded cognition (3E) as well as SpEED (Special Education Embodied Design), provide essential foundations for rethinking inclusive educational design. This perspective article explores the role of active teaching methodologies and innovative technologies in shaping a holistic framework to support inclusive learning design. This framework leverages immersive technologies, mobile applications, and artificial intelligence to scaffold multimodal learning experiences that meet the diverse needs of students with SEN. Through a proof-of-concept based on illustrative cases, this article presents a theoretical contribution to guide the design of inclusive and technology-supported learning environments aligned with the 3E approach, fostering engagement, equity, and personalized learning for students with SEN. © 2025 Elsevier B.V., All rights reserved.","3E approach; active methodologies; digital technologies; embodied design; inclusive education; special educational needs","","","","","Designing Constructionist Futures the Art Theory and Practice of Learning Designs, (2020); Handbook of Digital Resources in Mathematics Education, (2023); Aguayo, Claudio, Entangled cognition in immersive learning experience, Adaptive Behavior, 31, 5, pp. 497-515, (2023); Diagnostic and Statistic Manual of Mental Disorders, (2013); Acts of Meaning, (1990); Universal Design for Learning Guidelines Version 1 0, (2008); Promoting Inclusive Education for Diverse Societies A Conceptual Framework, (2021); Cisek, Paul E., An Evolutionary Perspective on Embodiment, pp. 547-571, (2021); Criscuolo, Antonio, Cognition through the lens of a body–brain dynamic system, Trends in Neurosciences, 45, 9, pp. 667-677, (2022); Danielson, Melissa L., Prevalence of Parent-Reported ADHD Diagnosis and Associated Treatment Among U.S. Children and Adolescents, 2016, Journal of Clinical Child and Adolescent Psychology, 47, 2, pp. 199-212, (2018)","","Frontiers Media SA","","","","","","2504284X","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-105008657126"
"Rupp, L.H.; Kumar, A.; Sadeghi, M.; Schindler-Gmelch, L.; Keinert, M.; Eskofier, B.M.; Berking, M.","Rupp, Lydia Helene (57220050972); Kumar, Akash (59896401300); Sadeghi, Misha (58759115500); Schindler-Gmelch, Lena (58759159500); Keinert, Marie (57990462700); Eskofier, Bjoern M. (26428080900); Berking, Matthias (10039087200)","57220050972; 59896401300; 58759115500; 58759159500; 57990462700; 26428080900; 10039087200","Stress can be detected during emotion-evoking smartphone use: a pilot study using machine learning","2025","Frontiers in Digital Health","7","","1578917","","","0","1","10.3389/fdgth.2025.1578917","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005117387&doi=10.3389%2Ffdgth.2025.1578917&partnerID=40&md5=bfa99d16de22e7764e51566630ae3fd0","Lehrstuhl für Klinische Psychologie und Psychotherapie, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Machine Learning and Data Analytics Lab, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Translational Digital Health Group, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany","Rupp, Lydia Helene, Lehrstuhl für Klinische Psychologie und Psychotherapie, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Kumar, Akash, Machine Learning and Data Analytics Lab, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Sadeghi, Misha, Machine Learning and Data Analytics Lab, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Schindler-Gmelch, Lena, Lehrstuhl für Klinische Psychologie und Psychotherapie, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Keinert, Marie, Lehrstuhl für Klinische Psychologie und Psychotherapie, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Eskofier, Bjoern M., Machine Learning and Data Analytics Lab, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany, Translational Digital Health Group, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Berking, Matthias, Lehrstuhl für Klinische Psychologie und Psychotherapie, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany","Introduction: The detrimental consequences of stress highlight the need for precise stress detection, as this offers a window for timely intervention. However, both objective and subjective measurements suffer from validity limitations. Contactless sensing technologies using machine learning methods present a potential alternative and could be used to estimate stress from externally visible physiological changes, such as emotional facial expressions. Although previous studies were able to classify stress from emotional expressions with accuracies of up to 88.32%, most works employed a classification approach and relied on data from contexts where stress was induced. Therefore, the primary aim of the present study was to clarify whether stress can be detected from facial expressions of six basic emotions (anxiety, anger, disgust, sadness, joy, love) and relaxation using a prediction approach. Method: To attain this goal, we analyzed video recordings of facial emotional expressions collected from n = 69 participants in a secondary analysis of a dataset from an interventional study. We aimed to explore associations with stress (assessed by the PSS-10 and a one-item stress measure). Results: Comparing two regression machine learning models [Random Forest (RF) and XGBoost], we found that facial emotional expressions were promising indicators of stress scores, with model fit being best when data from all six emotional facial expressions was used to train the model (one-item stress measure: MSE (XGB) = 2.31, MAE (XGB) = 1.32, MSE (RF) = 3.86, MAE (RF) = 1.69; PSS-10: MSE (XGB) = 25.65, MAE (XGB) = 4.16, MSE (RF) = 26.32, MAE (RF) = 4.14). XGBoost showed to be more reliable for prediction, with lower error for both training and test data. Discussion: The findings provide further evidence that non-invasive video recordings can complement standard objective and subjective markers of stress. © 2025 Elsevier B.V., All rights reserved.","automated stress recognition; emotion; emotion expression; machine learning; stress","adult; algorithm; anger; anxiety; Article; bootstrapping; demographics; disgust; emotion assessment; emotion evoking; facial expression; female; happiness; human; intervention study; Likert scale; love; machine learning; major clinical study; male; physiological stress; pilot study; random forest; sadness; training","","","The author(s) declare that financial support was received for the research and/or publication of this article. The study presented is part of the research project \u201COptimierung von Apps zur St\u00E4rkung der psychischen Gesundheit [Optimizing Apps to Strengthen Mental Health]\u201D that is part of the Bavarian Research Association on Healthy Use of Digital Technologies and Media (ForDigitHealth) and is funded by the Bavarian Ministry of Science and Arts. This study was (partly) funded by the German Research Foundation (Deutsche Forschungsgemeinschaft, DFG) - SFB 1483 \u2013 Project-ID 442419336, EmpkinS.","Stress Appraisal and Coping, (1984); Allen, Andrew Paul, The Trier Social Stress Test: Principles and practice, Neurobiology of Stress, 6, pp. 113-126, (2017); Rohleder, Nicolas, Stress and inflammation – The need to address the gap in the transition between acute and chronic stress effects, Psychoneuroendocrinology, 105, pp. 164-171, (2019); Cohen, Sheldon A., A global measure of perceived stress., Journal of Health and Social Behavior, 24, 4, pp. 385-396, (1983); Nederhof, Anton J., Methods of coping with social desirability bias: A review, European Journal of Social Psychology, 15, 3, pp. 263-280, (1985); Greenleaf, Eric A., Measuring extreme response style, Public Opinion Quarterly, 56, 3, pp. 328-351, (1992); Adam, Emma K., Assessing salivary cortisol in large-scale, epidemiological research, Psychoneuroendocrinology, 34, 10, pp. 1423-1436, (2009); Hellhammer, Dirk Helmut, Salivary cortisol as a biomarker in stress research, Psychoneuroendocrinology, 34, 2, pp. 163-171, (2009); Stalder, Tobias, Assessment of the cortisol awakening response: Expert consensus guidelines, Psychoneuroendocrinology, 63, pp. 414-432, (2016); Mishra, Varun, Evaluating the reproducibility of physiological stress detection models, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 4, 4, (2020)","","Frontiers Media SA","","","","","","2673253X","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-105005117387"
"Koumpouros, Y.","Koumpouros, Y. (6505509970)","6505509970","Digital Horizons: Enhancing Autism Support with Augmented Reality","2025","Journal of Autism and Developmental Disorders","","","","","","0","0","10.1007/s10803-024-06709-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002243995&doi=10.1007%2Fs10803-024-06709-4&partnerID=40&md5=6f314abd9d9895effbc82c452a147e00","Department of Public and Community Health, University of West Attica, Athens, Greece","Koumpouros, Y., Department of Public and Community Health, University of West Attica, Athens, Greece","Purpose: This paper aims to comprehensively review the application of Augmented Reality interventions in supporting individuals diagnosed with Autism Spectrum Disorder. The main research question guiding this review is: What effects do AR interventions have on various aspects of functioning in individuals with ASD? Methods: A systematic review methodology was employed to analyze 49 articles published between 2013 and 2023. These articles were selected based on their relevance to AR interventions for individuals with ASD. The review examines the diverse technological landscape of AR interventions, the various platforms utilized, and the effectiveness of different AR techniques. Results: Findings reveal the prevalence of smartphones, tablets, smart glasses, and head-mounted displays as primary platforms for AR interventions, with positive outcomes reported across various domains including social interaction skills, communication abilities, and academic performance. Marker-based, superimposition-based, and projection-based AR techniques demonstrate potential in creating personalized and engaging experiences tailored to the unique needs of individuals with ASD. Conclusion: Despite progress in communication and social skills interventions, gaps remain in understanding and addressing attention-related issues and emotion recognition. The review underscores the need for more rigorous study designs and objective evaluation methods to ascertain the efficacy of AR interventions for individuals with ASD. Looking ahead, collaborative efforts between researchers, developers, practitioners, and individuals with ASD are crucial for advancing innovation, addressing limitations, and ensuring the meaningful integration of AR technology into interventions aimed at enhancing the quality of life for individuals on the autism spectrum. Further exploration and utilization of the latest advancements in artificial intelligence and affective computing are warranted to develop solutions that effectively address real-world challenges faced by individuals with autism. © 2025 Elsevier B.V., All rights reserved.","ADHD; AR; ASD; Augmented Reality; Autism; Autism Spectrum Disorder; Disability; Mixed Reality; Virtual Reality","","","","","Ahsen, Tooba, Designing a Customizable Picture-Based Augmented Reality Application for Therapists and Educational Professionals Working in Autistic Contexts, (2022); Alkadhi, Bushra, Co-design of Augmented Reality Storybooks for Children with Autism Spectrum Disorder, Lecture Notes in Computer Science, 12426 LNCS, pp. 3-13, (2020); Lazo-Amado, Misael, Prototype of an augmented reality application for cognitive improvement in children with autism using the DesingScrum methodology, Advances in Science, Technology and Engineering Systems, 6, 1, pp. 587-596, (2021); Diagnostic Statistical Manual of Mental Disorders, (2013); Innovative Data Communication Technologies and Application, (2025); Bakir, Çiçek Nur, Use of augmented reality in mental health-related conditions: A systematic review, Digital Health, 9, (2023); Bauer, Valentin, Designing an Extended Reality Application to Expand Clinic-Based Sensory Strategies for Autistic Children Requiring Substantial Support: Participation of Practitioners, pp. 254-259, (2021); Berenguer, Carmen, Exploring the Impact of Augmented Reality in Children and Adolescents with Autism Spectrum Disorder: A Systematic Review, International Journal of Environmental Research and Public Health, 17, 17, pp. 1-15, (2020); Bouaziz, Rahma, Using Marker Based Augmented Reality to teach autistic eating skills, pp. 239-242, (2020); Bouaziz, Rahma, Enhancing Daily Life Skills Learning for Children with ASD Through Augmented Reality, Lecture Notes on Data Engineering and Communications Technologies, 72, pp. 1164-1173, (2021)","","Springer","","","","","","15733432; 01623257","","JADDD","","English","Article","aip","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-105002243995"
"Takami, M.; Nishiyama, D.; Tsutsui, S.; Nagata, K.; Ishimoto, Y.; Oda, K.; Iwasaki, H.; Hashizume, H.; Yamada, H.","Takami, Masanari (7102741078); Nishiyama, Daisuke (56814903300); Tsutsui, Shunji (26434955000); Nagata, Keiji (54581330700); Ishimoto, Yuyu (55097638500); Oda, Kotaro (57218175187); Iwasaki, Hiroshi (58368475800); Hashizume, Hiroshi (7102076842); Yamada, H. (58409042400)","7102741078; 56814903300; 26434955000; 54581330700; 55097638500; 57218175187; 58368475800; 7102076842; 58409042400","Pelvic Kinematics during Gait Following Long-Segment Spinal Fusion Due to Adult Spinal Deformity: An Analysis Using a Smartphone-Based Inertial Measurement Unit","2025","Spine Surgery and Related Research","9","2","","188","194","0","0","10.22603/ssrr.2024-0119","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001384899&doi=10.22603%2Fssrr.2024-0119&partnerID=40&md5=b9641aa1609e751bf2b1a8d98df8ed63","Department of Orthopaedic Surgery, Wakayama Medical University, Wakayama, Japan","Takami, Masanari, Department of Orthopaedic Surgery, Wakayama Medical University, Wakayama, Japan; Nishiyama, Daisuke, Department of Orthopaedic Surgery, Wakayama Medical University, Wakayama, Japan; Tsutsui, Shunji, Department of Orthopaedic Surgery, Wakayama Medical University, Wakayama, Japan; Nagata, Keiji, Department of Orthopaedic Surgery, Wakayama Medical University, Wakayama, Japan; Ishimoto, Yuyu, Department of Orthopaedic Surgery, Wakayama Medical University, Wakayama, Japan; Oda, Kotaro, Department of Orthopaedic Surgery, Wakayama Medical University, Wakayama, Japan; Iwasaki, Hiroshi, Department of Orthopaedic Surgery, Wakayama Medical University, Wakayama, Japan; Hashizume, Hiroshi, Department of Orthopaedic Surgery, Wakayama Medical University, Wakayama, Japan; Yamada, H., Department of Orthopaedic Surgery, Wakayama Medical University, Wakayama, Japan","Introduction: Gait changes could occur after thoracic to pelvic long-segment corrective fusion surgery, a common procedure for adult spinal deformity (ASD), potentially affecting the occurrence and progression of postoperative hip osteoarthritis. We aimed to clarify postoperative pelvic kinematics in patients with ASD by performing gait analysis using a system based on a smartphone-integrated inertial measurement unit (IMU). Methods: A total of 21 consecutive outpatients (73.6±4.6 years old, 2 men, 19 women) were enrolled. All had undergone long-segment fusion from the thoracic spine to the pelvis for ASD more than 1 year previously and could walk unassisted. A control group comprised 20 healthy volunteers. The IMU was fixed on the sacrum, and data were collected when subjects walked forward on a flat indoor floor. Acceleration in three axial directions and angular velocity around the three axes were recorded simultaneously during gait, and data were cut out for each gait cycle. Of 1043 features obtained, the top 20 features with the smallest p-value in a statistical comparison were selected. These features, plus gender and age, were classified using gradient boosting machine learning based on the decision tree algorithm. The classification accuracy and relative importance of the feature items were calculated. Results: The accuracy rate for gait classification between groups was 96.7% and the F1-score was 0.968. The factor that contributed most to the classification of gait in both groups was “y-angular,_change_quantiles,_f_agg=”var”,_isabs=True,_qh =0.6,_ql=0.2,” which means the variance of the change of the absolute value in the pelvic rotation angular velocity in the horizontal plane in the range of 20%-60% of the gait cycle. Its relative importance was 0.351, which was smaller in the group with fusion. Conclusions: Patients with ASD following long-segment fusion from the thoracic spine to the pelvis apparently have a gait style characterized by suppressed pelvic rotation in the horizontal plane. © 2025 Elsevier B.V., All rights reserved.","Adult spinal deformity; Gait analysis; Hip osteoarthritis; Long-segment spinal fusion surgery; Pelvic kinematics; Pelvic rotation; Smartphone-based inertial measurement unit","accuracy; adult spinal deformity; age; aged; Article; classification; clinical article; comparative study; controlled study; decision tree; female; gait; gender; human; kinematics; lumbar lordosis angle; male; Oswestry Disability Index; outpatient; pelvic incidence; retrospective study; rotation; sacral slope; sacrum; sagittal vertical axis; spine fusion; spine malformation; thoracic spine; velocity; visual analog scale","","","We would like to thank Yukako Hashimoto and Ayaka Shimazaki for their work in our laboratory. We acknowledge proofreading and editing by Benjamin Phillis at the Clinical Study Support Center at Wakayama Medical University.","Quarto, Emanuele, Adult spinal deformity surgery: posterior three-column osteotomies vs anterior lordotic cages with posterior fusion. Complications, clinical and radiological results. A systematic review of the literature, European Spine Journal, 30, 11, pp. 3150-3161, (2021); Schmitt, Paul J., Long-segment fusion for adult spinal deformity correction using low-dose recombinant human bone morphogenetic protein-2: A retrospective review of fusion rates, Neurosurgery, 79, 2, pp. 212-221, (2016); Watanabe, Kei, Significance of long corrective fusion to the ilium for physical function in patients with adult spinal deformity, Journal of Orthopaedic Science, 26, 6, pp. 962-967, (2021); Lee, Chongsuh, Long-term benefits of appropriately corrected sagittal alignment in reconstructive surgery for adult spinal deformity: Evaluation of clinical outcomes and mechanical failures, Journal of Neurosurgery: Spine, 34, 3, pp. 390-398, (2021); Kyrölä, Kati Kristiina, Long-term clinical and radiographic outcomes and patient satisfaction after adult spinal deformity correction, Scandinavian Journal of Surgery, 108, 4, pp. 343-351, (2019); Hart, Robert A.A., Lumbar Stiffness Disability Index: Pilot testing of consistency, reliability, and validity, Spine Journal, 13, 2, pp. 157-161, (2013); Togawa, Daisuke, Postoperative disability after long corrective fusion to the pelvis in elderly patients with spinal deformity, Spine, 43, 14, pp. E804-E812, (2018); Ishikawa, Yoshinori, Activities of daily living and patient satisfaction after long fusion for adult spinal deformity: a retrospective study, European Spine Journal, 28, 7, pp. 1670-1677, (2019); Park, Sejun, Factors Affecting Stiffness-Related Functional Disability After Long Segmental Fusion for Adult Spinal Deformity, Neurosurgery, 91, 5, pp. 756-763, (2022); Kozaki, Takuhei, Adjacent segment disease on hip joint as a complication of spinal fusion surgery including sacroiliac joint fixation, European Spine Journal, 30, 5, pp. 1314-1319, (2021)","","Japanese Society for Spine Surgery and Related Research","","","","","","2432261X","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-105001384899"
"Immel, D.; Hilpert, B.; Schwarz, P.; Hein, A.; Gebhard, P.; Barton, S.; Hurlemann, R.","Immel, Diana (59710714300); Hilpert, Bernhard (57213424869); Schwarz, Patricia (58743284600); Hein, Andreas (56529792300); Gebhard, Patrick (7004257059); Barton, Simon (59177043700); Hurlemann, René (9742914500)","59710714300; 57213424869; 58743284600; 56529792300; 7004257059; 59177043700; 9742914500","Patients’ and Health Care Professionals’ Expectations of Virtual Therapeutic Agents in Outpatient Aftercare: Qualitative Survey Study","2025","JMIR Formative Research","9","","e59527","","","0","0","10.2196/59527","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001136240&doi=10.2196%2F59527&partnerID=40&md5=ac376e523c257b2d2a2a9898a8fa28c6","Department of Psychiatry, Universität Oldenburg, Oldenburg, Germany; Affective Computing Group, German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; Universiteit Leiden, Leiden, Netherlands; Department of Computer Science, Vrije Universiteit Amsterdam, Amsterdam, Netherlands; Assistance Systems and Medical Device Technology, Universität Oldenburg, Oldenburg, Germany","Immel, Diana, Department of Psychiatry, Universität Oldenburg, Oldenburg, Germany; Hilpert, Bernhard, Affective Computing Group, German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany, Universiteit Leiden, Leiden, Netherlands, Department of Computer Science, Vrije Universiteit Amsterdam, Amsterdam, Netherlands; Schwarz, Patricia, Assistance Systems and Medical Device Technology, Universität Oldenburg, Oldenburg, Germany; Hein, Andreas, Assistance Systems and Medical Device Technology, Universität Oldenburg, Oldenburg, Germany; Gebhard, Patrick, Affective Computing Group, German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; Barton, Simon, Department of Psychiatry, Universität Oldenburg, Oldenburg, Germany; Hurlemann, René, Department of Psychiatry, Universität Oldenburg, Oldenburg, Germany","Background: Depression is a serious mental health condition that can have a profound impact on the individual experiencing the disorder and those providing care. While psychotherapy and medication can be effective, there are gaps in current approaches, particularly in outpatient care. This phase is often associated with a high risk of relapse and readmission, and patients often report a lack of support. Socially interactive agents represent an innovative approach to the provision of assistance. Often powered by artificial intelligence, these virtual agents can interact socially and elicit humanlike emotions. In health care, they are used as virtual therapeutic assistants to fill gaps in outpatient aftercare. Objective: We aimed to explore the expectations of patients with depression and health care professionals by conducting a qualitative survey. Our analysis focused on research questions related to the appearance and role of the assistant, the assistant-patient interaction (time of interaction, skills and abilities of the assistant, and modes of interaction) and the therapist-assistant interaction. Methods: A 2-part qualitative study was conducted to explore the perspectives of the 2 groups (patients and care providers). In the first step, care providers (n=30) were recruited during a regional offline meeting. After a short presentation, they were given a link and were asked to complete a semistructured web-based questionnaire. Next, patients (n=20) were recruited from a clinic and were interviewed in a semistructured face-to-face interview. Results: The survey findings suggested that the assistant should be a multimodal communicator (voice, facial expressions, and gestures) and counteract negative self-evaluation. Most participants preferred a female assistant or wanted the option to choose the gender. In total, 24 (80%) health care professionals wanted a selectable option, while patients exhibited a marked preference for a female or diverse assistant. Regrading patient-assistant interaction, the assistant was seen as a proactive recipient of information, and the patient as a passive one. Gaps in aftercare could be filled by the unlimited availability of the assistant. However, patients should retain their autonomy to avoid dependency. The monitoring of health status was viewed positively by both groups. A biofeedback function was desired to detect early warning signs of disease. When appropriate to the situation, a sense of humor in the assistant was desirable. The desired skills of the assistant can be summarized as providing structure and emotional support, especially warmth and competence to build trust. Consistency was important for the caregiver to appear authentic. Regarding the assistant–care provider interaction, 3 key areas were identified: objective patient status measurement, emergency suicide prevention, and an information tool and decision support system for health care professionals. Conclusions: Overall, the survey conducted provides innovative guidelines for the development of virtual therapeutic assistants to fill the gaps in patient aftercare. © 2025 Elsevier B.V., All rights reserved.","artificial intelligence; depression; digital health care; digital intervention; digital technology; e-mental health; major depressive disorder; mental disorder; mental illness; outpatient aftercare; public health; socially interactive agent; suicidal ideation; suicide prevention; virtual therapeutic assistant","","","","","Depression and Other Common Mental Disorders Global Health Estimates; Nationale Versorgungs Leitlinie, (2015); Hauke Felix Wiegand, Challenges in the transition from in-patient to out-patient treatment in depression - An analysis of administrative health care data from a large German health insurer, Deutsches Arzteblatt International, 117, 27-28, pp. 472-479, (2020); Baldessarini, Ross John, Suicidal risk factors in major affective disorders, British Journal of Psychiatry, 215, 4, pp. 621-626, (2019); Chung, Daniel Thomas, Suicide rates after discharge from psychiatric facilities: A systematic review and meta-analysis, JAMA Psychiatry, 74, 7, pp. 694-702, (2017); Castelli-Dransart, Dolores Angela, Stress reactions after a patient suicide and their relations to the profile of mental health professionals, BMC Psychiatry, 15, 1, (2015); Kleespies, Phillip M., The Stress of Patient Suicidal Behavior During Clinical Training: Incidence, Impact, and Recovery, Professional Psychology: Research and Practice, 24, 3, pp. 293-303, (1993); Cushway, Delia, Stress in clinical psychologists, International Journal of Social Psychiatry, 42, 2, pp. 141-149, (1996); Deutsch, Connie J., Self-reported sources of stress among psychotherapists, Professional Psychology: Research and Practice, 15, 6, pp. 833-845, (1984); Sfetcu, Raluca, Overview of post-discharge predictors for psychiatric re-hospitalisations: A systematic review of the literature, BMC Psychiatry, 17, 1, (2017)","","JMIR Publications Inc.","","","","","","2561326X","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-105001136240"
"Marin-Vega, H.; Alor-Hernández, G.; Bustos-Lopez, M.; Hernández-Capistrán, J.; Hernandez-Chaparro, N.L.; Ixmatlahua-Diaz, S.D.","Marin-Vega, Humberto (57192092213); Alor-Hernández, Giner (17433252100); Bustos-Lopez, Maritza (57190802586); Hernández-Capistrán, Jonathan (56786341100); Hernandez-Chaparro, Norma Leticia (57216851514); Ixmatlahua-Diaz, Sergio David (57216857953)","57192092213; 17433252100; 57190802586; 56786341100; 57216851514; 57216857953","TasksZE: A Task-Based and Challenge-Based Math Serious Game Using Facial Emotion Recognition","2024","Future Internet","16","12","440","","","0","0","10.3390/fi16120440","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213079081&doi=10.3390%2Ffi16120440&partnerID=40&md5=d5135389fcb77e7bf3348eab7121f970","Tecnológico Nacional de México, Mexico City, Mexico; Tecnológico Nacional de México, Mexico City, Mexico","Marin-Vega, Humberto, Tecnológico Nacional de México, Mexico City, Mexico, Tecnológico Nacional de México, Mexico City, Mexico; Alor-Hernández, Giner, Tecnológico Nacional de México, Mexico City, Mexico; Bustos-Lopez, Maritza, Tecnológico Nacional de México, Mexico City, Mexico; Hernández-Capistrán, Jonathan, Tecnológico Nacional de México, Mexico City, Mexico; Hernandez-Chaparro, Norma Leticia, Tecnológico Nacional de México, Mexico City, Mexico; Ixmatlahua-Diaz, Sergio David, Tecnológico Nacional de México, Mexico City, Mexico","Serious games play a significant role in the teaching and learning process by focusing on educational objectives rather than purely on entertainment. By addressing specific educational needs, these games provide targeted learning experiences. The integration of emotion recognition technology into serious games can further enhance teaching and learning by identifying areas where students may need additional support, The integration of emotion recognition into a serious game facilitates the learning of mathematics by allowing the identification of emotional impact on learning and the creation of a tailored learning experience for the student. This study proposes a challenge-based and task-based math serious game that integrates facial emotion recognition named TasksZE. TasksZE introduces a novel approach by adjusting gameplay based on detected emotions, which includes real-time emotion analysis and the cross-validation of emotions. We conducted a usability evaluation of the game using the System Usability Scale (SUS) as a reference, and the results indicate that the students feel that TasksZE is easy to use, the functions are well integrated, and most people can learn to use it very quickly. The students answered that they would use this system frequently since they felt motivated by game attributes, rewards, and level progression. These elements contributed to a more engaging and effective learning experience. © 2024 Elsevier B.V., All rights reserved.","emotion recognition; math learning; serious games","Adversarial machine learning; Emotion Recognition; Face recognition; Federated learning; Serious games; Students; Educational objectives; Emotion recognition; Facial emotions; Game plays; Learning experiences; Learning process; Math learning; Task-based; Teaching and learning; Teaching process; Contrastive Learning","","","Funding text 1: This work was supported by Mexico\u2019s National Technological Institute (TecNM) and sponsored by both Mexico\u2019s National Council of Humanities, Science and Technology (CONAHCYT) and the Secretariat of Public Education (SEP) through the PRODEP project (Programa para el Desarrollo Profesional Docente).; Funding text 2: This research was funded by the National Council of Humanities, Science and Technology (CONAHCYT) for the scholarship awarded by participating in the call for POST-DOCTORAL STAYS FOR MEXICO MODE 1, application number 2420859, to develop the project titled \u201CDevelopment of learning tools based on serious games, gamification and extended reality for teaching mathematics in basic education\u201D.","Boye, Eric Sefa, Effectiveness of problem-based learning strategy in improving teaching and learning of mathematics for pre-service teachers in Ghana, Social Sciences and Humanities Open, 7, 1, (2023); Nurlaily, Vivi Astuti, Elementary school teacher’s obstacles in the implementation of problem-based learning model in mathematics learning, Journal on Mathematics Education, 10, 2, pp. 229-238, (2019); Int J Educ, (2024); Telos, (2020); International Journal of Elementary Education, (2017); Ingenieria Investigacion Y Desarrollo, (2017); Jääskä, Elina, Game-based learning and students’ motivation in project management education, Project Leadership and Society, 3, (2022); Asadzadeh, Afsoon, Serious educational games for children: A comprehensive framework, Heliyon, 10, 6, (2024); Pérez, Jaime, Generation of probabilistic synthetic data for serious games: A case study on cyberbullying, Knowledge-Based Systems, 286, (2024); Li, Mingwei, Understanding how customer social capital accumulation in brand communities: A gamification affordance perspective, Journal of Retailing and Consumer Services, 78, (2024)","","Multidisciplinary Digital Publishing Institute (MDPI)","","","","","","19995903","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85213079081"
"Shambour, Q.Y.; Qandeel, N.; Alraba’nah, Y.; Abumariam, A.; Shambour, M.K.","Shambour, Qusai Yousef (35105920000); Qandeel, Nazem N. (58475742900); Alraba’nah, Yousef (57210390898); Abumariam, Anan Abu (59479316500); Shambour, Moh'd Khaled (57199622074)","35105920000; 58475742900; 57210390898; 59479316500; 57199622074","Artificial Intelligence Techniques for Early Autism Detection in Toddlers: A Comparative Analysis","2024","Journal of Applied Data Sciences","5","4","","1754","1764","0","10","10.47738/jads.v5i4.353","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212468940&doi=10.47738%2Fjads.v5i4.353&partnerID=40&md5=f659d1f169fb9efccab9428e6a8125d4","Department of Software Engineering, Al-Ahliyya Amman University, Amman, Jordan; Department of Special Education, Al-Ahliyya Amman University, Amman, Jordan; Umm Al-Qura University, Makkah, Saudi Arabia","Shambour, Qusai Yousef, Department of Software Engineering, Al-Ahliyya Amman University, Amman, Jordan; Qandeel, Nazem N., Department of Special Education, Al-Ahliyya Amman University, Amman, Jordan; Alraba’nah, Yousef, Department of Software Engineering, Al-Ahliyya Amman University, Amman, Jordan; Abumariam, Anan Abu, Department of Special Education, Al-Ahliyya Amman University, Amman, Jordan; Shambour, Moh'd Khaled, Umm Al-Qura University, Makkah, Saudi Arabia","Individuals with autism spectrum disorder are often characterized by complications in social interaction and communication, which can be attributed to disruptions in brain development affecting their perception and interaction with others. While ASD is not a treatable condition, early detection and diagnosis can significantly mitigate its effects. Recent advancements in artificial intelligence have enabled the development of novel diagnostic tools, which can detect ASD at an earlier stage than traditional methods. This study attempts to enhance and automate the diagnostic process by employing a variety of machine learning techniques to identify the most critical characteristics of ASD. To this end, we employed six state-of-the-art machine learning classification models, including Support Vector Machine, Random Forest, k-Nearest Neighbors, Logistic Regression, Decision Tree, and Naive Bayes classifiers, to analyze and predict ASD in toddlers. Our dataset comprises 1054 instances from a non-clinical ASD screening dataset for toddlers, sourced from the University of California-Irvine (UCI) Machine Learning repository. This data was collected using the ASDTests mobile application, which was developed based on Q-CHAT and AQ-10 screening tools. Our evaluation focused on a range of performance metrics, including accuracy, precision, recall, and F1-score, to assess the efficacy of each model. Notably, the Logistic Regression model demonstrated the highest accuracy in diagnosing ASD in toddlers, achieving a perfect score of 100% across all metrics. © 2024 Elsevier B.V., All rights reserved.","Artificial Intelligence; Autism Spectrum Disorder; Classifier; Early Diagnosis; Machine Learning; Toddlers; Q-Chat-10","","","","","Landa, Rebecca J., Developmental Trajectories in Children With and Without Autism Spectrum Disorders: The First 3 Years, Child Development, 84, 2, pp. 429-442, (2013); Rasul, Rownak Ara, An evaluation of machine learning approaches for early diagnosis of autism spectrum disorder, Healthcare Analytics, 5, (2024); Landa, Rebecca J., Diagnosis of autism spectrum disorders in the first 3 years of life, Nature Clinical Practice Neurology, 4, 3, pp. 138-147, (2008); Dawson, Geraldine, Randomized, controlled trial of an intervention for toddlers with autism: The early start Denver model, Pediatrics, 125, 1, pp. e17-e23, (2010); Vivanti, Giacomo, Outcome for Children Receiving the Early Start Denver Model Before and After 48 Months, Journal of Autism and Developmental Disorders, 46, 7, pp. 2441-2449, (2016); Landa, Rebecca J., Efficacy of early interventions for infants and young children with, and at risk for, autism spectrum disorders, International Review of Psychiatry, 30, 1, pp. 25-39, (2018); Song, Da Yea, The Use of Artificial Intelligence in Screening and Diagnosis of Autism Spectrum Disorder: A Literature Review, Journal of the Korean Academy of Child and Adolescent Psychiatry, 30, 4, pp. 145-152, (2019); Talukdar, Jyotismita, A comparative assessment of most widely used machine learning classifiers for analysing and classifying autism spectrum disorder in toddlers and adolescents, Healthcare Analytics, 3, (2023); Shehab, Mohammad M., Machine learning in medical applications: A review of state-of-the-art methods, Computers in Biology and Medicine, 145, (2022); Shahamiri, Seyed Reza, Autism AI: a New Autism Screening System Based on Artificial Intelligence, Cognitive Computation, 12, 4, pp. 766-777, (2020)","","Bright Publisher","","","","","","27236471","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85212468940"
"Keinert, M.; Schindler-Gmelch, L.; Rupp, L.H.; Sadeghi, M.; Capito, K.; Hager, M.; Rahimi, F.; Richer, R.; Egger, B.; Eskofier, B.M.; Berking, M.","Keinert, Marie (57990462700); Schindler-Gmelch, Lena (58759159500); Rupp, Lydia Helene (57220050972); Sadeghi, Misha (58759115500); Capito, Klara (57222747333); Hager, Malin (59469605400); Rahimi, Farnaz (59345856800); Richer, Robert (56323242800); Egger, Bernhard (57210675221); Eskofier, Bjoern M. (26428080900); Berking, Matthias (10039087200)","57990462700; 58759159500; 57220050972; 58759115500; 57222747333; 59469605400; 59345856800; 56323242800; 57210675221; 26428080900; 10039087200","Facing depression: evaluating the efficacy of the EmpkinS-EKSpression reappraisal training augmented with facial expressions – protocol of a randomized controlled trial","2024","BMC Psychiatry","24","1","896","","","0","3","10.1186/s12888-024-06361-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211795921&doi=10.1186%2Fs12888-024-06361-3&partnerID=40&md5=76ab52e24133cf9e62b87e5d7e0a882c","Department of Clinical Psychology and Psychotherapy, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Machine Learning and Data Analytics Lab, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Department of Computer Science, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Translational Digital Health Group, Helmholtz Center Munich German Research Center for Environmental Health, Oberschleissheim, Germany; Department of Clinical Psychology and Psychotherapy, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany","Keinert, Marie, Department of Clinical Psychology and Psychotherapy, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany, Department of Clinical Psychology and Psychotherapy, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Schindler-Gmelch, Lena, Department of Clinical Psychology and Psychotherapy, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Rupp, Lydia Helene, Department of Clinical Psychology and Psychotherapy, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Sadeghi, Misha, Machine Learning and Data Analytics Lab, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Capito, Klara, Department of Clinical Psychology and Psychotherapy, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Hager, Malin, Department of Clinical Psychology and Psychotherapy, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Rahimi, Farnaz, Machine Learning and Data Analytics Lab, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Richer, Robert, Machine Learning and Data Analytics Lab, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Egger, Bernhard, Department of Computer Science, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Eskofier, Bjoern M., Machine Learning and Data Analytics Lab, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany, Translational Digital Health Group, Helmholtz Center Munich German Research Center for Environmental Health, Oberschleissheim, Germany; Berking, Matthias, Department of Clinical Psychology and Psychotherapy, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany","Background: Dysfunctional depressogenic cognitions are considered a key factor in the etiology and maintenance of depression. In cognitive behavioral therapy (CBT), the current gold-standard psychotherapeutic treatment for depression, cognitive restructuring techniques are employed to address dysfunctional cognitions. However, high drop-out and non-response rates suggest a need to boost the efficacy of CBT for depression. This might be achieved by enhancing the role of emotional and kinesthetic (i.e., body movement perception) features of interventions. Therefore, we aim to evaluate the efficacy of a cognitive restructuring task augmented with the performance of anti-depressive facial expressions in individuals with and without depression. Further, we aim to investigate to what extent kinesthetic markers are intrinsically associated with and, hence, allow for the detection of, depression. Methods: In a four-arm, parallel, single-blind, randomized controlled trial (RCT), we will randomize 128 individuals with depression and 128 matched controls without depression to one of four study conditions: (1) a cognitive reappraisal training (CR); (2) CR enhanced with instructions to display anti-depressive facial expressions (CR + AFE); (3) facial muscle training focusing on anti-depressive facial expressions (AFE); and (4) a sham control condition. One week after diagnostic assessment, a single intervention of 90–120-minute duration will be administered, with a subsequent follow-up two weeks later. Depressed mood will serve as primary outcome. Secondary outcomes will include current positive mood, symptoms of depression, current suicidality, dysfunctional attitudes, automatic thoughts, emotional state, kinesthesia (i.e., facial expression, facial muscle activity, body posture), psychophysiological measures (e.g., heart rate (variability), respiration rate (variability), verbal acoustics), as well as feasibility measures (i.e., treatment integrity, compliance, usability, acceptability). Outcomes will be analyzed with multiple methods, such as hierarchical and conventional linear models and machine learning. Discussion: If shown to be feasible and effective, the inclusion of kinesthesia into both psychotherapeutic diagnostics and interventions may be a pivotal step towards the more prompt, efficient, and targeted treatment of individuals with depression. Trial registration: The study was preregistered in the Open Science Framework on August 12, 2022 (https://osf.io/mswfg/) and retrospectively registered in the German Clinical Trials Register on November 25, 2024. Clinical Trial Number: DRKS00035577. © 2024 Elsevier B.V., All rights reserved.","Cognitive reappraisal; Depression; Embodiment; Facial expression; Kinesthesia; Machine learning; Smartphone-based intervention","adult; Article; automatic thoughts questionnaire; Center for Epidemiological Studies Depression Scale; cognitive behavioral therapy; cognitive reappraisal; cognitive restructuring; controlled study; depression; depression assessment; DSM-5; Dysfunctional Beliefs and Attitudes about Sleep scale; electromyography; emotion regulation questionnaire; facial expression; feasibility study; female; Hamilton Depression Rating Scale; Hospital Anxiety and Depression Scale; human; intervention study; kinesthesia; major clinical study; male; mood; outcome assessment; Patient Health Questionnaire 9; psychometry; psychophysiological measures; psychophysiology; psychotherapist; psychotherapy; randomized controlled trial; self report; sham procedure; single blind procedure; suicide attempt; User Experience Questionnaire; emotion; middle aged; physiology; procedures; therapy; treatment outcome; young adult; Adult; Cognitive Behavioral Therapy; Depression; Emotions; Facial Expression; Female; Humans; Kinesthesis; Male; Middle Aged; Single-Blind Method; Treatment Outcome; Young Adult","","","This study is funded by the German Research Foundation (Deutsche Forschungsgemeinschaft, DFG) - SFB 1483 \u2013 Project-ID 442419336, EmpkinS.","Diagnostic and Statistical Manual of Mental Disorders, (2000); Depressive Disorder Depression, (2023); Cuijpers, Pim C., Comprehensive meta-analysis of excess mortality in depression in the general community versus patients with specific illnesses, American Journal of Psychiatry, 171, 4, pp. 453-462, (2014); Eaton, William W., Population-based study of first onset and chronicity in major depressive disorder, Archives of General Psychiatry, 65, 5, pp. 513-520, (2008); Cuijpers, Pim C., A meta-analysis of cognitive-behavioural therapy for adult depression, alone and in comparison with other treatments, Canadian Journal of Psychiatry, 58, 7, pp. 376-385, (2013); Hans, Eva, Effectiveness of and dropout from outpatient cognitive behavioral therapy for adult unipolar depression: A meta-analysis of nonrandomized effectiveness studies, Journal of Consulting and Clinical Psychology, 81, 1, pp. 75-88, (2013); Cuijpers, Pim C., The effects of psychotherapies for major depression in adults on remission, recovery and improvement: A meta-analysis, Journal of Affective Disorders, 159, pp. 118-126, (2014); Yimer, Tesfa Mekonen, Estimating the global treatment rates for depression: A systematic review and meta-analysis, Journal of Affective Disorders, 295, pp. 1234-1242, (2021); Beck, Aaron T., Advances in cognitive theory and therapy: The generic cognitive model, Annual Review of Clinical Psychology, 10, pp. 1-24, (2014); Beck, Aaron T., Cognitive therapy: Current status and future directions, Annual Review of Medicine, 62, pp. 397-409, (2011)","","BioMed Central Ltd","","","","","","1471244X","","BPMSC","39668374","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85211795921"
"Chen, J.; Chan, N.Y.; Li, C.-T.; Chan, J.W.Y.; Liu, Y.; Li, S.X.; Chau, S.W.H.; Leung, K.S.; Heng, P.-A.; Lee, T.M.C.; Li, T.M.H.; Wing, Y.-K.","Chen, Jie (56061912600); Chan, Ngan Yin (56145903800); Li, Chun Tung (57191502151); Chan, Joey Wy Y. (55318172500); Liu, Yaping (55356814300); Li, Shirley Xin (35181632000); Chau, Steven W.H. (34871537500); Leung, Kwongsak (57208699274); Heng, Pheng Ann (7006677755); Lee, Tatia Mei Chun (7501437381); Li, Tim M.H. (55728881700); Wing, Yun Kwok (55667177400)","56061912600; 56145903800; 57191502151; 55318172500; 55356814300; 35181632000; 34871537500; 57208699274; 7006677755; 7501437381; 55728881700; 55667177400","Multimodal digital assessment of depression with actigraphy and app in Hong Kong Chinese","2024","Translational Psychiatry","14","1","150","","","0","9","10.1038/s41398-024-02873-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188080134&doi=10.1038%2Fs41398-024-02873-4&partnerID=40&md5=383e7a9983dd26543cf96dcd3cde7981","Department of Psychiatry, Chinese University of Hong Kong, Faculty of Medicine, Hong Kong, Hong Kong; Department of Psychiatry, Fujian Medical University, Fuzhou, China; Center for Sleep and Circadian Medicine, Guangzhou Medical University, Guangzhou, China; The University of Hong Kong, State Key Laboratory of Brain and Cognitive Sciences, Hong Kong, Hong Kong; Department of Psychology, The University of Hong Kong, Hong Kong, Hong Kong; Department of Applied Data Science, Hong Kong Shue Yan University, Hong Kong, China; Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong, Hong Kong; Department of Psychology, The University of Hong Kong, Hong Kong, Hong Kong","Chen, Jie, Department of Psychiatry, Chinese University of Hong Kong, Faculty of Medicine, Hong Kong, Hong Kong, Department of Psychiatry, Fujian Medical University, Fuzhou, China; Chan, Ngan Yin, Department of Psychiatry, Chinese University of Hong Kong, Faculty of Medicine, Hong Kong, Hong Kong; Li, Chun Tung, Department of Psychiatry, Chinese University of Hong Kong, Faculty of Medicine, Hong Kong, Hong Kong; Chan, Joey Wy Y., Department of Psychiatry, Chinese University of Hong Kong, Faculty of Medicine, Hong Kong, Hong Kong; Liu, Yaping, Department of Psychiatry, Chinese University of Hong Kong, Faculty of Medicine, Hong Kong, Hong Kong, Center for Sleep and Circadian Medicine, Guangzhou Medical University, Guangzhou, China; Li, Shirley Xin, The University of Hong Kong, State Key Laboratory of Brain and Cognitive Sciences, Hong Kong, Hong Kong, Department of Psychology, The University of Hong Kong, Hong Kong, Hong Kong; Chau, Steven W.H., Department of Psychiatry, Chinese University of Hong Kong, Faculty of Medicine, Hong Kong, Hong Kong; Leung, Kwongsak, Department of Applied Data Science, Hong Kong Shue Yan University, Hong Kong, China, Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong, Hong Kong; Heng, Pheng Ann, Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong, Hong Kong; Lee, Tatia Mei Chun, The University of Hong Kong, State Key Laboratory of Brain and Cognitive Sciences, Hong Kong, Hong Kong, Department of Psychology, The University of Hong Kong, Hong Kong, Hong Kong; Li, Tim M.H., Department of Psychiatry, Chinese University of Hong Kong, Faculty of Medicine, Hong Kong, Hong Kong; Wing, Yun Kwok, Department of Psychiatry, Chinese University of Hong Kong, Faculty of Medicine, Hong Kong, Hong Kong","There is an emerging potential for digital assessment of depression. In this study, Chinese patients with major depressive disorder (MDD) and controls underwent a week of multimodal measurement including actigraphy and app-based measures (D-MOMO) to record rest-activity, facial expression, voice, and mood states. Seven machine-learning models (Random Forest [RF], Logistic regression [LR], Support vector machine [SVM], K-Nearest Neighbors [KNN], Decision tree [DT], Naive Bayes [NB], and Artificial Neural Networks [ANN]) with leave-one-out cross-validation were applied to detect lifetime diagnosis of MDD and non-remission status. Eighty MDD subjects and 76 age- and sex-matched controls completed the actigraphy, while 61 MDD subjects and 47 controls completed the app-based assessment. MDD subjects had lower mobile time (P = 0.006), later sleep midpoint (P = 0.047) and Acrophase (P = 0.024) than controls. For app measurement, MDD subjects had more frequent brow lowering (P = 0.023), less lip corner pulling (P = 0.007), higher pause variability (P = 0.046), more frequent self-reference (P = 0.024) and negative emotion words (P = 0.002), lower articulation rate (P < 0.001) and happiness level (P < 0.001) than controls. With the fusion of all digital modalities, the predictive performance (F1-score) of ANN for a lifetime diagnosis of MDD was 0.81 and 0.70 for non-remission status when combined with the HADS-D item score, respectively. Multimodal digital measurement is a feasible diagnostic tool for depression in Chinese. A combination of multimodal measurement and machine-learning approach has enhanced the performance of digital markers in phenotyping and diagnosis of MDD. © 2024 Elsevier B.V., All rights reserved.","","antidepressant agent; anxiolytic agent; mood stabilizer; neuroleptic agent; psychotropic agent; actimetry; adult; aged; Article; artificial neural network; Bayesian learning; case control study; Chinese; circadian rhythm; controlled clinical trial; controlled study; decision tree; depression assessment; emotion; facial expression; female; Hamilton Depression Rating Scale; Hong Kong; Hospital Anxiety and Depression Scale; human; insomnia; Insomnia Severity Index; k nearest neighbor; leave one out cross validation; logistic regression analysis; machine learning; major clinical study; major depression; male; mental hospital; mood; Morningness-Eveningness Questionnaire; multimodal digital assessment; natural language processing; physical activity; predictive value; random forest; remission; rest; sleep; sleep parameters; speech analysis; Structured Clinical Interview for DSM Disorders; support vector machine; videorecording; voice; Bayes theorem; depression; Actigraphy; Bayes Theorem; Depression; Depressive Disorder, Major; Humans; Mobile Applications","","","Funding text 1: Yun-Kwok Wing received a consultation fee from Eisai Co., Ltd., an honorarium from Eisai Hong Kong for a lecture, travel support from Lundbeck HK limited for the overseas conference, and an honorarium from Aculys Pharma, Inc for a lecture and Joey WY Chan received a personal fee from Eisai Co., Ltd and travel support from Lundbeck HK limited for overseas conference, which is outside the submitted work. All other authors declare no financial or non-financial competing interests.; Funding text 2: This study was funded by the Health and Medical Research Fund (HMRF, Ref: 09203066). The funder played no role in the study design, data collection, analysis, and interpretation of data, or the writing of this manuscript. We thank all the doctors who have helped with the recruitment of subjects and all the subjects who participated in this study.","Herrman, Helen Edith, Reducing the global burden of depression: a Lancet–World Psychiatric Association Commission, The Lancet, 393, 10189, pp. e42-e43, (2019); Lam, Chiu Wa Linda, Prevalence, psychosocial correlates and service utilization of depressive and anxiety disorders in Hong Kong: the Hong Kong Mental Morbidity Survey (HKMMS), Social Psychiatry and Psychiatric Epidemiology, 50, 9, pp. 1379-1388, (2015); Schnyder, Nina, Association between mental health-related stigma and active help-seeking: Systematic review and meta-analysis, British Journal of Psychiatry, 210, 4, pp. 261-268, (2017); Wong, Vanessa Ting Chi, Recruitment and training of psychiatrists in Hong Kong: What puts medical students off psychiatry-An international experience, International Review of Psychiatry, 25, 4, pp. 481-485, (2013); Ebner-Priemer, Ulrich Walter, Digital phenotyping: hype or hope?, The Lancet Psychiatry, 7, 4, pp. 297-299, (2020); Huckvale, Kit, Toward clinical digital phenotyping: a timely opportunity to consider purpose, quality, and safety, npj Digital Medicine, 2, 1, (2019); Pampouchidou, Anastasia, Automatic Assessment of Depression Based on Visual Cues: A Systematic Review, IEEE Transactions on Affective Computing, 10, 4, pp. 445-470, (2019); Cummins, Nicholas, A review of depression and suicide risk assessment using speech analysis, Speech Communication, 71, pp. 10-49, (2015); De Angel, Valeria, Digital health tools for the passive monitoring of depression: a systematic review of methods, npj Digital Medicine, 5, 1, (2022); Di̇Beklioğlu, Hamdi, Dynamic Multimodal Measurement of Depression Severity Using Deep Autoencoding, IEEE Journal of Biomedical and Health Informatics, 22, 2, pp. 525-536, (2018)","","Springer Nature","","","","","","21583188","","","38499546","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85188080134"
"Ruan, X.; Palansuriya, C.; Wang, K.; Constantin, A.","Ruan, Xingran (57802415600); Palansuriya, Charaka (15848922900); Wang, Kangcheng (55582750000); Constantin, Aurora (55841850800)","57802415600; 15848922900; 55582750000; 55841850800","Identifying Children Metacognitive Monitoring Performance Through Facial Expressions","2024","","","","","242","248","0","0","10.1145/3665463.3678790","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211606583&doi=10.1145%2F3665463.3678790&partnerID=40&md5=90b3486d0e6f526dfaa427ef09daa9fb","The University of Edinburgh, Edinburgh, United Kingdom; Shandong Normal University, Jinan, China","Ruan, Xingran, The University of Edinburgh, Edinburgh, United Kingdom; Palansuriya, Charaka, The University of Edinburgh, Edinburgh, United Kingdom; Wang, Kangcheng, Shandong Normal University, Jinan, China; Constantin, Aurora, The University of Edinburgh, Edinburgh, United Kingdom","Metacognitive monitoring involves evaluating one’s own thought processes and knowledge. In recent efforts to design serious games for educational purposes, scaffolds for the metacognitive monitoring process have been provided to help pupils achieve higher learning outcomes. In preliminary studies on the affects experienced in learning process, relationships between facial expressions and metacognitive monitoring performance (MMP) are revealed. As scaffolds are designed to avoid increasing cognitive load and disrupting the learning process, our research aims to automatically scaffold the metacognitive monitoring process through facial expressions without disturbing pupils’ learning process. We conducted a large-scale user study with pupils and collected their facial expressions while they performed the metacognitive monitoring process. So far, we built the video dataset (Affect2Metacognition) to train and test state-of-the-art neural networks for MMP identification. In addition, we proposed a strategy for scaffolding pupils’ metacognitive monitoring process, which is based on identifying their MMPs through facial expressions. In the long term of our research, we will analyse the impact of this strategy on pupils’ learning outcomes. © 2024 Elsevier B.V., All rights reserved.","Children; Deep learning; Facial expressions; Metacognitive monitoring performance; Scaffolds","Contrastive Learning; Deep learning; Face recognition; Federated learning; Game design; Scaffolds; Scaffolds (biology); Child; Facial Expressions; Learning outcome; Learning process; Metacognitive monitoring performance; Metacognitives; Monitoring performance; Monitoring process; Thought process; Process monitoring","","","We gratefully acknowledge the contributions of Zhang Yihao, Lu Chunyan, Hu Yufei, Chen Jiahui, Bai Changlin, and Xie Wenjing for their assistance in conducting this study. Additionally, we extend our appreciation to the Edinburgh Parallel Computing Center and the Great Britain-China Educational Trust for their support of Xingran Ruan\u2019s PhD research.","Andres, Juan Miguel Limjap, Analyzing student action sequences and affect while playing physics playground, CEUR Workshop Proceedings, 1446, (2015); Azevedo, Roger, MetaTutor: A MetaCognitive tool for enhancing self-regulated learning, FS-09-02, pp. 14-19, (2009); Bellon, Elien, Metacognition across domains: Is the association between arithmetic and metacognitive monitoring domain-specific?, PLOS ONE, 15, 3, (2020); Icml, (2021); Berthold, Kirsten, Do learning protocols support learning strategies and outcomes? The role of cognitive and metacognitive prompts, Learning and Instruction, 17, 5, pp. 564-577, (2007); Brookman-Byrne, Annie, Inhibitory control and counterintuitive science and maths reasoning in adolescence, PLOS ONE, 13, 6, (2018); Cloude, Elizabeth Brooke, How do Emotions Change during Learning with an Intelligent Tutoring System? Metacognitive Monitoring and Performance with MetaTutor, pp. 423-429, (2020); Handbook of Metacognition in Education, (2009); Donahue, Jeff, Long-term recurrent convolutional networks for visual recognition and description, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 07-12-June-2015, pp. 2625-2634, (2015); D’Mello, Sidney K., Dynamics of affective states during complex learning, Learning and Instruction, 22, 2, pp. 145-157, (2012)","","Association for Computing Machinery, Inc","ACM SIGCHI; Antidote Game Research Platform; TAMPERE; Tampere University","11th ACM SIGCHI Annual Symposium on Computer-Human Interaction in Play, CHI-PLAY Companion 2024","","Tampere","204163","","9798400706929","","","English","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85211606583"
"Bontchev, B.; Naydenov, I.; Adamov, I.","Bontchev, Boyan Paskalev (6506653436); Naydenov, Ivan (57435235900); Adamov, Ilko (57435236000)","6506653436; 57435235900; 57435236000","Intelligent Adaptation of Difficulty and NPC Behavior in Serious Video Games for Learning","2024","IFAC-PapersOnLine","58","3","","187","192","0","1","10.1016/j.ifacol.2024.07.148","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201543361&doi=10.1016%2Fj.ifacol.2024.07.148&partnerID=40&md5=3a25a970114908b9c96c539ab8fe5371","Sofia University St. Kliment Ohridski, Sofia, Bulgaria","Bontchev, Boyan Paskalev, Sofia University St. Kliment Ohridski, Sofia, Bulgaria; Naydenov, Ivan, Sofia University St. Kliment Ohridski, Sofia, Bulgaria; Adamov, Ilko, Sofia University St. Kliment Ohridski, Sofia, Bulgaria","User-centric adaptation of serious video games continues to be a very important issue because of its benefits, such as enhanced motivation and engagement of individual players. It is based on player/learner characteristics that can be measured, estimated, recognized, or found by classification or clusterization. The paper suggests a new approach for dynamic, user-centric tailoring of task difficulty and the behavior of non-player characters. The approach is based on the emotional state and the shown outcomes of the individual player. Recognizing the current emotional state is based on facial expression analysis by convolutional neural networks and on an analysis of physiological data measured by sensors while playing the game. There are provided examples of serious games of learning with a dynamic adaptation of task difficulty and non-player character behavior. © 2024 Elsevier B.V., All rights reserved.","adaptation; dynamic; educational; non-player character; Serious video games","Adaptation; Educational; Emotional state; Games for learning; Intelligent adaptation; Non-player character; Serious video game; Task difficulty; User-centric; Video-games; Convolutional neural networks","","","This research is funded by the EU through the ESI Funds and by the European Union -NextGenerationEU, through the National Recovery and Resilience Plan of the Republic of Bulgaria, project SUMMIT, No . BG-RRP-2.004-0008.","Serious Games, (1970); Aydin, Muharrem, Examination of adaptation components in serious games: a systematic review study, Education and Information Technologies, 28, 6, pp. 6541-6562, (2023); Intelligent Biofeedback Using an Immersive Competitive Environment, (2001); Bontchev, Boyan Paskalev, Adaptation in affective video games: A literature review, Cybernetics and Information Technologies, 16, 3, pp. 3-34, (2016); Apogee Project Final Scientific Report for the Second Stage of Project Implementation Output D8 3 Version 1 0, (2022); Bradley, Margaret M., Measuring emotion: The self-assessment manikin and the semantic differential, Journal of Behavior Therapy and Experimental Psychiatry, 25, 1, pp. 49-59, (1994); Colombetti, Giovanna, Scaffoldings of the affective mind, Philosophical Psychology, 28, 8, pp. 1157-1176, (2015); Comaniciu, Dorin, Mean shift: A robust approach toward feature space analysis, IEEE Transactions on Pattern Analysis and Machine Intelligence, 24, 5, pp. 603-619, (2002); Connolly, Thomas M., A systematic literature review of empirical evidence on computer games and serious games, Computers and Education, 59, 2, pp. 661-686, (2012); Deep Learning, (2015)","Stapleton, L.","Elsevier B.V.","et al.; International Federation of Automatic Control (IFAC) - TC 4.5. Human Machine Systems; International Federation of Automatic Control (IFAC) - TC 8.3. Modelling and Control of Environmental Systems; International Federation of Automatic Control (IFAC) - TC 9.2. Systems and Control for Societal Impact; International Federation of Automatic Control (IFAC) - TC 9.4. Control Education; International Federation of Automatic Control (IFAC) - Technology, Culture and International Stability (TECIS), TC 9.5","22nd IFAC Conference on Technology, Culture and International Stability, TECIS 2024","","Waterford","201612","24058963; 24058971; 14746670","9783902661869; 9788374810357; 8374810351; 9783902661463; 9783902661586; 9783902661906; 9783902661104; 9783902823007; 9783902823243; 9783902823106","","","English","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85201543361"
"Tang, C.; Qi, A.; Xie, B.","Tang, Chunqiu (34868859300); Qi, Ao (59175451500); Xie, Bin (59176244600)","34868859300; 59175451500; 59176244600","Research on lightweight speech emotion recognition in vehicle noisy environment","2024","Advances in Mechanical Engineering","16","6","","","","0","0","10.1177/16878132241260585","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196189572&doi=10.1177%2F16878132241260585&partnerID=40&md5=c15dec1bf6c9eb5d86dd5d2dcef350cb","School of Mechanical and Electronic Engineering, Wuhan University of Technology, Wuhan, China; Ltd., Wuhan, China","Tang, Chunqiu, School of Mechanical and Electronic Engineering, Wuhan University of Technology, Wuhan, China; Qi, Ao, School of Mechanical and Electronic Engineering, Wuhan University of Technology, Wuhan, China; Xie, Bin, Ltd., Wuhan, China","In order to reduce the incidence of traffic accidents caused by the emotional state of drivers, this study proposes an emotion recognition algorithm based on vehicle noise environment. This algorithm can effectively identify the emotional state of drivers and provide support for further improving their emotions. To address challenges in existing research on speech emotion recognition, such as excessive model parameters, poor generalization, and suboptimal performance in noisy environments, this paper proposes a lightweight network model suitable for small datasets. The model utilizes Power Normalized Cepstral Coefficients (PNCC) as input features, and employs parallel feature extraction layers at different scales. These features are then fed into a feature learning module for in-depth extraction, with the final determination of the driver’s emotional state made by the output layer. Experimental results show that the model achieves an accuracy of 96.08% on the EMO-DB speech dataset. Even in simulated in-vehicle noise environments, the model exhibits high accuracy and robustness. Moreover, compared to other lightweight models, it has fewer training parameters and faster processing speed, making it suitable for deployment on edge devices in mobile applications. © 2024 Elsevier B.V., All rights reserved.","convolutional neural network; lightweight model; PNCC features; speech emotion recognition; vehicle noise","","","","","El Ayadi, Moataz M.H., Survey on speech emotion recognition: Features, classification schemes, and databases, Pattern Recognition, 44, 3, pp. 572-587, (2011); Internet Things Technol, (2023); Diao, Wenhui, Efficient Saliency-Based Object Detection in Remote Sensing Images Using Deep Belief Networks, IEEE Geoscience and Remote Sensing Letters, 13, 2, pp. 137-141, (2016); Zhou, Haofei, Real-time Intelligent Monitoring for Manufacturing Processes with Big Data Based on Deep Belief Networks, Zhongguo Jixie Gongcheng/China Mechanical Engineering, 29, 10, pp. 1201-1213, (2018); Xu, Yunfeng, HGFM : A Hierarchical Grained and Feature Model for Acoustic Emotion Recognition, Proceedings - ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing, 2020-May, pp. 6499-6503, (2020); Dai, Dongyang, Learning Discriminative Features from Spectrograms Using Center Loss for Speech Emotion Recognition, Proceedings - ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing, 2019-May, pp. 7405-7409, (2019); Satt, Aharon, Efficient emotion recognition from speech using deep learning on spectrograms, Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH, 2017-August, pp. 1089-1093, (2017); Attention Based Fully Convolutional Network for Speech Emotion Recognition, (2019); Wani, Taiba Majid, A Comprehensive Review of Speech Emotion Recognition Systems, IEEE Access, 9, pp. 47795-47814, (2021); Speech Emotion Recognition Using Spectrogram Phoneme Embedding","","SAGE Publications Inc.","","","","","","16878132; 16878140","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85196189572"
"Li, K.; Zhang, R.; Chen, S.; Chen, B.; Sakashita, M.; Guimbretiére, F.; Zhang, C.","Li, Ke (57210600474); Zhang, Ruidong (57225000918); Chen, Siyuan (37035804900); Chen, Boao (58912497300); Sakashita, Mose (57190945486); Guimbretiére, François V. (6508204254); Zhang, Cheng (55876606900)","57210600474; 57225000918; 37035804900; 58912497300; 57190945486; 6508204254; 55876606900","EyeEcho: Continuous and Low-power Facial Expression Tracking on Glasses","2024","Conference on Human Factors in Computing Systems - Proceedings","","","319","","","0","17","10.1145/3613904.3642613","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194870485&doi=10.1145%2F3613904.3642613&partnerID=40&md5=b9b152efecfcd238cf78ba755126e0ed","Cornell University, Ithaca, United States","Li, Ke, Cornell University, Ithaca, United States; Zhang, Ruidong, Cornell University, Ithaca, United States; Chen, Siyuan, Cornell University, Ithaca, United States; Chen, Boao, Cornell University, Ithaca, United States; Sakashita, Mose, Cornell University, Ithaca, United States; Guimbretiére, François V., Cornell University, Ithaca, United States; Zhang, Cheng, Cornell University, Ithaca, United States","In this paper, we introduce EyeEcho, a minimally-obtrusive acoustic sensing system designed to enable glasses to continuously monitor facial expressions. It utilizes two pairs of speakers and microphones mounted on glasses, to emit encoded inaudible acoustic signals directed towards the face, capturing subtle skin deformations associated with facial expressions. The reflected signals are processed through a customized machine-learning pipeline to estimate full facial movements. EyeEcho samples at 83.3 Hz with a relatively low power consumption of 167mW. Our user study involving 12 participants demonstrates that, with just four minutes of training data, EyeEcho achieves highly accurate tracking performance across different real-world scenarios, including sitting, walking, and after remounting the devices. Additionally, a semi-in-the-wild study involving 10 participants further validates EyeEcho's performance in naturalistic scenarios while participants engage in various daily activities. Finally, we showcase EyeEcho's potential to be deployed on a commercial-off-the-shelf (COTS) smartphone, offering real-time facial expression tracking. © 2024 Elsevier B.V., All rights reserved.","Acoustic Sensing; Eye-mounted Wearable; Facial Expression Tracking; Low-power","Eye tracking; Face recognition; Wearable computers; Acoustic sensing; Acoustic signals; Eye-mounted wearable; Facial expression tracking; Facial Expressions; Low Power; Machine-learning; Reflected signal; Sensing systems; Skin deformation; Glass","","","This work is supported by National Science Foundation (NSF) under Grant No. 2239569, NSF's Innovation Corps (I-Corps) under Grant No. 2346817, NSF Award IIS-1925100, the Ignite Program at Cornell University, the Nakajima Foundation, and the Ann S. Bowers College of Computing and Information Science at Cornell University. We also appreciate the help of our lab mates on providing feedback on the hardware prototypes and paper writing. Specifically, we would like to thank Devansh Agarwal and Jian Wang for their efforts on developing the real-time demonstration of the system.","Pytorch Mobile Home Pytorch, (2022); Ando, Toshiyuki, CanalSense: Face-related movement recognition system based on sensing air pressure in ear canals, pp. 679-689, (2017); Aumi, Md Tanvir Islam, DopLink: Using the doppler effect for multi-device interaction, pp. 583-586, (2013); Owr 05049t 38d, (2023); Chen, Tuochao, NeckFace: Continuously Tracking Full Facial Expressions on Neck-mounted Wearables, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 5, 2, (2021); Chen, Tuochao, C-Face: Continuously reconstructing facial expressions by deep learning contours of the face with ear-mounted miniature cameras, pp. 112-125, (2020); Bose Frames Tempo, (2023); Proceedings of the ACM on Interactive Mobile Wearable and Ubiquitous Technologies, (2017); Moverio® BT 35es Smart Glasses, (2022); Gao, Yang, SonicFace: Tracking Facial Expressions Using a Commodity Microphone Array, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 5, 4, (2021)","","Association for Computing Machinery","ACM SIGCHI","2024 CHI Conference on Human Factors in Computing Sytems, CHI 2024","","Hybrid, Honolulu; HI","199441","","1581132484; 0897913833; 9781595936424; 1581136374; 9781450346559; 9781595930026; 9781605589312; 9781450356206; 9781450310161; 9781581137033","","","English","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85194870485"
"Nepal, S.; Pillai, A.; College, D.; Wang, W.; Griffin, T.; Collins, A.C.; Heinz, M.; Lekkas, D.; Mirjafari, S.; Nemesure, M.; Price, G.; Jacobson, N.C.; Campbell, A.T.","Nepal, Subigya Kumar (57217206424); Pillai, Arvind (57221860100); College, Dartmouth (57065671400); Wang, Weichen (57193859925); Griffin, Tess Z. (58172273600); Collins, Amanda C. (57209341553); Heinz, Michael V. (57219928532); Lekkas, Damien (57219924140); Mirjafari, Shayan (57209308067); Nemesure, Matthew D. (57201944734); Price, George D. (57220907543); Jacobson, Nicholas C. (37761530000); Campbell, Andrew T. (7403504710)","57217206424; 57221860100; 57065671400; 57193859925; 58172273600; 57209341553; 57219928532; 57219924140; 57209308067; 57201944734; 57220907543; 37761530000; 7403504710","MoodCapture: Depression Detection Using In-the-Wild Smartphone Images","2024","Conference on Human Factors in Computing Systems - Proceedings","","","996","","","0","8","10.1145/3613904.3642680","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194831407&doi=10.1145%2F3613904.3642680&partnerID=40&md5=288bbaa93d636d7c81fe8687bc12d74a","Dartmouth College, Hanover, United States","Nepal, Subigya Kumar, ; Pillai, Arvind, ; College, Dartmouth, ; Wang, Weichen, Dartmouth College, Hanover, United States; Griffin, Tess Z., Dartmouth College, Hanover, United States; Collins, Amanda C., Dartmouth College, Hanover, United States; Heinz, Michael V., Dartmouth College, Hanover, United States; Lekkas, Damien, Dartmouth College, Hanover, United States; Mirjafari, Shayan, Dartmouth College, Hanover, United States; Nemesure, Matthew D., Dartmouth College, Hanover, United States; Price, George D., Dartmouth College, Hanover, United States; Jacobson, Nicholas C., Dartmouth College, Hanover, United States; Campbell, Andrew T., Dartmouth College, Hanover, United States","MoodCapture presents a novel approach that assesses depression based on images automatically captured from the front-facing camera of smartphones as people go about their daily lives. We collect over 125,000 photos in the wild from N=177 participants diagnosed with major depressive disorder for 90 days. Images are captured naturalistically while participants respond to the PHQ-8 depression survey question: “I have felt down, depressed, or hopeless”. Our analysis explores important image attributes, such as angle, dominant colors, location, objects, and lighting. We show that a random forest trained with face landmarks can classify samples as depressed or non-depressed and predict raw PHQ-8 scores effectively. Our post-hoc analysis provides several insights through an ablation study, feature importance analysis, and bias assessment. Importantly, we evaluate user concerns about using MoodCapture to detect depression based on sharing photos, providing critical insights into privacy concerns that inform the future design of in-the-wild image-based mental health assessment tools. © 2024 Elsevier B.V., All rights reserved.","Depression; Face; Facial Expressions; In-the-wild; Machine Learning; Mental Health; Mood; Passive Sensing; PHQ; Smartphones","Machine learning; Depression; Face; Facial Expressions; In-the-wild; Machine-learning; Mental health; Mood; Passive sensing; PHQ; Smart phones; Smartphones","","","We sincerely thank the participants who kindly consented to share their photos for this study. The CHI reviewers helped lift this paper considerably. Their insight, detailed comments, and suggestions were invaluable. We thank them for their diligence and for caring about the subject matter and our paper. The research discussed in this paper was supported by the National Institute of Mental Health (NIMH) under award number R01MH123482-01. We acknowledge that the content of this manuscript is solely our responsibility and does not necessarily reflect the views of the NIMH. We also clarify that the funding body had no involvement in the study's design, data collection, analysis, interpretation, or manuscript preparation.","International Journal of Community Medicine and Public Health, (2020); Aşçı, Sinan, Left vs. Right-Handed UX: A comparative user study on a mobile application with left and right-handed users, Lecture Notes in Computer Science, 8518 LNCS, PART 2, pp. 173-183, (2014); Bace, Mihai, Quantification of Users' Visual Attention during Everyday Mobile Device Interactions, Conference on Human Factors in Computing Systems - Proceedings, (2020); Baltrušaitis, Tadas, Cross-dataset learning and person-specific normalisation for automatic Action Unit detection, 2015-January, (2015); Baltrušaitis, Tadas, Constrained local neural fields for robust facial landmark detection in the wild, Proceedings of the IEEE International Conference on Computer Vision, pp. 354-361, (2013); Baltrušaitis, Tadas, OpenFace 2.0: Facial behavior analysis toolkit, pp. 59-66, (2018); Manual for the Beck Depression Inventory, (1996); Belouali, Anas, Acoustic and language analysis of speech for suicidal ideation among US veterans, BioData Mining, 14, 1, (2021); Bertolote, Josè Manoel, Suicide and mental disorders: Do we know enough?, British Journal of Psychiatry, 183, NOV., pp. 382-383, (2003); Applying Machine Learning Techniques to Improve User Acceptance on Ubiquitous Environement, (2013)","","Association for Computing Machinery","ACM SIGCHI","2024 CHI Conference on Human Factors in Computing Sytems, CHI 2024","","Hybrid, Honolulu; HI","199441","","1581132484; 0897913833; 9781595936424; 1581136374; 9781450346559; 9781595930026; 9781605589312; 9781450356206; 9781450310161; 9781581137033","","","English","Conference paper","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85194831407"
"Hwang, S.H.; Yu, Y.; Kim, J.; Lee, T.; Park, Y.R.; Kim, H.-W.","Hwang, Sangho (57871467900); Yu, Yeonsoo (57743565100); Kim, Jichul (57280967300); Lee, Taeyeop (57198752381); Park, Yu-rang (36841202200); Kim, Hyo-won (13402798900)","57871467900; 57743565100; 57280967300; 57198752381; 36841202200; 13402798900","A Study on the Screening of Children at Risk for Developmental Disabilities Using Facial Landmarks Derived From a Mobile-Based Application","2024","Psychiatry Investigation","21","5","","496","505","0","0","10.30773/pi.2023.0315","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195153004&doi=10.30773%2Fpi.2023.0315&partnerID=40&md5=db13a75f312968c653e67ea6ea3f0207","Department of Biomedical Systems Informatics, Yonsei University College of Medicine, Seoul, South Korea; University of Ulsan College of Medicine, Seoul, South Korea; Department of Psychiatry, Asan Medical Center, Seoul, South Korea","Hwang, Sangho, Department of Biomedical Systems Informatics, Yonsei University College of Medicine, Seoul, South Korea; Yu, Yeonsoo, University of Ulsan College of Medicine, Seoul, South Korea; Kim, Jichul, Department of Psychiatry, Asan Medical Center, Seoul, South Korea; Lee, Taeyeop, Department of Psychiatry, Asan Medical Center, Seoul, South Korea; Park, Yu-rang, Department of Biomedical Systems Informatics, Yonsei University College of Medicine, Seoul, South Korea; Kim, Hyo-won, Department of Psychiatry, Asan Medical Center, Seoul, South Korea","Objective Early detection and intervention of developmental disabilities (DDs) are critical to improving the long-term outcomes of afflicted children. In this study, our objective was to utilize facial landmark features from mobile application to distinguish between children with DDs and typically developing (TD) children. Methods The present study recruited 89 children, including 33 diagnosed with DD, and 56 TD children. The aim was to examine the effectiveness of a deep learning classification model using facial video collected from children through mobile-based application. The study participants underwent comprehensive developmental assessments, which included the child completion of the Korean Psychoeducational Profile-Revised and caregiver completing the Korean versions of Vineland Adaptive Behavior Scale, Korean version of the Childhood Autism Rating Scale, Social Responsiveness Scale, and Child Behavior Checklist. We extracted facial landmarks from recorded videos using mobile application and performed DDs classification using long short-term memory with stratified 5-fold cross-validation. Results The classification model shows an average accuracy of 0.88 (range: 0.78–1.00), an average precision of 0.91 (range: 0.75–1.00), and an average F1-score of 0.80 (range: 0.60–1.00). Upon interpreting prediction results using SHapley Additive exPlanations (SHAP), we verified that the most crucial variable was the nodding head angle variable, with a median SHAP score of 2.6. All the top 10 contributing variables exhibited significant differences in distribution between children with DD and TD (p<0.05). Conclusion The results of this study provide evidence that facial landmarks, utilizing readily available mobile-based video data, can be used to detect DD at an early stage. © 2024 Elsevier B.V., All rights reserved.","Artificial intelligence; Autism; Developmental disability; Facial landmarks; Screening","accuracy; algorithm; anatomical landmark; Article; attention deficit hyperactivity disorder; autism; caregiver; cerebral palsy; child; Child Behavior Checklist; Childhood Autism Rating Scale; chin; computer assisted tomography; controlled study; data collection method; day care; developmental disorder; eye; facial recognition; female; head movement; histology; human; information processing; Korean (people); learning; long short term memory network; machine learning; major clinical study; male; mouth; nose; physician; preschool child; questionnaire; rank sum test; reliability; risk; scoring system; soft tissue; speech intelligibility; statistical analysis; Student t test; validation process; videorecording","","","This work was supported by the National Research Foundation of Korea (NRF) grant funded by the South Korean government (Ministry of Science and ICT) (NRF-2020R1A5A8017671).","Developmental Disabilities Delivery of Medical Care for Children and Adults, (1989); Zablotsky, Benjamin, Prevalence and trends of developmental disabilities among children in the United States: 2009–2017, Pediatrics, 144, 4, (2019); Rah, Sungsil, Prevalence and Incidence of Developmental Disorders in Korea: A Nationwide Population-Based Study, Journal of Autism and Developmental Disorders, 50, 12, pp. 4504-4511, (2020); Rattaz, Cécile, Quality of Life in Parents of Young Adults with ASD: EpiTED Cohort, Journal of Autism and Developmental Disorders, 47, 9, pp. 2826-2837, (2017); Cakir, Janet, The lifetime social cost of autism: 1990–2029, Research in Autism Spectrum Disorders, 72, (2020); Genereaux, Dallas, Costs of caring for children with an intellectual developmental disorder, Disability and Health Journal, 8, 4, pp. 646-651, (2015); Lee, Taeyeop, Predictors of Developmental Outcome in 4-to 6-Year-Olds With Developmental Disability, Psychiatry Investigation, 19, 7, pp. 519-526, (2022); Kakchapati, Sampurna, Factors associated with early child development in Nepal - A further analysis of multiple indicator cluster survey 2019, Asian Journal of Social Health and Behavior, 6, 1, pp. 21-29, (2023); McKenzie, Karen, Screening for intellectual disability in children: A review of the literature, Journal of Applied Research in Intellectual Disabilities, 25, 1, pp. 80-87, (2012); von Suchodoletz, Waldemar, Early identification of children with developmental language disorders - When and how?, Zeitschrift fur Kinder- und Jugendpsychiatrie und Psychotherapie, 39, 6, pp. 377-385, (2011)","","Korean Neuropsychiatric Association","","","","","","17383684; 19763026","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85195153004"
"Al Khuzayem, L.; Shafi, S.; Aljahdali, S.; Alkhamesie, R.; Alzamzami, O.","Al Khuzayem, Lama A. (55616483700); Shafi, Suha (59144998800); Aljahdali, Safia (59144998900); Alkhamesie, Rawan (59144999000); Alzamzami, Ohoud (57191410424)","55616483700; 59144998800; 59144998900; 59144999000; 57191410424","Efhamni: A Deep Learning-Based Saudi Sign Language Recognition Application","2024","Sensors","24","10","3112","","","0","13","10.3390/s24103112","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194218783&doi=10.3390%2Fs24103112&partnerID=40&md5=f56b4c8b8464f2f68ec2241206378898","Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia","Al Khuzayem, Lama A., Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; Shafi, Suha, Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; Aljahdali, Safia, Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; Alkhamesie, Rawan, Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; Alzamzami, Ohoud, Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia","Deaf and hard-of-hearing people mainly communicate using sign language, which is a set of signs made using hand gestures combined with facial expressions to make meaningful and complete sentences. The problem that faces deaf and hard-of-hearing people is the lack of automatic tools that translate sign languages into written or spoken text, which has led to a communication gap between them and their communities. Most state-of-the-art vision-based sign language recognition approaches focus on translating non-Arabic sign languages, with few targeting the Arabic Sign Language (ArSL) and even fewer targeting the Saudi Sign Language (SSL). This paper proposes a mobile application that helps deaf and hard-of-hearing people in Saudi Arabia to communicate efficiently with their communities. The prototype is an Android-based mobile application that applies deep learning techniques to translate isolated SSL to text and audio and includes unique features that are not available in other related applications targeting ArSL. The proposed approach, when evaluated on a comprehensive dataset, has demonstrated its effectiveness by outperforming several state-of-the-art approaches and producing results that are comparable to these approaches. Moreover, testing the prototype on several deaf and hard-of-hearing users, in addition to hearing users, proved its usefulness. In the future, we aim to improve the accuracy of the model and enrich the application with more features. © 2024 Elsevier B.V., All rights reserved.","CNN; deep learning; MobileNet; pose estimation; saudi sign language; sign language recognition","Deep learning; Mobile computing; Translation (languages); Arabic sign language; Hand gesture; Hard of hearings; Mobile applications; Mobilenet; Pose-estimation; Saudi sign languages; Sign language; Sign Language recognition; Audition; deep learning; hearing impaired person; hearing impairment; human; mobile application; pathophysiology; Saudi Arabia; sign language; Deafness; Deep Learning; Humans; Mobile Applications; Persons With Hearing Impairments; Sign Language","","","","Deafness and Hearing Loss, (2024); People with Disabilities Survey, (2018); Al-Obodi, Alaa H., A saudi sign language recognition system based on convolutional neural networks, International Journal of Engineering Research and Technology, 13, 11, pp. 3328-3334, (2020); Adeyanju, Ibrahim Adepoju, Machine learning methods for sign language recognition: A critical review and analysis, Intelligent Systems with Applications, 12, (2021); Amangeldy, Nurzada, Continuous Sign Language Recognition and Its Translation into Intonation-Colored Speech, Sensors, 23, 14, (2023); Tripathi, Kumud, Continuous Indian Sign Language Gesture Recognition and Sentence Formation, Procedia Computer Science, 54, pp. 523-531, (2015); Kumar Attar, Rakesh, State of the Art of Automation in Sign Language: A Systematic Review, ACM Transactions on Asian and Low-Resource Language Information Processing, 22, 4, (2023); Alsharif, Bader, Deep Learning Technology to Recognize American Sign Language Alphabet, Sensors, 23, 18, (2023); Alharthi, Nojood M., Vision Transformers and Transfer Learning Approaches for Arabic Sign Language Recognition, Applied Sciences (Switzerland), 13, 21, (2023); Al-Hammadi, Muneer H., Deep learning-based approach for sign language gesture recognition with efficient hand gesture representation, IEEE Access, 8, pp. 192527-192542, (2020)","","Multidisciplinary Digital Publishing Institute (MDPI)","","","","","","14248220","","","38793964","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85194218783"
"Ciancia, S.; Madeo, S.F.; Calabrese, O.; Iughetti, L.","Ciancia, Silvia (57209209316); Madeo, Simona Filomena (57208356277); Calabrese, Olga (6603410611); Iughetti, Lorenzo (57193233990)","57209209316; 57208356277; 6603410611; 57193233990","The Approach to a Child with Dysmorphic Features: What the Pediatrician Should Know","2024","Children","11","5","578","","","0","0","10.3390/children11050578","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193908332&doi=10.3390%2Fchildren11050578&partnerID=40&md5=180a14b68f4bd3cb93dd56f6d1f3b756","Department of Medical and Surgical Sciences for Mother, Università degli Studi di Modena e Reggio Emilia, Modena, Italy; Department of Medical and Surgical Sciences for Mother, Università degli Studi di Modena e Reggio Emilia, Modena, Italy","Ciancia, Silvia, Department of Medical and Surgical Sciences for Mother, Università degli Studi di Modena e Reggio Emilia, Modena, Italy; Madeo, Simona Filomena, Department of Medical and Surgical Sciences for Mother, Università degli Studi di Modena e Reggio Emilia, Modena, Italy; Calabrese, Olga, Department of Medical and Surgical Sciences for Mother, Università degli Studi di Modena e Reggio Emilia, Modena, Italy; Iughetti, Lorenzo, Department of Medical and Surgical Sciences for Mother, Università degli Studi di Modena e Reggio Emilia, Modena, Italy","The advancement of genetic knowledge and the discovery of an increasing number of genetic disorders has made the role of the geneticist progressively more complex and fundamental. However, most genetic disorders present during childhood; thus, their early recognition is a challenge for the pediatrician, who will be also involved in the follow-up of these children, often establishing a close relationship with them and their families and becoming a referral figure. In this review, we aim to provide the pediatrician with a general knowledge of the approach to treating a child with a genetic syndrome associated with dysmorphic features. We will discuss the red flags, the most common manifestations, the analytic collection of the family and personal medical history, and the signs that should alert the pediatrician during the physical examination. We will offer an overview of the physical malformations most commonly associated with genetic defects and the way to describe dysmorphic facial features. We will provide hints about some tools that can support the pediatrician in clinical practice and that also represent a useful educational resource, either online or through apps downloaded on a smartphone. Eventually, we will offer an overview of genetic testing, the ethical considerations, the consequences of incidental findings, and the main indications and limitations of the principal technologies. © 2024 Elsevier B.V., All rights reserved.","congenital malformation; dysmorphic feature; genetic syndrome; genetic testing; intellectual disability; neurodevelopmental delay","artificial intelligence; autism; Becker muscular dystrophy; body dysmorphic disorder; body mass; camptodactyly; chromosome rearrangement; clinical practice; comparative genomic hybridization; developmental delay; epilepsy; face dysmorphia; fluorescence in situ hybridization; frontal bossing; gene mutation; genetic analysis; genetic disorder; head circumference; health insurance; heterozygosity; human; hyperphagia; hypertelorism; hypospadias; hypotelorism; karyotype; karyotyping; lymphocytic choriomeningitis; Lymphocytic choriomeningitis virus; macrocephaly; macroglossia; microcephaly; Noonan syndrome; obesity; osteogenesis imperfecta; phenotype; prenatal screening; quality of life; Review; short stature; spinal muscular atrophy; syndactyly; toxoplasmosis; tumor suppressor gene; whole exome sequencing; whole genome sequencing; Zika virus","","","","Baird, Patricia A., Genetic disorders in children and young adults: A population study, American Journal of Human Genetics, 42, 5, pp. 677-693, (1988); Verma, Ishwar Chander, Global burden of genetic disease and the role of genetic screening, Seminars in Fetal and Neonatal Medicine, 20, 5, pp. 354-363, (2015); Cooper, Linda J., Feeding and swallowing dysfunction in genetic syndromes, Developmental Disabilities Research Reviews, 14, 2, pp. 147-157, (2008); Prasad, Asuri Narayan, Genetic evaluation of the floppy infant, Seminars in Fetal and Neonatal Medicine, 16, 2, pp. 99-108, (2011); Oliveira, Priscila H.A., Genetic syndromes associated with congenital cardiac defects and ophthalmologic changes – Systematization for diagnosis in the clinical practice, Arquivos Brasileiros de Cardiologia, 110, 1, pp. 84-90, (2018); Pauli, Richard M., Achondroplasia: A comprehensive clinical review, Orphanet Journal of Rare Diseases, 14, 1, (2019); Girirajan, Santhosh D., Phenotypic variability and genetic susceptibility to genomic disorders, Human Molecular Genetics, 19, R2, pp. R176-R187, (2010); McDonald-McGinn, Donna M., 22q11.2 deletion syndrome, Nature Reviews Disease Primers, 1, (2015); Linglart, Léa, Congenital heart defects in Noonan syndrome: Diagnosis, management, and treatment, American Journal of Medical Genetics, Part C: Seminars in Medical Genetics, 184, 1, pp. 73-80, (2020); Villani, Anita, Recommendations for cancer surveillance in individuals with RASopathies and other rare genetic conditions with increased cancer risk, Clinical Cancer Research, 23, 12, pp. e83-e90, (2017)","","Multidisciplinary Digital Publishing Institute (MDPI)","","","","","","22279067","","","","English","Review","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85193908332"
"Panda, P.K.; Elwadhi, A.; Gupta, D.; Palayullakandi, A.; Tomar, A.; Singh, M.; Vyas, A.; Kumar, D.; Sharawat, I.K.","Panda, Prateek Kumar (57210117271); Elwadhi, Aman (57201761370); Gupta, Diksha (58519845600); Palayullakandi, Achanya (57490108300); Tomar, Apurva (57223160861); Singh, Mayank (59165920400); Vyas, Antara (59166437800); Kumar, Deepak (59440767200); Sharawat, I. K. (56707448400)","57210117271; 57201761370; 58519845600; 57490108300; 57223160861; 59165920400; 59166437800; 59440767200; 56707448400","Effectiveness of IMPUTE ADT-1 mobile application in children with autism spectrum disorder: An interim analysis of an ongoing randomized controlled trial","2024","Iranian Journal of Materials Science and Engineering","15","2","","262","269","0","0","10.25259/JNRP_599_2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195593361&doi=10.25259%2FJNRP_599_2023&partnerID=40&md5=04ecb9dcf11a5b0dd86497d39adda6b0","Department of Pediatrics, All India Institute of Medical Sciences Rishikesh, Rishikesh, India; Department of Digital Medicine, IMPUTE Inc, Tokyo, Japan; Department of Pediatrics, All India Institute of Medical Sciences Rishikesh, Rishikesh, India","Panda, Prateek Kumar, Department of Pediatrics, All India Institute of Medical Sciences Rishikesh, Rishikesh, India; Elwadhi, Aman, Department of Pediatrics, All India Institute of Medical Sciences Rishikesh, Rishikesh, India; Gupta, Diksha, Department of Pediatrics, All India Institute of Medical Sciences Rishikesh, Rishikesh, India; Palayullakandi, Achanya, Department of Pediatrics, All India Institute of Medical Sciences Rishikesh, Rishikesh, India; Tomar, Apurva, Department of Pediatrics, All India Institute of Medical Sciences Rishikesh, Rishikesh, India; Singh, Mayank, Department of Digital Medicine, IMPUTE Inc, Tokyo, Japan; Vyas, Antara, Department of Digital Medicine, IMPUTE Inc, Tokyo, Japan; Kumar, Deepak, Department of Pediatrics, All India Institute of Medical Sciences Rishikesh, Rishikesh, India; Sharawat, I. K., Department of Pediatrics, All India Institute of Medical Sciences Rishikesh, Rishikesh, India","Objectives: IMPUTE Inc., a software firm dedicated to healthcare technology, has developed a mobile medical application known as IMPUTE ADT-1 for children with autism spectrum disorder (ASD) based on the principle of applied behavior analysis. Materials and Methods: The primary objective of this trial was to compare the efficacy of add-on treatment with IMPUTE ADT-1 in children with ASD aged two to six years as compared to standard care alone for 12 weeks (in terms of change in Autism Diagnostic Observation Schedule [ADOS-2] scores). The secondary objective of the study was to assess the compliance with IMPUTE ADT-1 among participants and also to evaluate the feedback of parents regarding IMPUTE ADT-1 at the end of 12 weeks. The application provides personalized programs tailored to each user’s needs, and the program evolves based on the user’s progress. It also utilizes face tracking, eye tracking, and body tracking to gather behavior-related information for each child and apply it in reinforcement learning employing artificial intelligence-based algorithms. Results: Till the time of interim analysis, 37 and 33 children had completed 12-week follow-up in IMPUTE ADT-1 and control arm. At 12 weeks, as compared to baseline, change in social affect domain, repetitive ritualistic behavior domain, total ADOS-2 score, and ADOS-2 comparison score was better in the intervention group as compared to the control group (P < 0.001 for all). A total of 30 (81%), 28 (75%), and 29 (78%) caregivers in the IMPUTE ADT-1 group believed that the ADT-1 app improved their child’s verbal skills, social skills, and reduced repetitive behavior, respectively. Conclusion: IMPUTE ADT-1 mobile application has the efficacy to improve the severity of autism symptoms in children. Parents of these children also feel that the application is beneficial for improving the socialization and verbal communication of their children. © 2024 Elsevier B.V., All rights reserved.","Applied behavior analysis; Artificial intelligence; Autism; Mobile technology; Neurodevelopmental disorders","","","","This research was funded by Impute Inc., Tokyo, Japan","Hirota, Tomoya, Autism Spectrum Disorder: A Review, JAMA, 329, 2, pp. 157-168, (2023); Foxx, Richard M., Applied Behavior Analysis Treatment of Autism: The State of the Art, Child and Adolescent Psychiatric Clinics of North America, 17, 4, pp. 821-834, (2008); Payakachat, Nalin, Autism spectrum disorders: A review of measures for clinical, health services and cost-effectiveness applications, Expert Review of Pharmacoeconomics and Outcomes Research, 12, 4, pp. 485-503, (2012); Kakooza-Mwesige, Angelina, The need to improve autism services in lower-resource settings, The Lancet, 399, 10321, pp. 217-220, (2022); Song, Da Yea, The Use of Artificial Intelligence in Screening and Diagnosis of Autism Spectrum Disorder: A Literature Review, Journal of the Korean Academy of Child and Adolescent Psychiatry, 30, 4, pp. 145-152, (2019); Moon, Sunjae, Mobile device applications and treatment of autism spectrum disorder: A systematic review and meta-analysis of effectiveness, Archives of Disease in Childhood, 105, 5, pp. 458-462, (2020); Sennott, Samuel C., AAC and Artificial Intelligence (AI), Topics in Language Disorders, 39, 4, pp. 389-403, (2019); Greene, Rachel K., Autism Diagnostic Observation Schedule (ADOS-2) elevations in a clinical sample of children and adolescents who do not have autism: Phenotypic profiles of false positives, Clinical Neuropsychologist, 36, 5, pp. 943-959, (2022); Maddox, Brenna Burns, The Accuracy of the ADOS-2 in Identifying Autism among Adults with Complex Psychiatric Conditions, Journal of Autism and Developmental Disorders, 47, 9, pp. 2703-2709, (2017); Kamp-Becker, Inge, Diagnostic accuracy of the ADOS and ADOS-2 in clinical practice, European Child and Adolescent Psychiatry, 27, 9, pp. 1193-1207, (2018)","","Iran University of Science and Technology","","","","","","17350808","","","","English","Article","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85195593361"
"Li, Y.","Li, Ying (58855280900)","58855280900","Analysis of Multimedia Recognition of Piano Playing Music Based on Fuzzy Neural Network","2024","Informatica (Slovenia)","48","7","","123","136","0","0","10.31449/INF.V48I7.5288","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195046007&doi=10.31449%2FINF.V48I7.5288&partnerID=40&md5=f80bfc0ddd87ccd044f659d8343e11a8","School of Accounting and Auditing, Guangxi Vocational Normal University, Nanning, China","Li, Ying, School of Accounting and Auditing, Guangxi Vocational Normal University, Nanning, China","Artistic aspects and creative talents abound in piano playing, making it a sort of unexpected conceptual art. It's a must-have for transporting those luscious piano tones. A striking demonstration of piano playing abilities is the creation of musical emotion and expressiveness. It is important to focus on honing and adapting one's performing abilities while playing the Piano. It is completely grounded in the aesthetic qualities of piano playing, guaranteeing the fullness and beauty of the musical performance. The piano music occurs throughout effectiveness even though its basic frequencies vary substantially during the acquisition procedure for the layout. In this study, we suggested a multimodal recognizing technique that uses piano music and a fuzzy neural network to address the issue of a poor recognition rate. We use a fuzzy neural network with a flexible architecture as a starting point for a program that might teach users to play Piano in a variety of genres. A smart mobile application for playing the Piano and playing games is developed using the network's differential capabilities. With its ability to fully use the benefits of Android's strong voice functionality, this system is a new kind of artificial intelligence (AI) software that combines the training, entertainment, and instruction of the Piano with the platform's other strengths. After analyzing research observations, we find that the suggested technique has provided an accuracy of 94%. © 2024 Elsevier B.V., All rights reserved.","artistic aspects; fuzzy neural network; multimedia recognition; musical emotion; piano playing","Emotion Recognition; Fuzzy inference; Music; Musical instruments; Aesthetic qualities; Artistic aspect; Creatives; Fuzzy-neural-networks; Multimedium recognition; Musical emotion; Musical expressiveness; Musical performance; Piano music; Piano playing; Fuzzy neural networks","","","","Zhu, Ying, Multimedia Recognition of Piano Music Based on the Hidden Markov Model, Advances in Multimedia, 2021, (2021); Niu, Yitian, Penetration of Multimedia Technology in Piano Teaching and Performance Based on Complex Network, Mathematical Problems in Engineering, 2021, (2021); Wang, Dongkui, Piano Intelligent Teaching Evaluation with IoT and Multimedia Technology, Mobile Information Systems, 2022, (2022); Zheng, Yundan, Training Strategy of Music Expression in Piano Teaching and Performance by Intelligent Multimedia Technology, International Transactions on Electrical Energy Systems, 2022, (2022); Wang, Tianshu, Neural Network-Based Dynamic Segmentation and Weighted Integrated Matching of Cross-Media Piano Performance Audio Recognition and Retrieval Algorithm, Computational Intelligence and Neuroscience, 2022, (2022); Luo, Wanshu, Toward Piano Teaching Evaluation Based on Neural Network, Scientific Programming, 2022, (2022); Gong, Luxin, Interactive Learning Environment for Effective Music Learning in Chinese Primary and Secondary Schools, Sustainability (Switzerland), 15, 3, (2023); Sun, Sibing, Evaluation of Potential Correlation of Piano Teaching Using Edge-Enabled Data and Machine Learning, Mobile Information Systems, 2021, (2021); Yu, Xiaofei, Developments and Applications of Artificial Intelligence in Music Education, Technologies, 11, 2, (2023); Wang, Xiaolu, Music teaching platform based on FPGA and neural network, Microprocessors and Microsystems, 80, (2021)","","Slovene Society Informatika","","","","","","18543871; 03505596","","INFOF","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85195046007"
"Toki, E.I.; Pange, J.; Tatsis, G.; Plachouras, K.; Tsoulos, I.G.","Toki, Eugenia I. (36544342700); Pange, Jenny (6507161641); Tatsis, Giorgos (35488955700); Plachouras, Konstantinos (57200880847); Tsoulos, Ioannis G. (10042633500)","36544342700; 6507161641; 35488955700; 57200880847; 10042633500","Utilizing Constructed Neural Networks for Autism Screening","2024","Applied Sciences (Switzerland)","14","7","3053","","","0","6","10.3390/app14073053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192568865&doi=10.3390%2Fapp14073053&partnerID=40&md5=c8f9fef56cfb8a0408ad271d13602b7b","Department of Speech and Language Therapy, University of Ioannina, Ioannina, Greece; Department of Early Childhood Education, University of Ioannina, Ioannina, Greece; Department of Physics, University of Ioannina, Ioannina, Greece; Department of Informatics and Telecommunications, University of Ioannina, Ioannina, Greece","Toki, Eugenia I., Department of Speech and Language Therapy, University of Ioannina, Ioannina, Greece, Department of Early Childhood Education, University of Ioannina, Ioannina, Greece; Pange, Jenny, Department of Early Childhood Education, University of Ioannina, Ioannina, Greece; Tatsis, Giorgos, Department of Speech and Language Therapy, University of Ioannina, Ioannina, Greece, Department of Physics, University of Ioannina, Ioannina, Greece; Plachouras, Konstantinos, Department of Speech and Language Therapy, University of Ioannina, Ioannina, Greece; Tsoulos, Ioannis G., Department of Informatics and Telecommunications, University of Ioannina, Ioannina, Greece","Autism Spectrum Disorder is known to cause difficulties in social interaction and communication, as well as repetitive patterns of behavior, interests, or hobbies. These challenges can significantly affect the individual’s daily life. Therefore, it is crucial to identify and assess children with Autism Spectrum Disorder early to significantly benefit the long-term health of children. Unfortunately, many children are not diagnosed or are misdiagnosed, which means they miss out on the necessary interventions. Clinicians and other experts face various challenges during the diagnostic process. Digital tools can facilitate early diagnosis effectively. This study aimed to explore the use of machine learning techniques on a dataset collected from a serious game designed for children with autism to investigate how these techniques can assist in classification and make the clinical process more efficient. The responses were gathered from children who participated in interactive games deployed on mobile devices, and the data were analyzed using various types of neural networks, such as multilayer perceptrons and constructed neural networks. The performance metrics of these models, including error rate, precision, and recall, were reported, and the comparative experiments revealed that the constructed neural network using the integer rule-based neural networks approach was superior. Based on the evaluation metrics, this method showed the lowest error rate of 11.77%, a high accuracy of 0.75, and a good recall of 0.66. Thus, it can be an effective way to classify both typically developed children and children with Autism Spectrum Disorder. Additionally, it can be used for automatic screening procedures in an intelligent system. The results indicate that clinicians could use these techniques to enhance conventional screening methods and contribute to providing better care for individuals with autism. © 2024 Elsevier B.V., All rights reserved.","Autism Spectrum Disorder (ASD); classification; constructed neural networks; machine learning; screening","","","","","Hyman, Susan L., Identification, evaluation, and management of children with autism spectrum disorder, Pediatrics, 145, 1, (2020); Hobson, Hannah M., Supporting the mental health of children with speech, language and communication needs: The views and experiences of parents, Autism and Developmental Language Impairments, 7, (2022); Text Revision Dsm IV TR, (1994); Plaza-Díaz, Julio, Physical Activity, Gut Microbiota, and Genetic Background for Children and Adolescents with Autism Spectrum Disorder, Children, 9, 12, (2022); Zeidan, Jinan, Global prevalence of autism: A systematic review update, Autism Research, 15, 5, pp. 778-790, (2022); Elsabbagh, M., The time has come for living systematic reviews in autism research, Autism Research, 15, 7, pp. 1187-1188, (2022); Abdullah, Azian Azamimi, Evaluation on Machine Learning Algorithms for Classification of Autism Spectrum Disorder (ASD), Journal of Physics: Conference Series, 1372, 1, (2019); Thabtah, F. Abdeljaber, An accessible and efficient autism screening method for behavioural data and predictive analyses, Health Informatics Journal, 25, 4, pp. 1739-1755, (2019); Rice, Catherine E., Defining in Detail and Evaluating Reliability of DSM-5 Criteria for Autism Spectrum Disorder (ASD) Among Children, Journal of Autism and Developmental Disorders, 52, 12, pp. 5308-5320, (2022); HOWLIN, P., Adults with Autism: Changes in Understanding Since DSM-111, Journal of Autism and Developmental Disorders, 51, 12, pp. 4291-4308, (2021)","","Multidisciplinary Digital Publishing Institute (MDPI)","","","","","","20763417","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85192568865"
"Wagner, W.; Ulrich, S.; Mario, B.","Wagner, Nicolas (57221691055); Ulrich, Schwanecke (58924038300); Mario, Botsch (58924038400)","57221691055; 58924038300; 58924038400","SparseSoftDECA — Efficient high-resolution physics-based facial animation from sparse landmarks","2024","Computers and Graphics","119","","103903","","","0","4","10.1016/j.cag.2024.103903","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186961967&doi=10.1016%2Fj.cag.2024.103903&partnerID=40&md5=b48915752169309c74bf97b1bb854439","Technische Universität Dortmund, Dortmund, Germany; Fachhochschule Wiesbaden, Wiesbaden, Germany","Wagner, Nicolas, Technische Universität Dortmund, Dortmund, Germany; Ulrich, Schwanecke, Fachhochschule Wiesbaden, Wiesbaden, Germany; Mario, Botsch, Technische Universität Dortmund, Dortmund, Germany","Facial animation on computationally limited systems still heavily relies on linear blendshape models. Nonetheless, these models exhibit common issues like volume loss, self-collisions, and inaccuracies in soft tissue elasticity. Furthermore, personalizing blendshapes models demands significant effort, but there are limited options for simulating or manipulating physical and anatomical characteristics afterwards. Also, second-order dynamics can only be partially represented. For many years, physics-based facial simulations have been explored as an alternative to linear blendshapes, however, those remain cumbersome to implement and result in a high computational burden. We present a novel deep learning approach that offers the advantages of physics-based facial animations while being effortless and fast to use on top of linear blendshapes. For this, we design an innovative hypernetwork that efficiently approximates a physics-based facial simulation while generalizing over the extensive DECA model of human identities, facial expressions, and a wide range of material properties that can be locally adjusted without re-training. In addition to our previous work, we also demonstrate how the hypernetwork can be applied to facial animation from a sparse set of tracked landmarks. Unlike before, we no longer require linear blendshapes as the foundation of our system but directly operate on neutral head representations. This application is also used to complement an existing framework for commodity smartphones that already implements high resolution scanning of neutral faces and expression tracking. © 2024 Elsevier B.V., All rights reserved.","Deep learning; Facial animation; Physics-based simulation","Animation; Blendshapes; Deep learning; Facial animation; High resolution; Hypernetwork; Physics-based; Physics-based Simulation; Soft tissue; Tissue elasticity; Volume loss","","","This research was supported by the German Federal Ministry of Education and Research (BMBF) through the project HiAvA (ID 16SV8785 ).","Cao, Chen, Authentic volumetric avatars from a phone scan, ACM Transactions on Graphics, 41, 4, (2022); undefined; undefined; Zielonka, Wojciech, Instant Volumetric Head Avatars, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2023-June, pp. 4574-4584, (2023); Eurographics State of the Art Reports, (2014); Ichim, Alexandru Eugen, Phace: Physics-based face modeling and animation, ACM Transactions on Graphics, 36, 4, (2017); undefined; Symposium on Computer Animation, (2016); undefined; Yang, Lingchen, Implicit neural representation for physics-driven actuated soft bodies, ACM Transactions on Graphics, 41, 4, (2022)","","Elsevier Ltd","","","","","","00978493","","COGRD","","English","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85186961967"
"Loftness, B.C.; Halvorson-Phelan, J.; O'Leary, A.; Bradshaw, C.; Prytherch, S.; Berman, I.; Torous, J.; Copeland, W.E.; Cheney, N.; McGinnis, R.S.; McGinnis, E.W.","Loftness, Bryn C. (57345878500); Halvorson-Phelan, Julia (57892250400); O'Leary, Aisling (57442943900); Bradshaw, Carter (58098532600); Prytherch, Shania (58098730700); Berman, Isabel (58107174500); Torous, John Blake (55816955800); Copeland, William E. (7006006867); Cheney, Nick A. (55838037900); McGinnis, Ryan S. (37077700000); McGinnis, Ellen Waxler (56113176400)","57345878500; 57892250400; 57442943900; 58098532600; 58098730700; 58107174500; 55816955800; 7006006867; 55838037900; 37077700000; 56113176400","The ChAMP App: A Scalable mHealth Technology for Detecting Digital Phenotypes of Early Childhood Mental Health","2024","IEEE Journal of Biomedical and Health Informatics","28","4","","2304","2313","0","9","10.1109/JBHI.2023.3337649","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179034446&doi=10.1109%2FJBHI.2023.3337649&partnerID=40&md5=6d19ae6684311c8b219286eb6657117a","The University of Vermont, Burlington, United States; University of Vermont Medical Center, Burlington, United States; The University of Vermont, Burlington, United States; Harvard Medical School, Boston, United States; College of Engineering and Mathematical Sciences, Burlington, United States","Loftness, Bryn C., The University of Vermont, Burlington, United States; Halvorson-Phelan, Julia, University of Vermont Medical Center, Burlington, United States; O'Leary, Aisling, The University of Vermont, Burlington, United States; Bradshaw, Carter, University of Vermont Medical Center, Burlington, United States; Prytherch, Shania, University of Vermont Medical Center, Burlington, United States; Berman, Isabel, University of Vermont Medical Center, Burlington, United States; Torous, John Blake, Harvard Medical School, Boston, United States; Copeland, William E., University of Vermont Medical Center, Burlington, United States; Cheney, Nick A., College of Engineering and Mathematical Sciences, Burlington, United States; McGinnis, Ryan S., The University of Vermont, Burlington, United States; McGinnis, Ellen Waxler, University of Vermont Medical Center, Burlington, United States","Childhood mental health problems are common, impairing, and can become chronic if left untreated. Children are not reliable reporters of their emotional and behavioral health, and caregivers often unintentionally under- or over-report child symptoms, making assessment challenging. Objective physiological and behavioral measures of emotional and behavioral health are emerging. However, these methods typically require specialized equipment and expertise in data and sensor engineering to administer and analyze. To address this challenge, we have developed the ChAMP (Childhood Assessment and Management of digital Phenotypes) System, which includes a mobile application for collecting movement and audio data during a battery of mood induction tasks and an open-source platform for extracting digital biomarkers. As proof of principle, we present ChAMP System data from 101 children 4-8 years old, with and without diagnosed mental health disorders. Machine learning models trained on these data detect the presence of specific disorders with 70-73% balanced accuracy, with similar results to clinical thresholds on established parent-report measures (63-82% balanced accuracy). Features favored in model architectures are described using Shapley Additive Explanations (SHAP). Canonical Correlation Analysis reveals moderate to strong associations between predictors of each disorder and associated symptom severity (r =.51-.83). The open-source ChAMP System provides clinically-relevant digital biomarkers that may later complement parent-report measures of emotional and behavioral health for detecting kids with underlying mental health conditions and lowers the barrier to entry for researchers interested in exploring digital phenotyping of childhood mental health. © 2024 Elsevier B.V., All rights reserved.","adhd; anxiety; anxiety; depression; digital biomarkers; Digital health; machine learning; mobile health; pediatric mental health","Behavioral research; Biomarkers; Diagnosis; Information management; Learning systems; mHealth; Open systems; Pediatrics; Physiology; Adhd; Anxiety; Anxiety disorder; Behavioral science; Depression; Digital biomarker; Digital health; Machine-learning; Mental health; Mood; Pediatric mental health; Task analysis; E-learning; biological marker; accuracy; anxiety; anxiety disorder; Article; attention deficit hyperactivity disorder; autism; child; childhood; depression; digital health; digital health technology; disability severity; disease severity; emotional stability; entropy; female; health literacy; human; intellectual impairment; machine learning; male; mania; mental disease; mental health; mhealth technology; phenotype; phobia; Positive and Negative Syndrome Scale; prevalence; psychometry; questionnaire; sensitivity and specificity; social phobia; telehealth; training","","","This work was supported in part by the US NSF under Grant 2046440 and in part by the US NIH under Grant MH123031.","McGinnis, Ellen Waxler, Parental perception of mental health needs in young children, Child and Adolescent Mental Health, 27, 4, pp. 328-334, (2022); Comprehensive Guide to Autism, (2014); Gagne, Jeffrey R., Deriving Childhood Temperament Measures From Emotion-Eliciting Behavioral Episodes: Scale Construction and Initial Validation, Psychological Assessment, 23, 2, pp. 337-353, (2011); Lord, Catherine E., The Autism Diagnostic Observation Schedule-Generic: A standard measure of social and communication deficits associated with the spectrum of autism, Journal of Autism and Developmental Disorders, 30, 3, pp. 205-223, (2000); Sousa, Antonio Carlos, The Use of Wearable Technologies in the Assessment of Physical Activity in Preschool- and School-Age Youth: Systematic Review, International Journal of Environmental Research and Public Health, 20, 4, (2023); McTeague, Lisa M., The anxiety spectrum and the reflex physiology of defense: From circumscribed fear to broad distress, Depression and Anxiety, 29, 4, pp. 264-281, (2012); Durbin, C. Emily, Stability of laboratory-assessed temperamental emotionality traits from ages 3 to 7, Emotion, 7, 2, pp. 388-399, (2007); Kring, Ann M., The Facial Expression Coding System (FACES): Development, Validation, and Utility, Psychological Assessment, 19, 2, pp. 210-224, (2007); Laboratory Temperament Assessment Battery Preschool Version, (1996); Moser, Jason S., Combining Neural and Behavioral Indicators in the Assessment of Internalizing Psychopathology in Children and Adolescents, Journal of Clinical Child and Adolescent Psychology, 44, 2, pp. 329-340, (2015)","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21682208; 21682194","","ITIBF","38019617","English","Article","Final","All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85179034446"
"Wang, S.; Zhong, L.; Fu, Y.; Chen, L.; Ren, J.; Zhang, Y.","Wang, Shuning (58093709000); Zhong, Linghui (59437487200); Fu, Yongjian (58093782600); Chen, Lili (57189897856); Ren, Ju (57211853336); Zhang, Yaoxue (7601311414)","58093709000; 59437487200; 58093782600; 57189897856; 57211853336; 7601311414","UFace: Your Smartphone Can ""Hear""Your Facial Expression!","2024","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","8","1","22","","","0","10","10.1145/3643546","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192799516&doi=10.1145%2F3643546&partnerID=40&md5=143274543aca7bc0964dd86ca74284bd","School of Computer Science and Engineering, Central South University, Changsha, China; Tsinghua University, Beijing, China; Beijing National Research Center for Information Science and Technology, Beijing, China; Zhongguancun Laboratory, Beijing, China","Wang, Shuning, School of Computer Science and Engineering, Central South University, Changsha, China; Zhong, Linghui, School of Computer Science and Engineering, Central South University, Changsha, China; Fu, Yongjian, School of Computer Science and Engineering, Central South University, Changsha, China; Chen, Lili, Tsinghua University, Beijing, China; Ren, Ju, Beijing National Research Center for Information Science and Technology, Beijing, China, Zhongguancun Laboratory, Beijing, China; Zhang, Yaoxue, Beijing National Research Center for Information Science and Technology, Beijing, China, Zhongguancun Laboratory, Beijing, China","Facial expression recognition (FER) is a crucial task for human-computer interaction and a multitude of multimedia applications that typically call for friendly, unobtrusive, ubiquitous, and even long-term monitoring. Achieving such a FER system meeting these multi-requirements faces critical challenges, mainly including the tiny irregular non-periodic deformation of emotion movements, high variability in facial positions and severe self-interference caused by users' own other behavior. In this work, we present UFace, a long-term, unobtrusive and reliable FER system for daily life using acoustic signals generated by a portable smartphone. We design an innovative network model with dual-stream input based on the attention mechanism, which can leverage distance-time profile features from various viewpoints to extract fine-grained emotion-related signal changes, thus enabling accurate identification of many kinds of expressions. Meanwhile, we propose effective mechanisms to deal with a series of interference issues during actual use. We implement UFace prototype with a daily-used smartphone and conduct extensive experiments in various real-world environments. The results demonstrate that UFace can successfully recognize 7 typical facial expressions with an average accuracy of 87.8% across 20 participants. Besides, the evaluation of different distances, angles, and interferences proves the great potential of the proposed system to be employed in practical scenarios. © 2024 Elsevier B.V., All rights reserved.","Acoustic sensing; Deep learning; Facial expression recognition; Smartphone","Deep learning; Emotion Recognition; Face recognition; Human computer interaction; Acoustic sensing; Critical challenges; Facial expression recognition; Facial Expressions; Long term monitoring; Multimedia applications; Periodic deformation; Recognition systems; Smart phones; Smartphones","","","","Amesaka, Takashi, Facial expression recognition using ear canal transfer function, Proceedings - International Symposium on Wearable Computers, ISWC, pp. 1-9, (2019); Ando, Toshiyuki, CanalSense: Face-related movement recognition system based on sensing air pressure in ear canals, pp. 679-689, (2017); Barsoum, Emad, Training deep networks for facial expression recognition with crowd-sourced label distribution, pp. 279-283, (2016); Chen, Huijie, EchoTrack: Acoustic device-free hand tracking on smart phones, Proceedings - IEEE INFOCOM, (2017); Chen, Tuochao, NeckFace: Continuously Tracking Full Facial Expressions on Neck-mounted Wearables, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 5, 2, (2021); Chen, Yanjiao, WiFace: Facial Expression Recognition Using Wi-Fi Signals, IEEE Transactions on Mobile Computing, 21, 1, pp. 378-391, (2022); Cheng, Haiming, Push the limit of device-free acoustic sensing on commercial mobile devices, Proceedings - IEEE INFOCOM, 2021-May, (2021); Choi, Seokmin, PPGface: Like What You arewatching? earphones can ""feel"" your facial expressions, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 6, 2, (2022); Handbook of Cognition and Emotion, (1999); BMJ, (2004)","","Association for Computing Machinery","","","","","","24749567","","","","English","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85192799516"
"Baran, K.","Baran, Katarzyna R. (57191337665)","57191337665","ShuffleNet and XGBoost classifier for stress reactions detection","2024","Procedia Computer Science","246","C","","3771","3780","0","1","10.1016/j.procs.2024.09.179","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213381149&doi=10.1016%2Fj.procs.2024.09.179&partnerID=40&md5=b9407b2a3d97eeac4c3f53279f1aef7e","Politechnika Lubelska, Lublin, Poland","Baran, Katarzyna R., Politechnika Lubelska, Lublin, Poland","A huge problem of the society of the present century is stress. It is not possible to eliminate it completely, but it can be controlled and actions can be taken to eliminate it or build a way of coping with stress. Stress can manifest itself in a physiological as well as psychological form. It can be visible through facial expressions, which is related to emotions. Emotions, like stress, can be positive or negative. Therefore, the observation of a person's emotions can lead to the determination of their stress response and even the intensity of stress. Emotions allow to regulate stress sensations - increase their intensity or, on the contrary, get rid of or relieve stress. Observing the rapid development of machine learning, it is worth using the resources of neural networks for stress control and monitoring purposes. Many detection techniques use complex neural networks, which at the same time require large computer resources, are time-consuming and expensive. Therefore, it is important to use and develop lightweight neural networks that do not require huge computer resources. In addition, it is worth considering the use of expensive measuring devices, while there are many handheld devices with convergent parameters on the technical market. An example is a smartphone thermal imaging camera that can provide recording accuracy, and the acquired thermal images can be a source of data. With the whole thing in mind, in this paper, the author proposes a lightweight convolutional network drawing on ShuffleNet to detect stress from thermal images of faces expressing various emotional forms. The proposed network includes additional layers and the XGBoost classifier. As a result of the calculations, the accuracy in the multi-class classification reached 87%. The research confirmed the validity and effectiveness of the proposed network and the capabilities of the detection technique. Certainly, the subject of stress and light convolutional neural networks is developmental and will be developed by the author in the future. © 2024 Elsevier B.V., All rights reserved.","emotion recognition; lightweight convolutional neural network; machine learning; stress detection; thermovision","Hand held computers; Multilayer neural networks; Stress analysis; Stress relief; Thermography (imaging); Computer resources; Convolutional neural network; Emotion recognition; Lightweight convolutional neural network; Machine-learning; Neural-networks; ShuffleNets; Stress detection; Thermal images; Thermovision; Convolutional neural networks","","","","Hong, Kan, Non-contact physical stress measurement using thermal imaging and blind source separation, Optical Review, 27, 1, pp. 116-125, (2020); Pavlidis, I. T., Seeing through the face of deception: Thermal imaging offers a promising hands-off approach to mass security screening, Nature, 415, 6867, (2002); Pavlidis, I. T., Interacting with human physiology, Computer Vision and Image Understanding, 108, 1-2, pp. 150-170, (2007); Jarlier, Sophie, Thermal analysis of facial muscles contractions, IEEE Transactions on Affective Computing, 2, 1, pp. 2-9, (2011); Hong, Kan, Detection of physical stress using multispectral imaging, Neurocomputing, 329, pp. 116-128, (2019); Hong, Kan, Real-time stress assessment using thermal imaging, Visual Computer, 32, 11, pp. 1369-1377, (2016); Ebisch, Sjoerd J.H., Mother and child in synchrony: Thermal facial imprints of autonomic contagion, Biological Psychology, 89, 1, pp. 123-129, (2012); Ioannou, Stephanos, The autonomic signature of guilt in children: A thermal infrared imaging study, PLOS ONE, 8, 11, (2013); Wu, Haoyu, Eulerian video magnification for revealing subtle changes in the world, ACM Transactions on Graphics, 31, 4, (2012); Puri, Colin, Stresscam: Non-contact measurement of users' emotional states through thermal imaging, Conference on Human Factors in Computing Systems - Proceedings, pp. 1725-1728, (2005)","Flearmoy, J.","Elsevier B.V.","","28th International Conference on Knowledge Based and Intelligent information and Engineering Systems, KES 2024","","Seville","150888","18770509","9781510849914","","","English","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85213381149"
"Abd El-Sattar, H.K.H.; Omar, M.; Mohamady, H.","Abd El-Sattar, Hussein Karam (25824564900); Omar, Manal (58282631400); Mohamady, Hoda (57697531500)","25824564900; 58282631400; 57697531500","Developing a participatory research framework through serious games to promote learning for children with autism","2024","Frontiers in Education","9","","1453327","","","0","3","10.3389/feduc.2024.1453327","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211200476&doi=10.3389%2Ffeduc.2024.1453327&partnerID=40&md5=a0af2fbf6c3706aa334fdc68a2bc4b8f","Computer Science Division, College of Science, Cairo, Egypt; Department of Child Psychological Studies, Ain Shams University, Cairo, Egypt","Abd El-Sattar, Hussein Karam, Computer Science Division, College of Science, Cairo, Egypt; Omar, Manal, Department of Child Psychological Studies, Ain Shams University, Cairo, Egypt; Mohamady, Hoda, Department of Child Psychological Studies, Ain Shams University, Cairo, Egypt","People with autism, or Autism Spectrum Condition (ASC), is becoming increasingly common worldwide. Since individuals with ASC vary in their skills and methods that work for one may not work for another, many technology designers find it challenging to engage effectively with this population. Serious games (SGs) offer an intelligent learning environment that supports lifelong learning for individuals with ASC. Despite the availability of several frameworks, the question of whether SGs for individuals with ASC can have a dedicated framework remains unresolved. The objective of this study is to create a general framework for the design of serious games that can be applied to a variety of SGs targeting individuals with autism. A new participatory research framework is presented to assist game designers and relevant stakeholders in developing effective SGs for people with ASC. Through participatory sessions and a design thinking process, this framework seeks to involve users and relevant stakeholders as “design partners” in the design process. The framework was employed in the development of a new SG, called SALY (Simulation, Attention, Learn, and PLAY), designed to improve attention span and emotion recognition in individuals with ASC. Three research questions are discussed, and the mixed-methods approach adopted for the investigation. Several usability metrics were used to evaluate the game’s effectiveness, efficiency, and user satisfaction. The results show that the proposed game holds significant potential and will be of interest to educators and learners alike. © 2024 Elsevier B.V., All rights reserved.","autism; design thinking; machine learning; participatory research; serious games","","","","","El-Sattar, Hussein Karam Abd, EMOCASH: An Intelligent Virtual-Agent Based Multiplayer Online Serious Game for Promoting Money and Emotion Recognition Skills in Egyptian Children with Autism, International Journal of Advanced Computer Science and Applications, 14, 4, pp. 116-129, (2023); Abd El-Sattar, Hussein Karam, A New Learning Theory-based Framework for Combining Flow State with Game Elements to Promote Engagement and Learning in Serious Games, Information Sciences Letters, 12, 6, pp. 2663-2677, (2023); Abd El-Sattar, Hussein Karam, Future metaverse-based education to promote daily living activities in learners with autism using immersive technologies, Education and Information Technologies, 30, 3, pp. 3145-3182, (2025); Measuring the User Experience Collecting Analyzing and Presenting Usability Metrics, (2013); Almeida, Leandro M., ALTRIRAS: A Computer Game for Training Children with Autism Spectrum Disorder in the Recognition of Basic Emotions, International Journal of Computer Games Technology, 2019, (2019); Alves, Samanta, LifeisGame prototype: A serious game about emotions for children with autism spectrum disorders, PsychNology Journal, 11, 3, pp. 191-211, (2013); Alzubaidi, Laith, Review of deep learning: concepts, CNN architectures, challenges, applications, future directions, Journal of Big Data, 8, 1, (2021); Anselma, Manou, Not only adults can make good decisions, we as children can do that as well evaluating the process of the youth-led participatory action research ‘kids in action’, International Journal of Environmental Research and Public Health, 17, 2, (2020); Atherton, Gray, Seeing more than human: Autism and anthropomorphic theory of mind, Frontiers in Psychology, 9, APR, (2018); Bai, Qiyu, A Systematic Review of Emoji: Current Research and Future Perspectives, Frontiers in Psychology, 10, (2019)","","Frontiers Media SA","","","","","","2504284X","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85211200476"
"Su, S.-W.; Hung, C.-H.; Chen, L.-X.; Yuan, S.-M.","Su, Shihwen (57848225600); Hung, Chaohsiang (56806027400); Chen, Lixian (56840959900); Yuan, Shyan Ming (57135992000)","57848225600; 56806027400; 56840959900; 57135992000","Development of an AI-Based System to Enhance School Counseling Models for Asian Elementary Students With Emotional Disorders","2024","IEEE Access","12","","","159121","159136","0","6","10.1109/ACCESS.2024.3483456","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207743621&doi=10.1109%2FACCESS.2024.3483456&partnerID=40&md5=e13c077621ef7af2db3e3b7e33144436","Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; General Education Center, National Taiwan University of Arts, Banchiao, Taiwan; Department of Biomedical Informatics, China Medical University, Taichung, Taiwan","Su, Shihwen, Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Hung, Chaohsiang, General Education Center, National Taiwan University of Arts, Banchiao, Taiwan; Chen, Lixian, Department of Biomedical Informatics, China Medical University, Taichung, Taiwan; Yuan, Shyan Ming, Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan","In Asia, the availability of school counselors is significantly lower than global standards recommend, particularly in elementary education settings. This shortage is exacerbated by rising mental health concerns among young students, particularly those with emotional disorders. Considering the critical gap in the provision of mental health services in Asia, this paper studies a digital intervention approach with an AI-driven supportive system developed by adopting OpenAI to enhance the effectiveness of counseling in elementary education. Twenty-two students with ADHD, autism spectrum disorder, and emotional disorders undergoing counseling at a primary school in Taiwan were recruited as participants for a three-month experiment, with the five Social-Emotional competencies as dependent variables. The treatment group utilized the proposed system with a digital journaling platform to help students reflect on their emotions, thoughts, and actions after counseling sessions, fostering an ongoing dialogue with their counselors through the system. Conversely, the control group received standard counseling without integrating the use of the proposed platform. The results of a two-factor mixed design ANOVA revealed that students who did not use the supportive system showed significant improvement in self-awareness. In contrast, students who went through the new model demonstrated significant changes in all competencies. These findings highlight the value of the proposed intervention approach for students with emotional disorders and suggest broader applications for AI technologies in school counseling, offering valuable insights for educators and policymakers. © 2024 Elsevier B.V., All rights reserved.","artificial intelligence; Cognitive behavioral therapy (CBT); OpenAI; school counseling; social-emotional learning (SEL)","Adversarial machine learning; Artificial intelligence; Diseases; Economic and social effects; Social psychology; Cognitive behavioral therapy; Cognitive-behavioral therapies; Elementary education; Elementary students; Emotional learning; Global standards; Mental health; Openai; School counseling; Social-emotional learning; Students","","","This work involved human subjects or animals in its research. Approval of all ethical and experimental procedures and protocols was granted by the National Changhua University of Education, Research Ethic Committee, under IRB No. NCUEREC-113-067. The authors would like to thank the school and research teams for assisting with this experiment. Their help made the production of this article possible","Journal of Benefit Cost Analysis, (2015); Durlak, Joseph A., The Impact of Enhancing Students' Social and Emotional Learning: A Meta-Analysis of School-Based Universal Interventions, Child Development, 82, 1, pp. 405-432, (2011); Kim, Dongil, The quality and effectiveness of Social-Emotional Learning (SEL) intervention studies in Korea: A meta-analysis, PLOS ONE, 17, 6 June, (2022); Taylor, Rebecca D., Promoting Positive Youth Development Through School-Based Social and Emotional Learning Interventions: A Meta-Analysis of Follow-Up Effects, Child Development, 88, 4, pp. 1156-1171, (2017); Hare, Megan M., Intervention response among preschoolers with ADHD: The role of emotion understanding, Journal of School Psychology, 84, pp. 19-31, (2021); Wei, Xin, Longitudinal effects of ADHD in children with learning disabilities or emotional disturbances, Exceptional Children, 80, 2, pp. 205-219, (2013); Corcoran, Roisin P., Effective universal school-based social and emotional learning programs for improving academic achievement: A systematic review and meta-analysis of 50 years of research, Educational Research Review, 25, pp. 56-72, (2018); Denham, Susanne Ayers, Plays nice with others"": Social-emotional learning and academic success, Early Education and Development, 21, 5, pp. 652-680, (2010); McCormick, Meghan P., Social-Emotional Learning and Academic Achievement: Using Causal Methods to Explore Classroom-Level Mechanisms, AERA Open, 1, 3, (2015); Panayiotou, Margarita, An empirical basis for linking social and emotional learning to academic performance, Contemporary Educational Psychology, 56, pp. 193-204, (2019)","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21693536","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85207743621"
"Rashid, Z.; Folarin, A.A.; Zhang, Y.; Ranjan, Y.; Conde, P.; Sankesara, H.; Sun, S.; Stewart, C.; Laiou, P.; Dobson, R.J.B.","Rashid, Zulqarnain (57208873135); Folarin, Amos A. (35766461900); Zhang, Yuezhou (57206486845); Ranjan, Yatharth (57205028192); Conde, Pauline (57210795333); Sankesara, Heet (57386555800); Sun, Shaoxiong (57192690141); Stewart, Callum L. (57205023094); Laiou, Petroula (56278014200); Dobson, Richard J.B. (8931612400)","57208873135; 35766461900; 57206486845; 57205028192; 57210795333; 57386555800; 57192690141; 57205023094; 56278014200; 8931612400","Digital Phenotyping of Mental and Physical Conditions: Remote Monitoring of Patients Through RADAR-Base Platform","2024","JMIR Mental Health","11","","e51259","","","0","3","10.2196/51259","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206190865&doi=10.2196%2F51259&partnerID=40&md5=d2b7677279312a5ff8f1aed553517cc2","King's College London, London, United Kingdom; Institute of Health Informatics, University College London, London, United Kingdom; South London and Maudsley NHS Foundation Trust, London, United Kingdom; University College London, London, United Kingdom; University College London Hospitals NHS Foundation Trust, London, United Kingdom; Department of Computer Science, The University of Sheffield, Sheffield, United Kingdom","Rashid, Zulqarnain, King's College London, London, United Kingdom; Folarin, Amos A., King's College London, London, United Kingdom, Institute of Health Informatics, University College London, London, United Kingdom, South London and Maudsley NHS Foundation Trust, London, United Kingdom, University College London, London, United Kingdom, University College London Hospitals NHS Foundation Trust, London, United Kingdom; Zhang, Yuezhou, King's College London, London, United Kingdom; Ranjan, Yatharth, King's College London, London, United Kingdom; Conde, Pauline, King's College London, London, United Kingdom; Sankesara, Heet, King's College London, London, United Kingdom; Sun, Shaoxiong, King's College London, London, United Kingdom, Department of Computer Science, The University of Sheffield, Sheffield, United Kingdom; Stewart, Callum L., King's College London, London, United Kingdom; Laiou, Petroula, King's College London, London, United Kingdom; Dobson, Richard J.B., King's College London, London, United Kingdom, Institute of Health Informatics, University College London, London, United Kingdom, South London and Maudsley NHS Foundation Trust, London, United Kingdom, University College London, London, United Kingdom, University College London Hospitals NHS Foundation Trust, London, United Kingdom","Background: The use of digital biomarkers through remote patient monitoring offers valuable and timely insights into a patient’s condition, including aspects such as disease progression and treatment response. This serves as a complementary resource to traditional health care settings leveraging mobile technology to improve scale and lower latency, cost, and burden. Objective: Smartphones with embedded and connected sensors have immense potential for improving health care through various apps and mobile health (mHealth) platforms. This capability could enable the development of reliable digital biomarkers from long-term longitudinal data collected remotely from patients. Methods: We built an open-source platform, RADAR-base, to support large-scale data collection in remote monitoring studies. RADAR-base is a modern remote data collection platform built around Confluent’s Apache Kafka to support scalability, extensibility, security, privacy, and quality of data. It provides support for study design and setup and active (eg, patient-reported outcome measures) and passive (eg, phone sensors, wearable devices, and Internet of Things) remote data collection capabilities with feature generation (eg, behavioral, environmental, and physiological markers). The back end enables secure data transmission and scalable solutions for data storage, management, and data access. Results: The platform has been used to successfully collect longitudinal data for various cohorts in a number of disease areas including multiple sclerosis, depression, epilepsy, attention-deficit/hyperactivity disorder, Alzheimer disease, autism, and lung diseases. Digital biomarkers developed through collected data are providing useful insights into different diseases. Conclusions: RADAR-base offers a contemporary, open-source solution driven by the community for remotely monitoring, collecting data, and digitally characterizing both physical and mental health conditions. Clinicians have the ability to enhance their insight through the use of digital biomarkers, enabling improved prevention, personalization, and early intervention in the context of disease management. © 2024 Elsevier B.V., All rights reserved.","biomarkers; data collection; digital biomarkers; Internet of Things; IoT; mHealth; mobile apps; mobile phone; open-source platform; phenotyping; platform; RADAR-base; real-time monitoring; remote data collection; smartphone; wearable; wearables","biological marker; access to information; Alzheimer disease; Article; attention deficit hyperactivity disorder; autism; behavior; cardiometabolic risk factor; central nervous system disease; comparative study; convalescence; coronavirus disease 2019; data privacy; data quality; depression; digital health technology; early intervention; eating disorder; environment; epilepsy; ethics; gyroscope sensor; health care facility; health data; human; information processing; information security; information storage; internet of things; lung disease; machine learning; major depression; medication compliance; mental disease; mental health; multiple sclerosis; patient-reported outcome; permanent atrial fibrillation; personalized medicine; phenotype; physical activity; physical disease; physiology; questionnaire; relapse; telecommunication; telehealth; telemonitoring; treatment response","","","Funding text 1: This study has received support from the European Union (EU)/European Federation of Pharmaceutical Industries and Associations (EFPIA) Innovative Medicines Initiative (IMI) Joint Undertaking 2 (Remote Assessment of Disease and Relapse \u2014Central Nervous System [RADAR-CNS] grant 115902). This communication reflects the views of the RADAR-CNS consortium, and neither IMI nor the EU and EFPIA are liable for any use that may be made of the information contained herein. The authors receive funding support from the National Institute for Health and Care Research (NIHR) Biomedical Research Centre at South London and Maudsley National Health Service (NHS) Foundation Trust and King\u2019s College London. The views expressed are those of the authors and not necessarily those of the NHS, the NIHR, or the Department of Health. The authors also acknowledge the support of NIHR University College London Hospitals Biomedical Research Centre. This work is also supported by the Medical Research Council/Arts and Humanities Research Council/Economic and Social Research Council Adolescence, Mental Health and the Developing Mind initiative as part of the EDIFY program (grant MR/W002418/1).; Funding text 2: The Rate Control Therapy Evaluation in Permanent Atrial Fibrillation (RATE-AF) study was designed to compare 2 strategies of rate control, based on either initial treatment with digoxin or \u03B2-blockers in 160 patients with atrial fibrillation (AF) in need for rate control therapy. Monitoring with wearable devices, phone sensors, and questionnaires was conducted over a continuous 6-month period. Objectives of the project included discovering new phenotypes, developing reliable subphenotyping, and informing new taxonomies of heart failure based on a better understanding of underlying disease processes. Sleep, heath rate, heart rate variation, and activity data were collected to develop new phenotypes. This work is additionally supported by the EU IMI2 BigData@Heart major program [39].","Motahari-Nezhad, Hossein, Digital Biomarker-Based Studies: Scoping Review of Systematic Reviews, JMIR mHealth and uHealth, 10, 10, (2022); Ranjan, Yatharth, Poster: RADAR-base: A novel open source m-health platform, pp. 223-226, (2018); Ranjan, Yatharth, Radar-base: Open source mobile health platform for collecting, monitoring, and analyzing data using sensors, wearables, and mobile devices, JMIR mHealth and uHealth, 7, 8, (2019); Grafana; Opoku Asare, Kennedy, Mood ratings and digital biomarkers from smartphone and wearable data differentiates and predicts depression status: A longitudinal data analysis, Pervasive and Mobile Computing, 83, (2022); Jacobson, Nicholas C., Digital biomarkers of social anxiety severity: Digital phenotyping using passive smartphone sensors, Journal of Medical Internet Research, 22, 5, (2020); Babrak, Lmar Marie, Traditional and Digital Biomarkers: Two Worlds Apart?, Digital Biomarkers, 3, 2, pp. 92-102, (2019); Bent, Brinnae, The digital biomarker discovery pipeline: An open-source software platform for the development of digital biomarkers using mHealth and wearables data, Journal of Clinical and Translational Science, 5, 1, (2021); Hossain, Syed Monowar, mCerebrum: A mobile sensing software platform for development and validation of digital biomarkers and interventions, 2017-January, (2017); Torous, John Blake, The new digital divide for digital biomarkers, Digital Biomarkers, 1, 1, pp. 87-91, (2017)","","JMIR Publications Inc.","","","","","","23687959","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85206190865"
"Tseng, Y.-L.; Lee, C.-H.; Chiu, Y.-N.; Tsai, W.-C.; Wang, J.-S.; Wu, W.-C.; Chien, Y.-L.","Tseng, Yili (25653143200); Lee, Chiahsin (58064620400); Chiu, Yen Nan (7202775750); Tsai, Wenche (12791356900); Wang, Juisheng (59293541100); Wu, Weichen (59293222100); Chien, Y. L. (26535653800)","25653143200; 58064620400; 7202775750; 12791356900; 59293541100; 59293222100; 26535653800","Characterizing Autism Spectrum Disorder Through Fusion of Local Cortical Activation and Global Functional Connectivity Using Game-Based Stimuli and a Mobile EEG System","2024","IEEE Transactions on Neural Systems and Rehabilitation Engineering","32","","","3026","3035","0","6","10.1109/TNSRE.2024.3417210","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201758515&doi=10.1109%2FTNSRE.2024.3417210&partnerID=40&md5=f65a472659e968bdb572f83f284a290c","National Sun Yat-Sen University, Kaohsiung, Taiwan; Department of Electrical Engineering, Fu Jen Catholic University, New Taipei City, Taiwan; Department of Psychiatry, National Taiwan University Hospital, Taipei, Taiwan","Tseng, Yili, National Sun Yat-Sen University, Kaohsiung, Taiwan; Lee, Chiahsin, National Sun Yat-Sen University, Kaohsiung, Taiwan, Department of Electrical Engineering, Fu Jen Catholic University, New Taipei City, Taiwan; Chiu, Yen Nan, Department of Psychiatry, National Taiwan University Hospital, Taipei, Taiwan; Tsai, Wenche, Department of Psychiatry, National Taiwan University Hospital, Taipei, Taiwan; Wang, Juisheng, National Sun Yat-Sen University, Kaohsiung, Taiwan, Department of Electrical Engineering, Fu Jen Catholic University, New Taipei City, Taiwan; Wu, Weichen, National Sun Yat-Sen University, Kaohsiung, Taiwan; Chien, Y. L., Department of Psychiatry, National Taiwan University Hospital, Taipei, Taiwan","The deficit in social interaction skills among individuals with autism spectrum disorder (ASD) is strongly influenced by personal experiences and social environments. Neuroimaging studies have previously highlighted the link between social impairment and brain activity in ASD. This study aims to develop a method for assessing and identifying ASD using a social cognitive game-based paradigm combined with electroencephalo-graphy (EEG) signaling features. Typically developing (TD) participants and autistic preadolescents and teenagers were recruited to participate in a social game while 12-channel EEG signals were recorded. The EEG signals underwent preprocessing to analyze local brain activities, including event-related potentials (ERPs) and time-frequency features. Additionally, the global brain network's functional connectivity between brain regions was evaluated using phase-lag indices (PLIs). Subsequently, machine learning models were employed to assess the neurophysiological features. Results indicated pronounced ERP components, particularly the late positive potential (LPP), in parietal regions during social training. Autistic preadolescents and teenagers exhibited lower LPP amplitudes and larger P200 amplitudes compared to TD participants. Reduced theta synchronization was also observed in the ASD group. Aberrant functional connectivity within certain time intervals was noted in the ASD group. Machine learning analysis revealed that support-vector machines achieved a sensitivity of 100%, specificity of 91.7%, and accuracy of 95.8% as part of the performance evaluation when utilizing ERP and brain oscillation features for ASD characterization. These findings suggest that social interaction difficulties in autism are linked to specific brain activation patterns. Traditional behavioral assessments face challenges of subjectivity and accuracy, indicating the potential use of social training interfaces and EEG features for cognitive assessment in ASD. © 2024 Elsevier B.V., All rights reserved.","autism spectrum disorder; brain oscillations; Electroencephalography; functional connectivity; support-vector machine","Breath controlled devices; Electrotherapeutics; Functional neuroimaging; Job analysis; Autism; Autism spectrum disorders; Brain oscillations; Event related potentials; Functional connectivity; Game; Game-Based; Social interactions; Support vectors machine; Task analysis; Support vector machines; accuracy; adolescent; Article; autism; brain region; child; cognition; controlled study; decision tree; diagnostic test accuracy study; electric potential; electroencephalogram; electroencephalography; event related potential; female; functional connectivity; game; game based stimuli; human; human experiment; k nearest neighbor; late positive potential; machine learning; male; neuroimaging; neuropsychological assessment; oscillation; sensitivity and specificity; social cognition; social environment; social interaction; support vector machine; algorithm; brain cortex; diagnostic imaging; evoked response; pathophysiology; physiology; procedures; smartphone; video game; Adolescent; Algorithms; Autism Spectrum Disorder; Cerebral Cortex; Child; Electroencephalography; Evoked Potentials; Female; Humans; Machine Learning; Male; Smartphone; Social Interaction; Video Games","","","This work was supported in part by the Taiwan National Science and Technology Council under Grant 112-2222-E-110-021-and National Health Research Institutes under Grant NHRI-EX113-11008PC. (Corresponding author: Yi-Ling Chien) Yi-Li Tseng and Wei-Chen Wu are with the Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan.","undefined, (2013); Prevalence of autism spectrum disorder among children aged 8 years - autism and developmental disabilities monitoring network, 11 sites, United States, 2010., MMWR Surveillance Summaries, 63, 2, pp. 1-21, (2014); Gross, Thomas F., The perception of four basic emotions in human and nonhuman faces by children with autism and other developmental disabilities, Journal of Abnormal Child Psychology, 32, 5, pp. 469-480, (2004); Behrmann, Marlene Behrmann, Seeing it differently: visual processing in autism, Trends in Cognitive Sciences, 10, 6, pp. 258-264, (2006); Mundy, Peter Clive, A review of joint attention and social-cognitive brain systems in typical development and autism spectrum disorder, European Journal of Neuroscience, 47, 6, pp. 497-514, (2018); Stephenson, Lisa J., From Gaze Perception to Social Cognition: The Shared-Attention System, Perspectives on Psychological Science, 16, 3, pp. 553-576, (2021); Monteiro, Raquel, Processing of Facial Expressions in Autism: a Systematic Review of EEG/ERP Evidence, Review Journal of Autism and Developmental Disorders, 4, 4, pp. 255-276, (2017); Oberwelland, Eileen, Look into my eyes: Investigating joint attention using interactive eye-tracking and fMRI in a developmental sample, NeuroImage, 130, pp. 248-260, (2016); Vaidya, Chandan J., Controlling attention to gaze and arrows in childhood: An fMRI study of typical development and Autism Spectrum Disorders, Developmental Science, 14, 4, pp. 911-924, (2011); Greene, Deanna J., Atypical neural networks for social orienting in autism spectrum disorders, NeuroImage, 56, 1, pp. 354-362, (2011)","","Institute of Electrical and Electronics Engineers Inc.","","","","","","15580210; 15344320","","ITNSB","39163173","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85201758515"
"Westin, T.; Rahmani, R.; Palosaari Eladhari, M.; Romero, M.","Westin, Thomas (54992643100); Rahmani, Rahim (8310584400); Palosaari Eladhari, Mirjam (17345716200); Romero, Mario (36096092000)","54992643100; 8310584400; 17345716200; 36096092000","An extended reality platform for inclusion of adults on the autism spectrum: A position paper","2024","Procedia Computer Science","238","","","476","483","0","1","10.1016/j.procs.2024.06.050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199493428&doi=10.1016%2Fj.procs.2024.06.050&partnerID=40&md5=0ab4938f59595ad0485ac0226fe1a397","Department of Computer Science and Information Systems, Stockholms universitet, Stockholm, Sweden; The Royal Institute of Technology (KTH), Stockholm, Sweden","Westin, Thomas, Department of Computer Science and Information Systems, Stockholms universitet, Stockholm, Sweden; Rahmani, Rahim, Department of Computer Science and Information Systems, Stockholms universitet, Stockholm, Sweden; Palosaari Eladhari, Mirjam, Department of Computer Science and Information Systems, Stockholms universitet, Stockholm, Sweden; Romero, Mario, The Royal Institute of Technology (KTH), Stockholm, Sweden","Extended reality (XR) enables both new opportunities but also introduces new barriers for inclusion in society. Furthermore, XR is less researched than web, desktop and mobile applications. This position paper presents the concept of an XR platform for inclusion, with the purpose to make people on the autism spectrum and with other disabilities, more independent of help from others in everyday life situations. Based on previous research, our position is that, through current and future XR technologies combined with civic and artificial intelligence, it is possible to create individually personalised support for this purpose, grounded in practice to ensure validation. © 2024 Elsevier B.V., All rights reserved.","augmented reality; autism; disability; inclusive; intellectual; metaverse; universal design; XR","Diseases; Electric grounding; Autism; Disability; Inclusive; Intellectual; Metaverses; Position papers; Spectra's; Universal Design; WEB application; XR; Augmented reality","","","","Blattgerste, Jonas, Augmented reality action assistance and learning for cognitively impaired people - A systematic literature review, ACM International Conference Proceeding Series, pp. 270-279, (2019); Khowaja, Kamran, Augmented reality for learning of children and adolescents with autism spectrum disorder (ASD): A systematic review, IEEE Access, 8, pp. 78779-78807, (2020); Berenguer, Carmen, Exploring the Impact of Augmented Reality in Children and Adolescents with Autism Spectrum Disorder: A Systematic Review, International Journal of Environmental Research and Public Health, 17, 17, pp. 1-15, (2020); Qiao, Xiuquan, Web AR: A Promising Future for Mobile Augmented Reality-State of the Art, Challenges, and Insights, Proceedings of the IEEE, 107, 4, pp. 651-666, (2019); Clark, Colin, Enabling architecture: How the GPII supports inclusive software development, Lecture Notes in Computer Science, 8516 LNCS, PART 4, pp. 368-377, (2014); Iglesias-Pérez, Andrés, Accessibility through preferences: Context-aware recommender of settings, Lecture Notes in Computer Science, 8513 LNCS, PART 1, pp. 224-235, (2014); Kaklanis, Nikolaos T., A semantic framework for assistive technologies description to strengthen UI adaptation, Lecture Notes in Computer Science, 8513 LNCS, PART 1, pp. 236-245, (2014); W3c Web Content Accessibility Guidelines; W3c Wai Involving Users in Web Projects for Better Easier Accessibility; W3c Xr Accessibility User Requirements W3c Working Group Note, (2021)","Shakshuki, E.","Elsevier B.V.","","15th International Conference on Ambient Systems, Networks and Technologies Networks, ANT 2024 / The 7th International Conference on Emerging Data and Industry 4.0, EDI40 2024","","Hasselt; Hasselt University","200877","18770509","9781510849914","","","English","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85199493428"
"Anto-Chavez, C.; Maguiña-Bernuy, R.; Ugarte, W.","Anto-Chavez, Carolain (59139859100); Maguiña-Bernuy, Richard (59138829000); Ugarte, Willy (55440420500)","59139859100; 59138829000; 55440420500","Real-Time CNN Based Facial Emotion Recognition Model for a Mobile Serious Game","2024","International Conference on Information and Communication Technologies for Ageing Well and e-Health, ICT4AWE - Proceedings","","","","84","92","0","0","10.5220/0012683800003699","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193959213&doi=10.5220%2F0012683800003699&partnerID=40&md5=f617db04acb0b727dcd7668515ec341f","Universidad Peruana de Ciencias Aplicadas, Lima, Peru","Anto-Chavez, Carolain, Universidad Peruana de Ciencias Aplicadas, Lima, Peru; Maguiña-Bernuy, Richard, Universidad Peruana de Ciencias Aplicadas, Lima, Peru; Ugarte, Willy, Universidad Peruana de Ciencias Aplicadas, Lima, Peru","Every year, the increase in human-computer interaction is noticeable. This brings with it the evolution of computer vision to improve this interaction to make it more efficient and effective. This paper presents a CNN-based emotion face recognition model capable to be executed on mobile devices, in real time and with high accuracy. Different models implemented in other research are usually of large sizes, and although they obtained high accuracy, they fail to make predictions in an optimal time, which prevents a fluid interaction with the computer. To improve these, we have implemented a lightweight CNN model trained with the FER-2013 dataset to obtain the prediction of seven basic emotions. Experimentation shows that our model achieves an accuracy of 66.52% in validation, can be stored in a 13.23MB file and achieves an average processing time of 14.39ms and 16.06ms, on a tablet and a phone, respectively. © 2024 Elsevier B.V., All rights reserved.","Emotion; Expression; Facial; FER; Machine Learning; Mobile; Real-Time; Recognition","Computer graphics; Computer vision; Emotion Recognition; Face recognition; Human computer interaction; Machine learning; Emotion; Expression; Facial; FER; High-accuracy; Machine-learning; Mobile; Real- time; Recognition; Recognition models; Serious games","","","","Zhang, Chuanjie, Facial Expression Recognition Integrating Multiple CNN Models, pp. 1410-1414, (2020); Cornejo, Leonardo, Mobile Application for Controlling a Healthy Diet in Peru Using Image Recognition, Conference of Open Innovation Association, FRUCT, 2021-October, pp. 32-41, (2021); Dantas, Adilmar Coelho, Recognition of Emotions for People with Autism: An Approach to Improve Skills, International Journal of Computer Games Technology, 2022, (2022); Ekman, Paul, Constants across cultures in the face and emotion, Journal of Personality and Social Psychology, 17, 2, pp. 124-129, (1971); Akhmedov, Farkhod, Development of Real-Time Landmark-Based Emotion Recognition CNN for Masked Faces, Sensors, 22, 22, (2022); Garcia-Garcia, Jose Maria, Using emotion recognition technologies to teach children with autism spectrum disorder how to identify and express emotions, Universal Access in the Information Society, 21, 4, pp. 809-825, (2022); Hua, Wentao, HERO: Human Emotions Recognition for Realizing Intelligent Internet of Things, IEEE Access, 7, pp. 24321-24332, (2019); Leon-Urbano, Charles, End-to-end electroencephalogram (EEG) motor imagery classification with Long Short-Term, pp. 2814-2820, (2020); Lozano-Mejia, Diego Jesus, Content-Based Image Classification for Sheet Music Books Recognition, (2020); Minaee, Shervin, Deep-emotion: Facial expression recognition using attentional convolutional network, Sensors, 21, 9, (2021)","Mulvenna, M.; Perez, M.L.; Ziefl e, M.","Science and Technology Publications, Lda","Institute for Systems and Technologies of Information, Control and Communication (INSTICC)","10th International Conference on Information and Communication Technologies for Ageing Well and e-Health, ICT4AWE 2024","","Angers","199566","21844984","9789897587009; 9789897586453; 9789897585661; 9789897587436; 9789897585067","","","English","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85193959213"
"Almanza-Conejo, O.; Avina-Cervantes, J.G.; García-Pérez, A.; Ibarra-Manzano, M.A.","Almanza-Conejo, Oscar (57218557871); Avina-Cervantes, Juan Gabriel (6507093766); García-Pérez, Arturo (24450208500); Ibarra-Manzano, Mario-Alberto (15837259000)","57218557871; 6507093766; 24450208500; 15837259000","Emotion Recognition in Gaming Dataset to Reduce Artifacts in the Self-Assessed Labeling Using Semi-Supervised Clustering","2024","IEEE Access","12","","","52659","52668","0","0","10.1109/ACCESS.2024.3387357","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190169379&doi=10.1109%2FACCESS.2024.3387357&partnerID=40&md5=ea0690f73c86d21a71616a069ed8cf05","Department of Electronics Engineering, Universidad de Guanajuato, Guanajuato, Mexico","Almanza-Conejo, Oscar, Department of Electronics Engineering, Universidad de Guanajuato, Guanajuato, Mexico; Avina-Cervantes, Juan Gabriel, Department of Electronics Engineering, Universidad de Guanajuato, Guanajuato, Mexico; García-Pérez, Arturo, Department of Electronics Engineering, Universidad de Guanajuato, Guanajuato, Mexico; Ibarra-Manzano, Mario-Alberto, Department of Electronics Engineering, Universidad de Guanajuato, Guanajuato, Mexico","Popular comments suggest that continuous exposure of children and adolescents to video games yields a non-benefit behavior in the players' mental health. Contrarily, several studies have proven that commercial and serious games improve mental activity; some are used in treating psychological and physical disorders. This paper presents a method based on electroencephalogram signals analysis to classify multiple emotions recorded from subjects' gameplay seasons. In the core of this study, a self-assessed labeling method is evaluated using the Force, EEG, and Emotion Labelled Dataset (FEEL) for emotion recognition tasks. Besides, a 1-D Local Binary Pattern (LBP) method transforms the EEG temporal behavior to extract time-frequency features. Complementarily, the database artifacts were removed using a novel Conflict Learning approach for machine learning models, associating the extracted samples with the subjects' emotion labeling. A semi-supervised clustering method was employed to show the similarity between self-assessed subjects' labels. Finally, numerical results suggested a conflict between 23 original labels, improving the classification by over 92% in accuracy for 19 self-assessed classes. © 2024 Elsevier B.V., All rights reserved.","clustering; conflict learning; Emotion recognition; gaming; machine learning","Clustering algorithms; Electroencephalography; Electrophysiology; Extraction; Feature extraction; Human computer interaction; Learning systems; Serious games; Speech recognition; Brain modeling; Clusterings; Conflict learning; Emotion recognition; Features extraction; Game; Gaming; Machine-learning; Video-games; Emotion Recognition","","","","Zayeni, Darius, Therapeutic and Preventive Use of Video Games in Child and Adolescent Psychiatry: A Systematic Review, Frontiers in Psychiatry, 11, (2020); Lobel, Adam, Video Gaming and Children’s Psychosocial Wellbeing: A Longitudinal Study, Journal of Youth and Adolescence, 46, 4, pp. 884-897, (2017); Prescott, Anna T., Metaanalysis of the relationship between violent video game play and physical aggression over time, Proceedings of the National Academy of Sciences of the United States of America, 115, 40, pp. 9882-9888, (2018); Piaget, Jean, Play, dreams and imitation in childhood, pp. 1-296, (2013); Wouters, Pieter, How to optimize learning from animated models: A review of guidelines based on cognitive load, Review of Educational Research, 78, 3, pp. 645-675, (2008); Mura, Gioia, Active exergames to improve cognitive functioning in neurological disabilities: A systematic review and meta-analysis, European Journal of Physical and Rehabilitation Medicine, 54, 3, pp. 450-462, (2018); Wang, Zhen, Sensory Integration Training and Social Sports Games Integrated Intervention for the Occupational Therapy of Children with Autism, Occupational Therapy International, 2022, (2022); Quiles, Clélia, Benefits of video games for people with schizophrenia: A literature review, Current Opinion in Psychiatry, 36, 3, pp. 184-193, (2023); Damásio, António R., The somatic marker hypothesis and the possible functions of the prefrontal cortex, Philosophical Transactions of the Royal Society B: Biological Sciences, 351, 1346, pp. 1413-1420, (1996); Patel, Pragati, EEG-based human emotion recognition using entropy as a feature extraction measure, Brain Informatics, 8, 1, (2021)","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21693536","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85190169379"
"Melinda, M.; Arnia, F.; Yafi, A.; Andryani, N.A.C.; Enriko, I.K.A.","Melinda, Melinda (53264423700); Arnia, Fitri (14027791000); Yafi, Al (58201986100); Andryani, Nur Afny Catur (35174188700); Enriko, I. Ketut Agung (57164890600)","53264423700; 14027791000; 58201986100; 35174188700; 57164890600","Design and Implementation of Mobile Application for CNN-Based EEG Identification of Autism Spectrum Disorder","2024","International Journal on Advanced Science, Engineering and Information Technology","14","1","","57","64","0","6","10.18517/ijaseit.14.1.19676","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185942816&doi=10.18517%2Fijaseit.14.1.19676&partnerID=40&md5=4de4ae56271e9b194b66b6dcd3264581","Department of Electrical & Computer Engineering, Universitas Syiah Kuala, Banda Aceh, Indonesia; Computer Science Program, Bina Nusantara University, Jakarta, Indonesia; Department of Telecommunication Engineering, Telkom University, Bandung, Indonesia","Melinda, Melinda, Department of Electrical & Computer Engineering, Universitas Syiah Kuala, Banda Aceh, Indonesia; Arnia, Fitri, Department of Electrical & Computer Engineering, Universitas Syiah Kuala, Banda Aceh, Indonesia; Yafi, Al, Department of Electrical & Computer Engineering, Universitas Syiah Kuala, Banda Aceh, Indonesia; Andryani, Nur Afny Catur, Computer Science Program, Bina Nusantara University, Jakarta, Indonesia; Enriko, I. Ketut Agung, Department of Telecommunication Engineering, Telkom University, Bandung, Indonesia","Autism spectrum disorder (ASD) is a disorder of the nervous system from birth and during infancy. This disorder affects children's development, making it difficult for nerve function to develop, and causes the child concerned to have difficulty in fostering social relationships. Early detection of children with ASD is needed so that treatment is fast and on target. Currently, facilities and research on early diagnosis of ASD patients through EEG signals are still very few, requiring much cost and more effort to analyze EEG signals in examinations related to ASD detection cases. This study proposes a mobile phone application that can distinguish people living with ASD and normal data signals based on asynchronous EEG brain signals. This research also produces a preprocessing algorithm and BCI2000 EEG data signal so that it can be automated using Python. This research also produces an output model, namely the Deep Learning Convolutional Neural Network, which is deployed using Python-Flask so that the diagnosis of EEG signals with ASD and normal patients can be used on various platforms through restAPI. This research is also expected to help the community and support the diagnosis of ASD sufferers so that they can be handled appropriately. Data for ASD sufferers and normal data were correctly classified into the appropriate class. Handling this disease requires close and integrated cooperation, so this ASD classification will be very helpful for patients and can make a diagnosis in a faster time, enabling patients to receive targeted treatment and therapy. © 2024 Elsevier B.V., All rights reserved.","BCI2000; convolutional neural network; mobile apps; OpenCV; signal to image","","","","We acknowledge Universitas Syiah Kuala and all parties that have contributed to this work.","Tawhid, Md Nurul Ahad, Diagnosis of autism spectrum disorder from EEG using a time-frequency spectrogram image-based approach, Electronics Letters, 56, 25, pp. 1372-1375, (2020); Islam, Shirajul, Autism Spectrum Disorder Detection in Toddlers for Early Diagnosis Using Machine Learning, (2020); Subudhi, Asit Kumar, Automated Delimitation and Classification of Autistic Disorder Using EEG Signal, IETE Journal of Research, 69, 2, pp. 951-959, (2023); Kaya, Volkan, Detection and classification of different weapon types using deep learning, Applied Sciences (Switzerland), 11, 16, (2021); Behavioral Features Based Autism Spectrum Disorder Detection Using Decision Trees, (2021); Ann Indian Psychiatry, (2022); Alhaddad, Mohammed Jaffer, Diagnosis autism by Fisher Linear Discriminant Analysis FLDA via EEG, International Journal of Bio-Science and Bio-Technology, 4, 2, pp. 45-54, (2012); Arts, Lukas P.A., The fast continuous wavelet transformation (fCWT) for real-time, high-quality, noise-resistant time–frequency analysis, Nature Computational Science, 2, 1, pp. 47-58, (2022); Chatterjee, Shubhojeet, Review of noise removal techniques in ECG signals, IET Signal Processing, 14, 9, pp. 569-590, (2020); Xu, Tao, Decode Brain System: A Dynamic Adaptive Convolutional Quorum Voting Approach for Variable-Length EEG Data, Complexity, 2020, (2020)","","Insight Society","","","","","","20885334; 24606952","","","","English","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85185942816"
"Savchenko, A.V.","Savchenko, Andrey V. (42962245900)","42962245900","AutoFace: How to Obtain Mobile Neural Network-Based Facial Feature Extractor in Less Than 10 Minutes?","2024","IEEE Access","12","","","25106","25118","0","2","10.1109/ACCESS.2024.3365928","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185898928&doi=10.1109%2FACCESS.2024.3365928&partnerID=40&md5=6807ac55554711efff1616b93f97de1f","Sber AI Lab, Moscow, Russian Federation; Laboratory of Algorithms and Technologies for Network Analysis, HSE University, Moscow, Russian Federation","Savchenko, Andrey V., Sber AI Lab, Moscow, Russian Federation, Laboratory of Algorithms and Technologies for Network Analysis, HSE University, Moscow, Russian Federation","Various mobile and edge devices have significantly different processing capabilities, making it challenging to develop a single universal architecture of a neural network to extract facial embeddings. In this paper, we study the automated machine learning techniques to design a neural network with the best performance on a concrete device. The novel procedure is proposed to choose the better subnetwork of the Supernet based on a genetic algorithm with a surrogate binary classifier to compare the expected accuracy of two subnetworks. The latter uses only encoding of a candidate subnetwork and does not require directly estimating its accuracy on a validation set. As a result, the most computationally efficient and accurate model in TensorFlow Lite format is obtained in less than 10 minutes for a specific device and latency constraint. An Android demo application has been developed to demonstrate the potential of designed neural networks. It is experimentally shown that the proposed approach is universal: it can extract deep embeddings for tasks such as face verification and facial expression recognition and for various types of devices, including smartphones and Raspberry Pi single-board mini-computers. Our models process one facial image in real-time and achieve much higher accuracy when compared to the best-known lightweight networks. © 2024 Elsevier B.V., All rights reserved.","edge devices; evolutionary search; face verification; facial expression recognition; genetic algorithms; Mobile applications","Computer architecture; Feature extraction; Genetic algorithms; Job analysis; Learning systems; Network architecture; Neural networks; Edge device; Evolutionary search; Face Verification; Facial expression recognition; Features extraction; Mobile applications; Neural-networks; Performances evaluation; Subnetworks; Task analysis; Face recognition","","","","Kocaçinar, Büşra, A Real-Time CNN-Based Lightweight Mobile Masked Face Recognition System, IEEE Access, 10, pp. 63496-63507, (2022); Zhao, Feng, DGFaceNet: Lightweight and efficient face recognition, Engineering Applications of Artificial Intelligence, 124, (2023); Deng, Jiankang, ArcFace: Additive angular margin loss for deep face recognition, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 4685-4694, (2019); Kharchevnikova, Angelina S., Neural Networks in Video-Based Age and Gender Recognition on Mobile Platforms, Optical Memory and Neural Networks (Information Optics), 27, 4, pp. 246-259, (2018); Grechikhin, Ivan S., User Modeling on Mobile Device Based on Facial Clustering and Object Detection in Photos and Videos, Lecture Notes in Computer Science, 11868 LNCS, pp. 429-440, (2019); Wang, Xiang, A Privacy-Preserving Edge Computation-Based Face Verification System for User Authentication, IEEE Access, 7, pp. 14186-14197, (2019); Savchenko, Andrey V., Preference prediction based on a photo gallery analysis with scene recognition and object detection, Pattern Recognition, 121, (2022); Cao, Qiong, VGGFace2: A dataset for recognising faces across pose and age, pp. 67-74, (2018); Korinevskaya, Alisa, Fast Depth Map Super-Resolution Using Deep Neural Network, pp. 117-122, (2018); Savchenko, Andrey V., Fast inference in convolutional neural networks based on sequential three-way decisions, Information Sciences, 560, pp. 370-385, (2021)","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21693536","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85185898928"
"Krumnikl, M.; Maiwald, V.","Krumnikl, Michal (21834007500); Maiwald, Vojtech (58862875400)","21834007500; 58862875400","Facial Emotion Recognition for Mobile Devices: A Practical Review","2024","IEEE Access","12","","","15735","15747","0","7","10.1109/ACCESS.2024.3358455","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183951109&doi=10.1109%2FACCESS.2024.3358455&partnerID=40&md5=2a1917b7cf2780258b4d26a5cb7db7d9","Department of Computer Science, VSB – Technical University of Ostrava, Ostrava, Czech Republic","Krumnikl, Michal, Department of Computer Science, VSB – Technical University of Ostrava, Ostrava, Czech Republic; Maiwald, Vojtech, Department of Computer Science, VSB – Technical University of Ostrava, Ostrava, Czech Republic","Communicating via email or various chat applications on smartphones is part of most people's daily lives. But in written form, human communication loses a lot of valuable information, such as the facial expressions and emotions of the person you are communicating with. Thanks to techniques from the field of image processing, it is now possible to capture these non-verbal phenomena, and supplement written input with their non-verbal characteristics. In this paper, we explore the possibilities of emotion recognition from front camera images in mobile and embedded devices. A total of 63 classification and 28 regression models based on twelve different neural network architectures optimized for low performance mobile devices were trained and evaluated for success rate and latency. The training and evaluation of each neural network model is performed within the Keras API of the TensorFlow library and then converted to the TensorFlow Lite standard to reduce memory and computational requirements. Great care is taken to ensure that the entire process, from face detection to emotion classification, can operate in real time. To demonstrate and compare the performance of the evaluated models, a freely available optimized application running on Android mobile devices is created and published on Google Play, the source code of which is also available. © 2024 Elsevier B.V., All rights reserved.","Android; emotion classification; face detection; Keras; MLKit; neural networks; TensorFlow; TensorFlow Lite","Android (operating system); Computer architecture; Face recognition; Network architecture; Neural networks; Regression analysis; Speech recognition; Android; Brain modeling; Convolutional neural network; Emojis; Emotion classification; Emotion recognition; Faces detection; Keras; MLKit; Neural-networks; Tensorflow; Tensorflow lite; Emotion Recognition","","","","Annual Review of Psychology, (1979); Jack, Rachael E., Facial expressions of emotion are not culturally universal, Proceedings of the National Academy of Sciences of the United States of America, 109, 19, pp. 7241-7244, (2012); Russell, James A., Culture and the categorization of emotions, Psychological Bulletin, 110, 3, pp. 426-450, (1991); Keltner, Dacher J., What Basic Emotion Theory Really Says for the Twenty-First Century Study of Emotion, Journal of Nonverbal Behavior, 43, 2, pp. 195-201, (2019); Jack, Rachael E., Four not six: Revealing culturally common facial expressions of emotion, Journal of Experimental Psychology: General, 145, 6, pp. 708-730, (2016); Ortony, Andrew, Are All “Basic Emotions” Emotions? A Problem for the (Basic) Emotions Construct, Perspectives on Psychological Science, 17, 1, pp. 41-61, (2022); Expression of the Emotions in Man and Animals, (1872); Spapé, Michiel M., The semiotics of the message and the messenger: How nonverbal communication affects fairness perception, Cognitive, Affective and Behavioral Neuroscience, 19, 5, pp. 1259-1272, (2019); Silent Messages, (1971); Mehrabian, Albert, INFERENCE OF ATTITUDES FROM NONVERBAL COMMUNICATION IN TWO CHANNELS, Journal of Consulting Psychology, 31, 3, pp. 248-252, (1967)","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21693536","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85183951109"
"Mukherjee, D.; Bhavnani, S.; Lockwood-Estrin, G.; Rao, V.; Dasgupta, J.; Irfan, H.; Chakrabarti, B.; Patel, V.; Belmonte, M.K.","Mukherjee, Debarati (57213526915); Bhavnani, Supriya (57191954853); Lockwood-Estrin, Georgia (54941042100); Rao, Vaisnavi (57959343000); Dasgupta, Jayashree (57191964613); Irfan, Hiba (57959120000); Chakrabarti, Bhismadev (35253357900); Patel, Vikram Harshad (7402495238); Belmonte, Matthew Kenneth (8647375900)","57213526915; 57191954853; 54941042100; 57959343000; 57191964613; 57959120000; 35253357900; 7402495238; 8647375900","Digital tools for direct assessment of autism risk during early childhood: A systematic review","2024","Autism","28","1","","6","31","0","16","10.1177/13623613221133176","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141642698&doi=10.1177%2F13623613221133176&partnerID=40&md5=5d068b018776ff24e25653e353f13822","Public Health Foundation of India, New Delhi, India; Child Development Group, Sangath, Bhopal, India; Birkbeck, University of London, London, United Kingdom; University of East London, London, United Kingdom; Institute for Democracy and Economic Affairs (IDEAS), Kuala Lumpur, Malaysia; University of Reading, Reading, United Kingdom; Ashoka University, Sonipat, India; India Autism Center, Kolkata, India; Harvard Medical School, Boston, United States; Harvard T.H. Chan School of Public Health, Boston, United States; Communication Deall, Bengaluru, India; Nottingham Trent University, Nottingham, United Kingdom","Mukherjee, Debarati, Public Health Foundation of India, New Delhi, India; Bhavnani, Supriya, Child Development Group, Sangath, Bhopal, India; Lockwood-Estrin, Georgia, Birkbeck, University of London, London, United Kingdom, University of East London, London, United Kingdom; Rao, Vaisnavi, Institute for Democracy and Economic Affairs (IDEAS), Kuala Lumpur, Malaysia; Dasgupta, Jayashree, Child Development Group, Sangath, Bhopal, India; Irfan, Hiba, Child Development Group, Sangath, Bhopal, India; Chakrabarti, Bhismadev, University of Reading, Reading, United Kingdom, Ashoka University, Sonipat, India, India Autism Center, Kolkata, India; Patel, Vikram Harshad, Child Development Group, Sangath, Bhopal, India, Harvard Medical School, Boston, United States, Harvard T.H. Chan School of Public Health, Boston, United States; Belmonte, Matthew Kenneth, Communication Deall, Bengaluru, India, Nottingham Trent University, Nottingham, United Kingdom","Current challenges in early identification of autism spectrum disorder lead to significant delays in starting interventions, thereby compromising outcomes. Digital tools can potentially address this barrier as they are accessible, can measure autism-relevant phenotypes and can be administered in children’s natural environments by non-specialists. The purpose of this systematic review is to identify and characterise potentially scalable digital tools for direct assessment of autism spectrum disorder risk in early childhood. In total, 51,953 titles, 6884 abstracts and 567 full-text articles from four databases were screened using predefined criteria. Of these, 38 met inclusion criteria. Tasks are presented on both portable and non-portable technologies, typically by researchers in laboratory or clinic settings. Gamified tasks, virtual-reality platforms and automated analysis of video or audio recordings of children’s behaviours and speech are used to assess autism spectrum disorder risk. Tasks tapping social communication/interaction and motor domains most reliably discriminate between autism spectrum disorder and typically developing groups. Digital tools employing objective data collection and analysis methods hold immense potential for early identification of autism spectrum disorder risk. Next steps should be to further validate these tools, evaluate their generalisability outside laboratory or clinic settings, and standardise derived measures across tasks. Furthermore, stakeholders from underserved communities should be involved in the research and development process. Lay abstract: The challenge of finding autistic children, and finding them early enough to make a difference for them and their families, becomes all the greater in parts of the world where human and material resources are in short supply. Poverty of resources delays interventions, translating into a poverty of outcomes. Digital tools carry potential to lessen this delay because they can be administered by non-specialists in children’s homes, schools or other everyday environments, they can measure a wide range of autistic behaviours objectively and they can automate analysis without requiring an expert in computers or statistics. This literature review aimed to identify and describe digital tools for screening children who may be at risk for autism. These tools are predominantly at the ‘proof-of-concept’ stage. Both portable (laptops, mobile phones, smart toys) and fixed (desktop computers, virtual-reality platforms) technologies are used to present computerised games, or to record children’s behaviours or speech. Computerised analysis of children’s interactions with these technologies differentiates children with and without autism, with promising results. Tasks assessing social responses and hand and body movements are the most reliable in distinguishing autistic from typically developing children. Such digital tools hold immense potential for early identification of autism spectrum disorder risk at a large scale. Next steps should be to further validate these tools and to evaluate their applicability in a variety of settings. Crucially, stakeholders from underserved communities globally must be involved in this research, lest it fail to capture the issues that these stakeholders are facing. © 2024 Elsevier B.V., All rights reserved.","ASD; assessments; computer; digital; gamified; low-resource; mHealth; scalable; smartphone; tablet; virtual reality","Article; audio recording; autism; autism assessment; childhood; developmental delay; human; language; language delay; machine learning; phenotype; poverty; risk assessment; speech; systematic review; training; virtual reality; child; movement (physiology); preschool child; recreation; Autism Spectrum Disorder; Autistic Disorder; Child; Child, Preschool; Humans; Movement; Play and Playthings; Poverty","","","The author(s) disclosed receipt of the following financial support for the research, authorship and/or publication of this article: D.M. was supported by the Department of Science and Technology-INSPIRE Faculty Award (2016/DST/INSPIRE/04-I/2016/000001). G.L.E. was supported by a Sir Henry Wellcome Fellowship (Wellcome Trust Grant No. 204706/Z/16/Z). The authors acknowledge funding from the Medical Research Council UK (STREAM, MR/S036423/1 awarded to B.C.).","Abaza, Haitham, mHealth application areas and technology combinations: A comparison of literature from high and low/middle income countries, Methods of Information in Medicine, 56, MethodsOpen, pp. e105-e122, (2017); Alcañiz Raya, Mariano L., Eye gaze as a biomarker in the recognition of autism spectrum disorder using virtual reality and machine learning: A proof of concept for diagnosis, Autism Research, 15, 1, pp. 131-145, (2022); Alcañiz Raya, Mariano L., Machine learning and virtual reality on body movements’ behaviors to classify children with autism spectrum disorder, Journal of Clinical Medicine, 9, 5, (2020); Text Revision Dsm IV TR, (1994); Anzulewicz, Anna, Toward the Autism Motor Signature: Gesture patterns during smart tablet gameplay identify children with autism, Scientific Reports, 6, (2016); Aresti-Bartolome, Nuria, Cognitive rehabilitation system for children with autism spectrum disorder using serious games: A pilot study, Bio-Medical Materials and Engineering, 26, pp. S811-S824, (2015); Baron-Cohen, Simon B., Does the autistic child have a ""theory of mind"" ?, Cognition, 21, 1, pp. 37-46, (1985); Baxter, Amanda J., The epidemiology and global burden of autism spectrum disorders, Psychological Medicine, 45, 3, pp. 601-613, (2015); Belmonte, Matthew Kenneth, Oral motor deficits in speech-impaired children with Autism, Frontiers in Integrative Neuroscience, JUN, (2013); Bhavnani, Supriya, “I was Confused … and Still am” Barriers Impacting the Help-Seeking Pathway for an Autism Diagnosis in Urban North India: A Mixed Methods Study, Journal of Autism and Developmental Disorders, 52, 4, pp. 1778-1788, (2022)","","SAGE Publications Ltd","","","","","","13623613; 14617005","","AUTIF","36336996","English","Article","Final","All Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85141642698"
"Valuch, C.; Pelowski, M.; Peltoketo, V.-T.; Hakala, J.; Leder, H.","Valuch, Christian (55751387500); Pelowski, Matthew (16304982900); Peltoketo, Veli Tapani (56048214800); Hakala, Jussi (57226403497); Leder, Helmut (8656461700)","55751387500; 16304982900; 56048214800; 57226403497; 8656461700","Let’s put a smile on that face—A positive facial expression improves aesthetics of portrait photographs","2023","Royal Society Open Science","10","10","230413","","","0","6","10.1098/rsos.230413","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178279005&doi=10.1098%2Frsos.230413&partnerID=40&md5=16a54c3a0798598ad3a1e1a159795456","Faculty of Psychology, Universität Wien, Vienna, Austria; Universität Wien, Vienna, Austria; Huawei Technologies Oy (Finland) Co. Ltd, Tampere, Finland","Valuch, Christian, Faculty of Psychology, Universität Wien, Vienna, Austria; Pelowski, Matthew, Faculty of Psychology, Universität Wien, Vienna, Austria, Universität Wien, Vienna, Austria; Peltoketo, Veli Tapani, Huawei Technologies Oy (Finland) Co. Ltd, Tampere, Finland; Hakala, Jussi, Huawei Technologies Oy (Finland) Co. Ltd, Tampere, Finland; Leder, Helmut, Faculty of Psychology, Universität Wien, Vienna, Austria","In today’s age of social media and smartphones, portraits—such as selfies or pictures of friends and family—are very frequently produced, shared and viewed images. Despite their prevalence, the psychological factors that characterize a ‘good’ photo—one that people will generally like, keep, and think is especially aesthetically pleasing—are not well understood. Here, we studied how a subtle change in facial expression (smiling) in portraits determines their aesthetic image value (beyond a more positive appearance of the depicted person). We used AI-based image processing tools in a broad set of portrait photographs and generated neutral and slightly smiling versions of the same pictures. Consistent across two experiments, portraits with a subtle smile increased both spontaneous aesthetic preferences in a swiping task as well as improving more explicit aesthetic ratings after prolonged viewing. Participants distinguished between aspects associated with image beauty and the depicted person’s attractiveness, resulting in specific interactions between variables related to participant traits, image content, and task. Our study confirms that a subtle—and in this case fully artificial—smile reliably increases the aesthetic quality of portraits, illustrating how current image processing methods can target psychologically important variables and thereby increase the aesthetic value of photographs. © 2023 Elsevier B.V., All rights reserved.","artificial intelligence; cultural context; facial attractiveness; image beauty; individual differences","","","","Funding text 1: J.H. and V.-T.P. were both employed by Huawei Technologies Oy (Finland) Co. Ltd during the time this research was conducted. This project received funding from Huawei Technologies Oy (Finland) Co. Ltd. who internally approved the decision to publish the manuscript. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.; Funding text 2: This project received research funding from Huawei Technologies Oy (Finland) Co. Ltd. The open access publication is supported by the University of Vienna. Acknowledgements","Leder, Helmut, Swipes and Saves: A Taxonomy of Factors Influencing Aesthetic Assessments and Perceived Beauty of Mobile Phone Photographs, Frontiers in Psychology, 13, (2022); Hu, Chuanshen, Virtual portraitist: An intelligent tool for taking well-posed selfies, ACM Transactions on Multimedia Computing, Communications and Applications, 15, 1s, (2019); Bakhshi, Saeideh, Faces engage us: Photos with faces attract more likes and comments on instagram, Conference on Human Factors in Computing Systems - Proceedings, pp. 965-974, (2014); Hu, Yuheng, What we instagram: A first analysis of instagram photo content and user types, pp. 595-598, (2014); Mazza, Filippo, Would you hire me? Selfie portrait images perception in a recruitment context, Proceedings of SPIE - The International Society for Optical Engineering, 9014, (2014); Leder, Helmut, Ten years of a model of aesthetic appreciation and aesthetic judgments: The aesthetic episode - Developments and challenges in empirical aesthetics, British Journal of Psychology, 105, 4, pp. 443-464, (2014); Pelowski, Matthew, Move me, astonish me… delight my eyes and brain: The Vienna Integrated Model of top-down and bottom-up processes in Art Perception (VIMAP) and corresponding affective, evaluative, and neurophysiological correlates, Physics of Life Reviews, 21, pp. 80-125, (2017); Gangestad, Steven W., The evolution of human physical attractiveness, Annual Review of Anthropology, 34, pp. 523-548, (2005); Little, Anthony C., Facial attractiveness, Wiley Interdisciplinary Reviews: Cognitive Science, 5, 6, pp. 621-634, (2014); Gillian I. Rhodes, Gillian, Attractiveness of own-race, other-race, and mixed-race faces, Perception, 34, 3, pp. 319-340, (2005)","","Royal Society Publishing","","","","","","20545703","","","","English","Article","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85178279005"
"Hanano, T.; Seo, M.; Chen, Y.-W.","Hanano, Tatsuya (57426895600); Seo, Masataka (35280835900); Chen, Yenwei Weif (56036268200)","57426895600; 35280835900; 56036268200","Generation of High-Resolution Facial Expression Images Using a Super-Resolution Technique and Self-Supervised Guidance","2023","Journal of Image and Graphics (United Kingdom)","11","3","","302","308","0","0","10.18178/joig.11.3.302-308","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171463007&doi=10.18178%2Fjoig.11.3.302-308&partnerID=40&md5=a07074683f16bd991b3c7f37f666aaa5","School of Information Science and Engineering, Ritsumeikan University Biwako-Kusatsu Campus, Kusatsu, Japan; Osaka Institute of Technology, Osaka, Japan","Hanano, Tatsuya, School of Information Science and Engineering, Ritsumeikan University Biwako-Kusatsu Campus, Kusatsu, Japan; Seo, Masataka, Osaka Institute of Technology, Osaka, Japan; Chen, Yenwei Weif, School of Information Science and Engineering, Ritsumeikan University Biwako-Kusatsu Campus, Kusatsu, Japan","The recent spread of smartphones and social networking services has increased the means of seeing images of human faces. Particularly, in the face image field, the generation of face images using facial expression transformation has already been realized using deep learning–based approaches. However, in the existing deep learning–based models, only low-resolution images can be generated due to limited computational resources. Consequently, the generated images are blurry or aliasing. To address this problem, we proposed a two-step method to enhance the resolution of the generated facial images by combining a super-resolution network following the generative model, which can be considered a serial model, in our previous work. We further proposed a parallel model that trains a generative adversarial network and a super-resolution network through multitask learning. In this paper, we propose a new model that integrates self-supervised guidance encoders into the parallel model to further improve the accuracy of the generated results. Using the peak signal-to-noise ratio as an evaluation index, image quality was improved by 0.25 dB for the male test data and 0.28 dB for the female test data compared with our previous multitask-based parallel model. © 2023 Elsevier B.V., All rights reserved.","deep learning; facial expression transformation; generative adversarial networks; image processing; super resolution","","","","This work was supported in part by the Grant in Aid for Scientific Research from the Japanese Ministry for Education, Science, Culture and Sports (MEXT) under the Grant Nos. 21H04903, 21K11936 and 20K11867.","Isola, Phillip J., Image-to-image translation with conditional adversarial networks, 2017-January, pp. 5967-5976, (2017); Kawai, Yoshiharu, Automatic Generation of Facial Expression Using Generative Adversarial Nets, pp. 329-330, (2018); Zhu, Junyan, Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks, Proceedings of the IEEE International Conference on Computer Vision, 2017-October, pp. 2242-2251, (2017); Choi, Yunjey, StarGAN: Unified Generative Adversarial Networks for Multi-domain Image-to-Image Translation, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 8789-8797, (2018); International Conference on Consumer Electronics, (2020); Hanano, Tatsuya, Automatic Generation of High-Resolution Facial Expression Images with End-to-End Models Using Pix2Pix and Super-Resolution Convolutional Neural Network, pp. 798-801, (2021); Hanano, Tatsuya, Accurate and Efficient Generation of High-Resolution Facial Expression Images by Multi-Task Learning Using Generative Adversarial Networks, pp. 765-768, (2022); Wang, Hongyi, Patch-Free 3D Medical Image Segmentation Driven by Super-Resolution Technique and Self-Supervised Guidance, Lecture Notes in Computer Science, 12901 LNCS, pp. 131-141, (2021); Hanano, Tatsuya, An Improved cGAN with Self-Supervised Guidance Encoder for Generation of High-Resolution Facial Expression Images, Digest of Technical Papers - IEEE International Conference on Consumer Electronics, 2023-January, (2023); Conditional Generative Adversarial Nets, (2014)","","University of Portsmouth","","","","","","23013699","","","","English","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85171463007"
"Parlett-Pelleriti, C.M.; Stevens, E.; Dixon, D.; Linstead, E.J.","Parlett-Pelleriti, Chelsea M. (57208574344); Stevens, Elizabeth (57193318465); Dixon, Dennis R. (7401748113); Linstead, Erik J. (16307496400)","57208574344; 57193318465; 7401748113; 16307496400","Applications of Unsupervised Machine Learning in Autism Spectrum Disorder Research: a Review","2023","Review Journal of Autism and Developmental Disorders","10","3","","406","421","0","44","10.1007/s40489-021-00299-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123917389&doi=10.1007%2Fs40489-021-00299-y&partnerID=40&md5=b3dd58e4f6799f1c4609fdf4ee93c9e5","Machine Learning and Assistive Technology Lab, Chapman University, Orange, United States; Center for Autism and Related Disorders, Tarzana, United States","Parlett-Pelleriti, Chelsea M., Machine Learning and Assistive Technology Lab, Chapman University, Orange, United States; Stevens, Elizabeth, Machine Learning and Assistive Technology Lab, Chapman University, Orange, United States; Dixon, Dennis R., Center for Autism and Related Disorders, Tarzana, United States; Linstead, Erik J., Machine Learning and Assistive Technology Lab, Chapman University, Orange, United States","Large amounts of autism spectrum disorder (ASD) data is created through hospitals, therapy centers, and mobile applications; however, much of this rich data does not have pre-existing classes or labels. Large amounts of data—both genetic and behavioral—that are collected as part of scientific studies or a part of treatment can provide a deeper, more nuanced insight into both diagnosis and treatment of ASD. This paper reviews 43 papers using unsupervised machine learning in ASD, including k-means clustering, hierarchical clustering, model-based clustering, and self-organizing maps. The aim of this review is to provide a survey of the current uses of unsupervised machine learning in ASD research and provide insight into the types of questions being answered with these methods. © 2023 Elsevier B.V., All rights reserved.","ASD; Autism; Autism spectrum disorder; Clustering; Phenotypes; Subgroups; Unsupervised machine learning","","","","CPP was funded by NSF GRFP, Fellowship ID #1849569.","An, Michael, Multimodal MRI analysis of brain subnetworks in autism using multi-view EM, Conference Record of the Asilomar Conference on Signals, Systems and Computers, pp. 786-789, (2010); Text Revision Dsm IV TR, (1994); Baio, Jon, Prevalence of autism spectrum disorder among children aged 8 Years - Autism and developmental disabilities monitoring network, 11 Sites, United States, 2014, MMWR Surveillance Summaries, 67, 6, pp. 1-23, (2018); Bekele, Esube T., Understanding how adolescents with autism respond to facial expressions in virtual reality environments, IEEE Transactions on Visualization and Computer Graphics, 19, 4, pp. 711-720, (2013); Ben-Sasson, Ayelet, Sensory clusters of toddlers with autism spectrum disorders: Differences in affective symptoms, Journal of Child Psychology and Psychiatry and Allied Disciplines, 49, 8, pp. 817-825, (2008); Bitsika, Vicki, An exploratory analysis of the use of cognitive, adaptive and behavioural indices for cluster analysis of ASD subgroups, Journal of Intellectual Disability Research, 52, 11, pp. 973-985, (2008); Bozdogan, Hamparsum, Model selection and Akaike's Information Criterion (AIC): The general theory and its analytical extensions, Psychometrika, 52, 3, pp. 345-370, (1987); Constantino, John Nicholas, The factor structure of autistic traits, Journal of Child Psychology and Psychiatry and Allied Disciplines, 45, 4, pp. 719-726, (2004); Cuccaro, Michael L., Exploring the relationship between autism spectrum disorder and epilepsy using latent class cluster analysis, Journal of Autism and Developmental Disorders, 42, 8, pp. 1630-1641, (2012); Doshi-Velez, Finale, Comorbidity clusters in autism spectrum disorders: An electronic health record time-series analysis, Pediatrics, 133, 1, pp. e54-e63, (2014)","","Springer","","","","","","21957185; 21957177","","","","English","Review","Final","All Open Access; Green Final Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85123917389"
"Wali, A.; Alfrihidi, M.; Alasiri, N.; Alsabei, N.","Wali, Arwa M. (56001792500); Alfrihidi, Modi (58592224500); Alasiri, Nada (57883742800); Alsabei, Najah (58591778700)","56001792500; 58592224500; 57883742800; 58591778700","Aawn: An Interactive Mobile Application for Improving the Communication Skills of Arab Children with Autism","2023","TEM Journal","12","3","","1307","1315","0","5","10.18421/TEM123-10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171462467&doi=10.18421%2FTEM123-10&partnerID=40&md5=6651f15a53a97483bafc876e8e9a18e5","Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia","Wali, Arwa M., Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; Alfrihidi, Modi, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; Alasiri, Nada, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; Alsabei, Najah, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia","Autism spectrum disorder (ASD) is a neurodevelopmental disorder that causes challenges in communication and social interaction. Since there is no single treatment for autism, autistic children need extra attention from their parents or caregivers to overcome their linguistic deficiency. Scientific studies have shown that technology-based educational methods are effective and can lead to a significant improvement, especially for autistic children. In recent years, an increasing number of mobile and multimedia applications have been developed to enhance autistic children's verbal communication, emotions, social behavior, and interaction skills. However, many of these systems are either in English or in local autistic children’s native languages. Others are designed to promote specific knowledge, i.e., emotions, or are limited in their features. This paper designs and implements a prototype for Arabic mobile application, called Aawn, to help and improve Arab autistic children's, in their own language, communication and emotions, as well as educational and organizational skills. Aawn is integrated with various supporting technologies based on the Picture Exchange Communication System (PECS) and augmented with graphical features. The system is built and developed using Android Studio and various cloud-based tools. The system can be extended by artificial intelligence (AI) technologies. © 2023 Elsevier B.V., All rights reserved.","mobile application; picture exchange communication system (PECS); Spectrum disorder (ADS)","","","","","Reference Reviews, (2014); Tulshan, Amrita S., Krisha: An Interactive Mobile Application for Autism Children, Communications in Computer and Information Science, 1046, pp. 207-218, (2019); Alharbi, Mohammed N., An augmentative system with facial and emotion recognition for improving social skills of children with autism spectrum disorders, (2020); Aziz, Naziatul Shima Abdul, A study on mobile applications developed for children with autism, Advances in Intelligent Systems and Computing, 843, pp. 772-780, (2019); Picture Exchange Communication System Pecs®, (2023); Maaal, (2020); Iop Conference Series Materials Science and Engineering, (2021); Procedia Technology, (2013); Letmetalk Version 1 4 29 Mobile App, (2024); Otsimo, (2023)","","UIKTEN - Association for Information Communication Technology Education and Science","","","","","","22178309; 22178333","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85171462467"
"Milojevich, H.M.; Stickel, D.; Swingler, M.M.; Zhang, X.; Terrell, J.; Sheridan, M.A.; Tan, X.","Milojevich, Helen M. (55655333500); Stickel, Daniel (58556460400); Swingler, Margaret M. (57000665000); Zhang, Xinyi (57222745725); Terrell, Jeffery (58589315300); Sheridan, Margaret A. (57206385727); Tan, Xianming (57191753918)","55655333500; 58556460400; 57000665000; 57222745725; 58589315300; 57206385727; 57191753918","Building an ecological momentary assessment smartphone app for 4- to 10-year-old children: A pilot study","2023","PLOS ONE","18","8 August","e0290148","","","0","2","10.1371/journal.pone.0290148","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169230077&doi=10.1371%2Fjournal.pone.0290148&partnerID=40&md5=5f98134a3834284067b777d8ef4aa37c","Duke University, Durham, United States; College of Arts & Sciences, Chapel Hill, United States; The University of North Carolina at Chapel Hill, Chapel Hill, United States; UNC Gillings School of Global Public Health, Chapel Hill, United States; Department of Computer Science, Chapel Hill, United States","Milojevich, Helen M., Duke University, Durham, United States; Stickel, Daniel, College of Arts & Sciences, Chapel Hill, United States; Swingler, Margaret M., The University of North Carolina at Chapel Hill, Chapel Hill, United States; Zhang, Xinyi, UNC Gillings School of Global Public Health, Chapel Hill, United States; Terrell, Jeffery, Department of Computer Science, Chapel Hill, United States; Sheridan, Margaret A., College of Arts & Sciences, Chapel Hill, United States; Tan, Xianming, UNC Gillings School of Global Public Health, Chapel Hill, United States","Objective Ecological momentary assessment (EMA) minimizes recall burden and maximizes ecological validity and has emerged as a valuable tool to characterize individual differences, assess contextual associations, and document temporal associations. However, EMA has yet to be reliably utilized in young children, in part due to concerns about responder reliability and limited compliance. The present study addressed these concerns by building a developmentally appropriate EMA smartphone app and testing the app for feasibility and usability with young children ages 4-10 (N = 20; m age = 7.7, SD = 2.0). Methods To pilot test the app, children completed an 11-item survey about their mood and behavior twice a day for 14 days. Parents also completed brief surveys twice a day to allow for parent- child comparisons of responses. Finally, at the end of the two weeks, parents provided user feedback on the smartphone app. Results Results indicated a high response rate (nearly 90%) across child surveys and high agreement between parents and children ranging from 0.89-0.97. Conclusions Overall, findings suggest that this developmentally appropriate EMA smartphone app is a reliable and valid tool for collecting in-the-moment data from young children outside of a laboratory setting. © 2023 Elsevier B.V., All rights reserved.","","adolescent; algorithm; anxiety; Article; autism; child; clinical article; cost effectiveness analysis; design; ecological momentary assessment; feasibility study; health care survey; human; interrater reliability; Likert scale; machine learning; mood; perception; pilot study; Positive and Negative Affect Schedule; questionnaire; rating scale; self report; usability; validity; preschool child; recall; reproducibility; Child; Child, Preschool; Ecological Momentary Assessment; Humans; Mental Recall; Mobile Applications; Pilot Projects; Reproducibility of Results","","","This study was supported by NC TraCS grant 550KR221923 (PI: Tan) and NIMH grant R01 MH115004 (PI: Sheridan). The authors thank the two anonymous referees for their constructive comments which helped us to improve our manuscript.","Shiffman, Saul M., Ecological momentary assessment, Annual Review of Clinical Psychology, 4, pp. 1-32, (2008); Dunton, Genevieve Fridlund, Mapping the social and physical contexts of physical activity across adolescence using ecological momentary assessment, Annals of Behavioral Medicine, 34, 2, pp. 144-153, (2007); Janicki, Denise L., Application of ecological momentary assessment to the study of marital adjustment and social interactions during daily life, Journal of Family Psychology, 20, 1, pp. 168-172, (2006); Trull, Timothy J., Ambulatory assessment, Annual Review of Clinical Psychology, 9, pp. 151-176, (2013); Nock, Matthew K., Revealing the Form and Function of Self-Injurious Thoughts and Behaviors: A Real-Time Ecological Assessment Study Among Adolescents and Young Adults, Journal of Abnormal Psychology, 118, 4, pp. 816-827, (2009); Russell, Michael A., Annual Research Review: Ecological momentary assessment studies in child psychology and psychiatry, Journal of Child Psychology and Psychiatry and Allied Disciplines, 61, 3, pp. 376-394, (2020); Businelle, Michael S., Predicting quit attempts among homeless smokers seeking cessation treatment: An ecological momentary assessment study, Nicotine and Tobacco Research, 16, 10, pp. 1371-1378, (2014); Kleiman, Evan M., Examination of real-time fluctuations in suicidal ideation and its risk factors: Results from two ecological momentary assessment studies, Journal of Abnormal Psychology, 126, 6, pp. 726-738, (2017); Dunton, Genevieve Fridlund, Investigating children's physical activity and sedentary behavior using ecological momentary assessment with mobile phones, Obesity, 19, 6, pp. 1205-1212, (2011); Fogleman, Nicholas D., Peer victimization linked to negative affect in children with and without ADHD, Journal of Applied Developmental Psychology, 46, pp. 1-10, (2016)","","Public Library of Science","","","","","","19326203","","POLNC","37647264","English","Article","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85169230077"
"Lootus, M.; Beatson, L.; Atwood, L.; Bourdais, T.; Steyaert, S.; Sarabu, C.; Framroze, Z.; Dickinson, H.; Steels, J.-C.; Lewis, E.; Shah, N.R.; Rinaldo, F.","Lootus, Meelis (55822990400); Beatson, Lulu (58550480000); Atwood, Lucas (58550580400); Bourdais, Théo (58550372000); Steyaert, Sandra (56418037200); Sarabu, Chethan (55203694900); Framroze, Zeenia (57769606700); Dickinson, Harriet (58540912400); Steels, Jean Christophe (58540568300); Lewis, Emily (58540683500); Shah, Nirav R. (9333876600); Rinaldo, Francesca (57835334400)","55822990400; 58550480000; 58550580400; 58550372000; 56418037200; 55203694900; 57769606700; 58540912400; 58540568300; 58540683500; 9333876600; 57835334400","Development and Assessment of an Artificial Intelligence-Based Tool for Ptosis Measurement in Adult Myasthenia Gravis Patients Using Selfie Video Clips Recorded on Smartphones","2023","Digital Biomarkers","7","1","","63","73","0","8","10.1159/000531224","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168812472&doi=10.1159%2F000531224&partnerID=40&md5=baeb65845b8f7c4d5b26a127e287a0c8","Sharecare, Atlanta, United States; Center for Pediatric Bioinformatics, Stanford University, Stanford, United States; UCB S.A., Braine-l'Alleud, Belgium; UCB S.A., Braine-l'Alleud, Belgium; Clinical Excellence Research Center, Stanford University, Stanford, United States","Lootus, Meelis, Sharecare, Atlanta, United States; Beatson, Lulu, Sharecare, Atlanta, United States; Atwood, Lucas, Sharecare, Atlanta, United States; Bourdais, Théo, Sharecare, Atlanta, United States; Steyaert, Sandra, Center for Pediatric Bioinformatics, Stanford University, Stanford, United States; Sarabu, Chethan, Sharecare, Atlanta, United States; Framroze, Zeenia, Sharecare, Atlanta, United States; Dickinson, Harriet, UCB S.A., Braine-l'Alleud, Belgium; Steels, Jean Christophe, UCB S.A., Braine-l'Alleud, Belgium; Lewis, Emily, UCB S.A., Braine-l'Alleud, Belgium; Shah, Nirav R., Clinical Excellence Research Center, Stanford University, Stanford, United States; Rinaldo, Francesca, Sharecare, Atlanta, United States","Introduction: Myasthenia gravis (MG) is a rare autoimmune disease characterized by muscle weakness and fatigue. Ptosis (eyelid drooping) occurs due to fatigue of the muscles for eyelid elevation and is one symptom widely used by patients and healthcare providers to track progression of the disease. Margin reflex distance 1 (MRD1) is an accepted clinical measure of ptosis and is typically assessed using a hand-held ruler. In this work, we develop an AI model that enables automated measurement of MRD1 in self-recorded video clips collected using patient smartphones. Methods: A 3-month prospective observational study collected a dataset of video clips from patients with MG. Study participants were asked to perform an eyelid fatigability exercise to elicit ptosis while filming ""selfie""videos on their smartphones. These images were collected in nonclinical settings, with no in-person training. The dataset was annotated by non-clinicians for (1) eye landmarks to establish ground truth MRD1 and (2) the quality of the video frames. The ground truth MRD1 (in millimeters, mm) was calculated from eye landmark annotations in the video frames using a standard conversion factor, the horizontal visible iris diameter of the human eye. To develop the model, we trained a neural network for eye landmark detection consisting of a ResNet50 backbone plus two dense layers of 78 dimensions on publicly available datasets. Only the ResNet50 backbone was used, discarding the last two layers. The embeddings from the ResNet50 were used as features for a support vector regressor (SVR) using a linear kernel, for regression to MRD1, in mm. The SVR was trained on data collected remotely from MG patients in the prospective study, split into training and development folds. The model's performance for MRD1 estimation was evaluated on a separate test fold from the study dataset. Results: On the full test fold (N = 664 images), the correlation between the ground truth and predicted MRD1 values was strong (r = 0.732). The mean absolute error was 0.822 mm; the mean of differences was -0.256 mm; and 95% limits of agreement (LOA) were -0.214-1.768 mm. Model performance showed no improvement when test data were gated to exclude ""poor""quality images. Conclusions: On data generated under highly challenging real-world conditions from a variety of different smartphone devices, the model predicts MRD1 with a strong correlation (r = 0.732) between ground truth and predicted MRD1. © 2024 Elsevier B.V., All rights reserved.","Computer vision; Decentralized study; Personal smartphones; Ptosis","Computer vision; Diseases; Medical computing; mHealth; Multilayer neural networks; Muscle; Statistical tests; Video cameras; Decentralised; Decentralized study; Ground truth; Myasthenia gravis; Personal smartphone; Ptosis; Smart phones; Support vector regressor; Video frame; Video-clips; Smartphones; adult; algorithm; anatomic landmark; Article; artificial intelligence; automation; clinical assessment; clinical evaluation; clinical trial; computer vision; correlation analysis; embedding; exercise; eyelid; eyelid movement; eyelid reflex; facial expression; fatigue; feature extraction; female; human; image analysis; information processing; iris; major clinical study; male; margin reflex distance 1; mean absolute error; myasthenia gravis; observational study; patient positioning; performance indicator; prospective study; ptosis (eyelid); regression analysis; resnet50; support vector machine; videorecording","","","The study was funded by UCB Pharma and conducted in collaboration with Sharecare, Inc. UCB consulted with Sharecare on the study objectives and design. UCB was not involved in the data collection, model development, or evaluation.","Thymus, (2020); Phillips, Lawrence H., The epidemiology of myasthenia gravis, Seminars in Neurology, 24, 1, pp. 17-20, (2004); Nair, Akshay Gopinathan, Ocular myasthenia gravis: A review, Indian Journal of Ophthalmology, 62, 10, pp. 985-991, (2014); Wang, Lili, Clinical predictors for the prognosis of myasthenia gravis, BMC Neurology, 17, 1, (2017); Conti-Fine, Bianca M., Myasthenia gravis: Past, present, and future, Journal of Clinical Investigation, 116, 11, pp. 2843-2854, (2006); Human Eye Structure and Function, (1999); Toyka, Klaus Viktor, Ptosis in myasthenia gravis: Extended fatigue and recovery bedside test, Neurology, 67, 8, (2006); Yoganathan, Katie T., Bedside and laboratory diagnostic testing in myasthenia, Journal of Neurology, 269, 6, pp. 3372-3384, (2022); Putterman, Allen M., Margin Reflex Distance (MRD) 1, 2, and 3, Ophthalmic Plastic and Reconstructive Surgery, 28, 4, pp. 308-311, (2012); Kerala J Ophthalmol, (2019)","","S. Karger AG","","","","","","2504110X","","","","English","Article","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85168812472"
"Gwak, S.; Park, K.","Gwak, Sojung (58492993200); Park, Kyudong (56419061800)","58492993200; 56419061800","Designing Effective Visual Feedback for Facial Rehabilitation Exercises: Investigating the Role of Shape, Transparency, and Age on User Experience","2023","Healthcare (Switzerland)","11","13","1835","","","0","2","10.3390/healthcare11131835","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165083116&doi=10.3390%2Fhealthcare11131835&partnerID=40&md5=7163bc9714cc3778bcd54299bddeee37","Department of Artificial Intelligence Application, Kwangwoon University, Seoul, South Korea; School of Information Convergence, Kwangwoon University, Seoul, South Korea","Gwak, Sojung, Department of Artificial Intelligence Application, Kwangwoon University, Seoul, South Korea; Park, Kyudong, Department of Artificial Intelligence Application, Kwangwoon University, Seoul, South Korea, School of Information Convergence, Kwangwoon University, Seoul, South Korea","Facial expression recognition technology has been utilized both for entertainment purposes and as a valuable aid in rehabilitation and facial exercise assistance. This technology leverages artificial intelligence models to predict facial landmark points and provide visual feedback, thereby facilitating users’ facial movements. However, feedback designs that disregard user preferences may cause discomfort and diminish the benefits of exercise. This study aimed to develop a feedback design guide for facial rehabilitation exercises by investigating user responses to various feedback design methods. We created a facial recognition mobile application and designed six feedback variations based on shape and transparency. To evaluate user experience, we conducted a usability test involving 48 participants (24 subjects in their 20s and 24 over 60 years of age), assessing factors such as feedback, assistance, disturbance, aesthetics, cognitive ease, and appropriateness. The experimental results revealed significant differences in transparency, age, and the interaction between transparency and age. Consequently, it is essential to consider both transparency and user age when designing facial recognition feedback. The findings of this study could potentially inform the design of more effective and personalized visual feedback for facial motion, ultimately benefiting users in rehabilitation and exercise contexts. © 2023 Elsevier B.V., All rights reserved.","facial expression recognition; human–AI interaction; interaction design; mobile application; visual feedback design","","","","Funding: This research was funded by an Institute of Information and Communications Technology Planning and Evaluation (IITP) grant funded by the Korean government (MSIT) (No. 2022-0-00687, Development and commercialization of self-training and monitoring solutions with artificial intelligence-based for Parkinson’s disease patients), by a National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (No. NRF-2021R1G1A1012063), and by the Research Resettlement Fund 2020 for the new faculty of Kwangwoon University.","Introduction to Computer Vision, (2011); Kaneko, Takuhiro, Adaptive visual feedback generation for facial expression improvement with multi-task deep neural networks, pp. 327-331, (2016); Korean J Animat, (2018); Debnath, Bappaditya, A review of computer vision-based approaches for physical rehabilitation and assessment, Multimedia Systems, 28, 1, pp. 209-239, (2022); Body Language Communication an International Handbook on Multimodality in Human Interaction, (2014); Lindsay, Robin Williams, Comprehensive facial rehabilitation improves function in people with facial paralysis: A 5-year experience at the Massachusetts eye and ear infirmary, Physical Therapy, 90, 3, pp. 391-397, (2010); Pereira, Ligia Maxwell, Facial exercise therapy for facial palsy: Systematic review and meta-analysis, Clinical Rehabilitation, 25, 7, pp. 649-658, (2011); VanSwearingen, Jessie M., Changes in facial movement and synkinesis with facial neuromuscular reeducation, Plastic and Reconstructive Surgery, 111, 7, pp. 2370-2375, (2003); Sanford, Sean Patrick, The Effects of Visual Feedback Complexity on Training the Two-Legged Squat Exercise, ACM International Conference Proceeding Series, (2020); Valstar, Michel F., FERA 2017 - Addressing Head Pose in the Third Facial Expression Recognition and Analysis Challenge, pp. 839-847, (2017)","","Multidisciplinary Digital Publishing Institute (MDPI)","","","","","","22279032","","","","English","Article","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85165083116"
"Toki, E.I.; Tatsis, G.; Tatsis, V.A.; Plachouras, K.; Pange, J.; Tsoulos, I.G.","Toki, Eugenia I. (36544342700); Tatsis, Giorgos (35488955700); Tatsis, Vasileios A. (14625686000); Plachouras, Konstantinos (57200880847); Pange, Jenny (6507161641); Tsoulos, Ioannis G. (10042633500)","36544342700; 35488955700; 14625686000; 57200880847; 6507161641; 10042633500","Employing Classification Techniques on SmartSpeech Biometric Data towards Identification of Neurodevelopmental Disorders","2023","Signals","4","2","","401","420","0","11","10.3390/signals4020021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178371286&doi=10.3390%2Fsignals4020021&partnerID=40&md5=ddf01b874572f669d8d6257aff7cd0da","Department of Speech and Language Therapy, University of Ioannina, Ioannina, Greece; Department of Early Childhood Education, University of Ioannina, Ioannina, Greece; Department of Physics, University of Ioannina, Ioannina, Greece; Department of Computer Science and Engineering, University of Ioannina, Ioannina, Greece; Department of Informatics and Telecommunications, University of Ioannina, Ioannina, Greece","Toki, Eugenia I., Department of Speech and Language Therapy, University of Ioannina, Ioannina, Greece, Department of Early Childhood Education, University of Ioannina, Ioannina, Greece; Tatsis, Giorgos, Department of Speech and Language Therapy, University of Ioannina, Ioannina, Greece, Department of Physics, University of Ioannina, Ioannina, Greece; Tatsis, Vasileios A., Department of Speech and Language Therapy, University of Ioannina, Ioannina, Greece, Department of Computer Science and Engineering, University of Ioannina, Ioannina, Greece; Plachouras, Konstantinos, Department of Speech and Language Therapy, University of Ioannina, Ioannina, Greece; Pange, Jenny, Department of Early Childhood Education, University of Ioannina, Ioannina, Greece; Tsoulos, Ioannis G., Department of Informatics and Telecommunications, University of Ioannina, Ioannina, Greece","Early detection and evaluation of children at risk of neurodevelopmental disorders and/or communication deficits is critical. While the current literature indicates a high prevalence of neurodevelopmental disorders, many children remain undiagnosed, resulting in missed opportunities for effective interventions that could have had a greater impact if administered earlier. Clinicians face a variety of complications during neurodevelopmental disorders’ evaluation procedures and must elevate their use of digital tools to aid in early detection efficiently. Artificial intelligence enables novelty in taking decisions, classification, and diagnosis. The current research investigates the efficacy of various machine learning approaches on the biometric SmartSpeech datasets. These datasets come from a new innovative system that includes a serious game which gathers children’s responses to specifically designed speech and language activities and their manifestations, intending to assist during the clinical evaluation of neurodevelopmental disorders. The machine learning approaches were used by utilizing the algorithms Radial Basis Function, Neural Network, Deep Learning Neural Networks, and a variation of Grammatical Evolution (GenClass). The most significant results show improved accuracy (%) when using the eye tracking dataset; more specifically: (i) for the class Disorder with GenClass (92.83%), (ii) for the class Autism Spectrum Disorders with Deep Learning Neural Networks layer 4 (86.33%), (iii) for the class Attention Deficit Hyperactivity Disorder with Deep Learning Neural Networks layer 4 (87.44%), (iv) for the class Intellectual Disability with GenClass (86.93%), (v) for the class Specific Learning Disorder with GenClass (88.88%), and (vi) for the class Communication Disorders with GenClass (88.70%). Overall, the results indicated GenClass to be nearly the top competitor, opening up additional probes for future studies toward automatically classifying and assisting clinical assessments for children with neurodevelopmental disorders. © 2024 Elsevier B.V., All rights reserved.","classification; deep learning neural networks; grammatical evolution; machine learning optimizers; neurodevelopmental disorders; radial basis function neural network; SmartSpeech","","","","This research was funded by the project titled “Smart Computing Models, Sensors, and Early Diagnostic Speech and Language Deficiencies Indicators in Child Communi-cation”, with code HP1AB-28185 (MIS: 5033088), supported by the European Regional Development Fund (ERDF).","Text Revision Dsm IV TR, (1994); Dsm 5 Intellectual Disability Fact Sheet, (2013); Intellectual Disability, (2022); Thapar, Anita K., Neurodevelopmental disorders, The Lancet Psychiatry, 4, 4, pp. 339-346, (2017); Harris, James C., New classification for neurodevelopmental disorders in DSM-5, Current Opinion in Psychiatry, 27, 2, pp. 95-97, (2014); Identification of Specific Learning Disabilities A Summary of Research on Best Practices, (2019); Hyman, Susan L., Identification, evaluation, and management of children with autism spectrum disorder, Pediatrics, 145, 1, (2020); Bishop, Dorothy Vera Margaret, CATALISE: A multinational and multidisciplinary Delphi consensus study. Identifying language impairments in children, PLOS ONE, 11, 7, (2016); Hobson, Hannah M., Supporting the mental health of children with speech, language and communication needs: The views and experiences of parents, Autism and Developmental Language Impairments, 7, (2022); Rice, Catherine E., Defining in Detail and Evaluating Reliability of DSM-5 Criteria for Autism Spectrum Disorder (ASD) Among Children, Journal of Autism and Developmental Disorders, 52, 12, pp. 5308-5320, (2022)","","Multidisciplinary Digital Publishing Institute (MDPI)","","","","","","26246120","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85178371286"
"Vacca, R.A.; Augello, A.; Gallo, L.; Caggianese, G.; Malizia, V.; La Grutta, S.; Murero, M.; Valenti, D.; Tullo, A.; Balech, B.; Marzano, F.; Ghezzo, A.; Tancredi, G.; Turchetta, A.; Riccio, M.P.; Bravaccio, C.; Scala, I.","Vacca, Rosa Anna Nna (6701582913); Augello, Agnese (23392168800); Gallo, Luigi (24072878600); Caggianese, Giuseppe (55372183800); Malizia, Velia (16432530700); La Grutta, Stefania (6701854558); Murero, Monica (6505909536); Valenti, Daniela (57194326441); Tullo, Apollonia (7005615636); Balech, Bachir (56692978900); Marzano, Flaviana (54397313800); Ghezzo, Alessandro (35304817600); Tancredi, Giancarlo (7004011260); Turchetta, Attilio (7004262371); Riccio, Maria Pia (55202207900); Bravaccio, Carmela (6603209429); Scala, Iris (6507742398)","6701582913; 23392168800; 24072878600; 55372183800; 16432530700; 6701854558; 6505909536; 57194326441; 7005615636; 56692978900; 54397313800; 35304817600; 7004011260; 7004262371; 55202207900; 6603209429; 6507742398","Serious Games in the new era of digital-health interventions: A narrative review of their therapeutic applications to manage neurobehavior in neurodevelopmental disorders","2023","Neuroscience and Biobehavioral Reviews","149","","105156","","","0","30","10.1016/j.neubiorev.2023.105156","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152648451&doi=10.1016%2Fj.neubiorev.2023.105156&partnerID=40&md5=ef1d5e67883bb63842f58fe85fb50454","Bioenergetics and Molecular Biotechnologies, Consiglio Nazionale delle Ricerche, Rome, Italy; Institute for High Performance Computing and Networking, National Research Council of Italy (ICAR-CNR), Italy; Institute of Translational Pharmacology, Consiglio Nazionale delle Ricerche, Rome, Italy; Department of Social Sciences, Università degli Studi di Napoli Federico II, Naples, Italy; Grioni Center-Danelli Foundation, Lodi, Italy; Department of Pediatrics, Sapienza Università di Roma, Rome, Italy; Sport Medicine Unit, IRCCS Ospedale Pediatrico Bambino Gesù, Rome, Italy; Department of Maternal and Child Health, Azienda Ospedaliera Universitaria Federico II, Naples, Italy; Department of Translational Medical Sciences, Università degli Studi di Napoli Federico II, Naples, Italy; Department of Maternal and Child Health, Università degli Studi di Napoli Federico II, Naples, Italy","Vacca, Rosa Anna Nna, Bioenergetics and Molecular Biotechnologies, Consiglio Nazionale delle Ricerche, Rome, Italy; Augello, Agnese, Institute for High Performance Computing and Networking, National Research Council of Italy (ICAR-CNR), Italy; Gallo, Luigi, Institute for High Performance Computing and Networking, National Research Council of Italy (ICAR-CNR), Italy; Caggianese, Giuseppe, Institute for High Performance Computing and Networking, National Research Council of Italy (ICAR-CNR), Italy; Malizia, Velia, Institute of Translational Pharmacology, Consiglio Nazionale delle Ricerche, Rome, Italy; La Grutta, Stefania, Institute of Translational Pharmacology, Consiglio Nazionale delle Ricerche, Rome, Italy; Murero, Monica, Department of Social Sciences, Università degli Studi di Napoli Federico II, Naples, Italy; Valenti, Daniela, Bioenergetics and Molecular Biotechnologies, Consiglio Nazionale delle Ricerche, Rome, Italy; Tullo, Apollonia, Bioenergetics and Molecular Biotechnologies, Consiglio Nazionale delle Ricerche, Rome, Italy; Balech, Bachir, Bioenergetics and Molecular Biotechnologies, Consiglio Nazionale delle Ricerche, Rome, Italy; Marzano, Flaviana, Bioenergetics and Molecular Biotechnologies, Consiglio Nazionale delle Ricerche, Rome, Italy; Ghezzo, Alessandro, Grioni Center-Danelli Foundation, Lodi, Italy; Tancredi, Giancarlo, Department of Pediatrics, Sapienza Università di Roma, Rome, Italy; Turchetta, Attilio, Sport Medicine Unit, IRCCS Ospedale Pediatrico Bambino Gesù, Rome, Italy; Riccio, Maria Pia, Department of Maternal and Child Health, Azienda Ospedaliera Universitaria Federico II, Naples, Italy; Bravaccio, Carmela, Department of Translational Medical Sciences, Università degli Studi di Napoli Federico II, Naples, Italy; Scala, Iris, Department of Maternal and Child Health, Università degli Studi di Napoli Federico II, Naples, Italy","Children and adolescents with neurodevelopmental disorders generally show adaptive, cognitive and motor skills impairments associated with behavioral problems, i.e., alterations in attention, anxiety and stress regulation, emotional and social relationships, which strongly limit their quality of life. This narrative review aims at providing a critical overview of the current knowledge in the field of serious games (SGs), known as digital instructional interactive videogames, applied to neurodevelopmental disorders. Indeed, a growing number of studies is drawing attention to SGs as innovative and promising interventions in managing neurobehavioral and cognitive disturbs in children with neurodevelopmental disorders. Accordingly, we provide a literature overview of the current evidence regarding the actions and the effects of SGs. In addition, we describe neurobehavioral alterations occurring in some specific neurodevelopmental disorders for which a possible therapeutic use of SGs has been suggested. Finally, we discuss findings obtained in clinical trials using SGs as digital therapeutics in neurodevelopment disorders and suggest new directions and hypotheses for future studies to bridge the gaps between clinical research and clinical practice. © 2023 Elsevier B.V., All rights reserved.","Attention-Deficit Hyperactivity Disorder; Autism spectrum disorder; Behavioral symptomatology; Digital therapeutics; Down syndrome; Fragile X syndrome; Human-Machine Communication; Neurodevelopmental disorders; Serious games","fragile X mental retardation protein; adaptive behavior; artificial intelligence; attention deficit hyperactivity disorder; autism; behavior disorder; child; clinical practice; clinical research; cognition; comorbidity; digital health intervention; Down syndrome; electroencephalography; fragile X syndrome; game; health service; human; internet of things; mental disease; nerve cell differentiation; physical activity; quality of life; Review; serious games; social competence; social interaction; systematic review; video game; virtual reality; adolescent; anxiety; human relation; psychology; Adolescent; Anxiety; Attention Deficit Disorder with Hyperactivity; Autism Spectrum Disorder; Child; Humans; Interpersonal Relations; Neurodevelopmental Disorders; Quality of Life","","","This work was funded by the Italian National Council of Research with the grant progetti@cnr (1/2020) SMILER (Serious gaMes as emerging e-health Interventions for young people with neurologicaL or rEspiratory disoRders).","Adinolfi, Francesco, SmartCARE—An ICT platform in the domain of stroke pathology to manage rehabilitation treatment and telemonitoring at home, Smart Innovation, Systems and Technologies, 55, pp. 39-49, (2016); Alba-Rueda, Alvaro, Exergaming for Physical Therapy in Patients with Down Syndrome: A Systematic Review and Meta-Analysis of Randomized-Controlled Trials, Games for Health Journal, 11, 2, pp. 67-78, (2022); Serious Games Simulation for Risks Management, (2011); Anderson-Hanley, Cay, Autism and exergaming: Effects on repetitive behaviors and cognition, Psychology Research and Behavior Management, 4, pp. 129-137, (2011); International Conference on Games and Learning Alliance, (2015); Avila-Pesantez, Diego Fernando, Towards the improvement of ADHD children through augmented reality serious games: Preliminary results, IEEE Global Engineering Education Conference, EDUCON, 2018-April, pp. 843-848, (2018); Bach, Snow, Rett Syndrome and Fragile X Syndrome: Different Etiology With Common Molecular Dysfunctions, Frontiers in Cellular Neuroscience, 15, (2021); Bal, Elgiz, Emotion recognition in children with autism spectrum disorders: Relations to eye gaze and autonomic state, Journal of Autism and Developmental Disorders, 40, 3, pp. 358-370, (2010); Balogh, Lívia, Genetics in the ADHD Clinic: How Can Genetic Testing Support the Current Clinical Practice?, Frontiers in Psychology, 13, (2022); Balzotti, Angela, Comparison of the efficacy of gesture-verbal treatment and doll therapy for managing neuropsychiatric symptoms in older patients with dementia, International Journal of Geriatric Psychiatry, 34, 9, pp. 1308-1315, (2019)","","Elsevier Ltd","","","","","","18737528; 01497634","","NBRED","37019246","English","Review","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85152648451"
"Silva, T.; Verde, D.; Paiva, S.; Barreto, L.; Pereira, A.I.","Silva, Tania (57315189900); Verde, David (57315607600); Paiva, Sara (36447905300); Barreto, Luís (24723589900); Pereira, Ana I. (15071961600)","57315189900; 57315607600; 36447905300; 24723589900; 15071961600","Accessibility strategies to promote inclusive mobility through multi-objective approach","2023","SN Applied Sciences","5","5","150","","","0","4","10.1007/s42452-023-05349-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158069564&doi=10.1007%2Fs42452-023-05349-0&partnerID=40&md5=1a89349665c2f7ce11e2f677ae62a219","ADiT-LAB, Instituto Politécnico de Viana do Castelo, Viana do Castelo, Portugal; Research Centre in Digitalization and Intelligent Robotics (CeDRI), Instituto Politécnico de Bragança, Braganca, Portugal; ALGORITMI Research Centre/LASI, Universidade do Minho, Braga, Portugal","Silva, Tania, ADiT-LAB, Instituto Politécnico de Viana do Castelo, Viana do Castelo, Portugal, Research Centre in Digitalization and Intelligent Robotics (CeDRI), Instituto Politécnico de Bragança, Braganca, Portugal; Verde, David, ADiT-LAB, Instituto Politécnico de Viana do Castelo, Viana do Castelo, Portugal; Paiva, Sara, ADiT-LAB, Instituto Politécnico de Viana do Castelo, Viana do Castelo, Portugal, ALGORITMI Research Centre/LASI, Universidade do Minho, Braga, Portugal; Barreto, Luís, ADiT-LAB, Instituto Politécnico de Viana do Castelo, Viana do Castelo, Portugal; Pereira, Ana I., Research Centre in Digitalization and Intelligent Robotics (CeDRI), Instituto Politécnico de Bragança, Braganca, Portugal","In recent decades, urban mobility has assumed a need for adaptation due to the more significant congestion experienced in cities and the growing focus on sustainability. Several solutions are proposed to help citizens move around in an urban environment. Most are not yet aware of the universal and accessible aspect that these solutions must have. This paper proposes a route support system embedded in a mobile application, Viana+Acessível, using a multi-objective approach. The application aims to promote accessible mobility within the city, contributing to physical and psychological well-being for citizens with reduced mobility, temporary or permanently, such as people with spectrum autism disorder, the visually impaired, wheelchair users, pregnant, and the elderly. For the evaluation of the algorithms, four objective measures were considered: accessibility, slope, time, and length of the paths. The tests carried out with different routing algorithms showed that the A-Star presented the fastest results in terms of execution time compared to the Dijkstra, Floyd–Warshall, and Bellman–Ford. When analysing in a multi-objective approach, time, slope and accessibility were demonstrated to be conflicting objectives. Bi-objective and tri-objective were applied and Pareto front was explored. Graphical abstract: [Figure not available: see fulltext.] © 2023 Elsevier B.V., All rights reserved.","Decision support system; Inclusive mobility; Multi-objective optimisation; Routing algorithm","Artificial intelligence; Multiobjective optimization; Routing algorithms; Sustainable development; Inclusive mobility; Mobile applications; Multi objective; Multi-objectives optimization; Objective approaches; Psychological well-being; Spectra's; Support systems; Urban environments; Urban mobility; Decision support systems","","","This work has been supported by FCT—Fundação para a Ciência e Tecnologia within the Project Scope UIDB/05757/2020 and UIDP/05757/2020.","Planning and Design for Sustainable Urban Mobility Global Report on Human Settlements 2013, (2013); undefined; Rosado da Cruz, António Miguel, Modern software engineering methodologies for mobile and cloud environments, pp. 1-355, (2016); Roca Bosch, Elisabet, The DIGNITY Project-Toward a System of Inclusive Digital Mobility in the Barcelona Metropolitan Area, Transportation Research Procedia, 58, pp. 134-141, (2021); Azevedo, Gislaine A., Sustainable urban mobility analysis for elderly and disabled people in São Paulo, Scientific Reports, 11, 1, (2021); Tieben, Nicole, Social mobility and inequality in the life course: Exploring the relevance of context, Research in Social Stratification and Mobility, 32, 1, pp. 1-6, (2013); undefined; Wang, Yushuai, Epigenetic and transcriptional responses underlying mangrove adaptation to UV-B, iScience, 24, 10, (2021); Plyushteva, Anna, Does night-time public transport contribute to inclusive night mobility? Exploring Sofia's night bus network from a gender perspective, Transport Policy, 87, pp. 41-50, (2020); Ntakolia, Charis, A route planning framework for smart wearable assistive navigation systems, SN Applied Sciences, 3, 1, (2021)","","Springer Nature","","","","","","25233971","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85158069564"
"Mavridou, I.; Archer, J.A.W.; Cleal, A.; Fatoorechi, M.; Stankoski, S.; Kiprijanovska, I.; Broulidakis, J.; Gjoreski, M.; Nduka, C.; Gjoreski, H.","Mavridou, Ifigeneia I. (57063037500); Archer, James William (57993417700); Cleal, Andrew (57194046069); Fatoorechi, Mohsen (56394589700); Stankoski, Simon (57212023515); Kiprijanovska, Ivana (57212017647); Broulidakis, John (57203764688); Gjoreski, Martin (56470741800); Nduka, Charles C. (6701344479); Gjoreski, Hristijan (53979604500)","57063037500; 57993417700; 57194046069; 56394589700; 57212023515; 57212017647; 57203764688; 56470741800; 6701344479; 53979604500","OCOsense Glasses for Facial Expressions Recognition and Contextual Affective Computing in Real World and Augmented Reality","2023","","","","","81","84","0","1","10.1145/3544793.3560325","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158933058&doi=10.1145%2F3544793.3560325&partnerID=40&md5=f4f513ddf61847c13f725bb4a20c384a","Emteq Ltd., Brighton, United Kingdom; Università della Svizzera italiana, Lugano, Switzerland; Ss. Cyril and Methodius University in Skopje, Skopje, North Macedonia","Mavridou, Ifigeneia I., Emteq Ltd., Brighton, United Kingdom; Archer, James William, Emteq Ltd., Brighton, United Kingdom; Cleal, Andrew, Emteq Ltd., Brighton, United Kingdom; Fatoorechi, Mohsen, Emteq Ltd., Brighton, United Kingdom; Stankoski, Simon, Emteq Ltd., Brighton, United Kingdom; Kiprijanovska, Ivana, Emteq Ltd., Brighton, United Kingdom; Broulidakis, John, Emteq Ltd., Brighton, United Kingdom; Gjoreski, Martin, Università della Svizzera italiana, Lugano, Switzerland; Nduka, Charles C., Emteq Ltd., Brighton, United Kingdom; Gjoreski, Hristijan, Emteq Ltd., Brighton, United Kingdom, Ss. Cyril and Methodius University in Skopje, Skopje, North Macedonia","The paper presents the novel OCOSenseTM smart glasses with integrated sensors, primarily non-contact optomyographic (OMG) OCOTM sensors, 9-axis inertial measurement unit (IMU), and an altimeter. The glasses connect with a smartphone application, which facilitates the continuous and real-time measurements of facial-muscles activation and head movement, thus allowing for the detection of facial expressions and the activities of the user in real-time. We will demonstrate how the system is used in practice, i.e., a participant will wear the OCOSenseTM glasses, which will stream the sensor data to a tablet, where the real-time visualization of the sensor data and the data interpretation will be presented such as facial expressions (smile, frown, surprise) and activities. We believe that the OCOSenseTM glasses are the next big thing in wearables, which will allow for better understanding of the user's context, activities, emotional state, and more, which can be easily coupled within Augmented and Extended Reality environments. © 2025 Elsevier B.V., All rights reserved.","Activity; Affective Computing; Emotion Recognition; Facial Expressions; Glasses; IMU; Machine Learning; OMG; Valence","Augmented reality; Data visualization; Face recognition; Glass; Internet of things; Machine learning; Wearable computers; Activity; Affective Computing; Emotion recognition; Facial expression recognition; Facial Expressions; Inertial measurements units; Machine-learning; Optomyographic; Sensors data; Valence; Emotion Recognition","","","We thank all our colleagues from Emteq Ltd. who worked on the development of the OCOSenseTM system. This work was supported by Innovate UK under the project \u201CMobile Observation Of Depression (MOOD) platform for digital phenotyping\u201D (Grant number 105207).","Facial Action Coding System, (1978); Expression of the Emotions in Man and Animals, (1872); Mecanisme De La Physionomie Humaine, (1862); Fridlund, Alan J., Guidelines for Human Electromyographic Research, Psychophysiology, 23, 5, pp. 567-589, (1986); Feldman Barrett, Lisa Feldman, Emotional Expressions Reconsidered: Challenges to Inferring Emotion From Human Facial Movements, Psychological Science in the Public Interest, 20, 1, pp. 1-68, (2019); Russell, James A., Core affect, prototypical emotional episodes, and other things called emotion: Dissecting the elephant, Journal of Personality and Social Psychology, 76, 5, pp. 805-819, (1999); Measurements, (2015); How Emotions are Made by Lisa Feldman Barrett, (2018)","","Association for Computing Machinery, Inc","ACM SIGCHI; ACM SIGMOBILE","2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers, UbiComp/ISWC 2022","","Cambridge","188163","","9781450394239","","","English","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85158933058"
"Espina Romero, L.C.; Aguirre Franco, S.L.; Dworaczek Conde, H.O.; Guerrero Alcedo, J.M.; Ríos Parra, D.E.; Rave Ramírez, J.C.","Espina Romero, Lorena Del Carmen (57496122100); Aguirre Franco, Sandra Lucia (58179453500); Dworaczek Conde, Helga Ofelia (58179453600); Guerrero Alcedo, Jesús Manuel (57188701107); Ríos Parra, Doile Enrique (57221951450); Rave Ramírez, Juan Carlos (58180113700)","57496122100; 58179453500; 58179453600; 57188701107; 57221951450; 58180113700","Soft skills in personnel training: Report of publications in scopus, topics explored and future research agenda","2023","Heliyon","9","4","e15468","","","0","19","10.1016/j.heliyon.2023.e15468","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152270571&doi=10.1016%2Fj.heliyon.2023.e15468&partnerID=40&md5=0ea4eb17f63625bf43e246f64431f663","Graduate School, Universidad San Ignacio de Loyola, Lima, Peru; Unidad Central del Valle del Cauca, Tulua, Colombia; Universidad Santo Tomás, Bogota, Colombia; Universidad Científica del Sur, Lima, Peru; Universidad Popular del Cesar, Valledupar, Colombia; Universidad del Quindio, Armenia, Colombia","Espina Romero, Lorena Del Carmen, Graduate School, Universidad San Ignacio de Loyola, Lima, Peru; Aguirre Franco, Sandra Lucia, Unidad Central del Valle del Cauca, Tulua, Colombia; Dworaczek Conde, Helga Ofelia, Universidad Santo Tomás, Bogota, Colombia; Guerrero Alcedo, Jesús Manuel, Universidad Científica del Sur, Lima, Peru; Ríos Parra, Doile Enrique, Universidad Popular del Cesar, Valledupar, Colombia; Rave Ramírez, Juan Carlos, Universidad del Quindio, Armenia, Colombia","Recent research has documented the interest of organizations in training their staff in soft skills, but few studies have been found. Therefore, the objective of this research was to analyze 753 publications in the Scopus database related to soft skills in staff training during the period 1999–2021. These documents were analyzed to identify the main information, the most explored areas, and a future research agenda; all under a bibliometric and bibliographic approach with the help of RStudio and VOSviewer software. The results showed that the keywords with the most co-occurrence were personnel training (n = 110) and soft skills (n = 79). The year with the most documents was 2021 (n = 121). The country with the most publications was the United Kingdom (n = 199). Medicine is the subject area with the most documents (n = 278) and the Article is the type of document with the most studies (n = 566). Eleven areas of further exploration were identified: “Soft skills in software engineering at the higher education level”, “Soft skills and communication”, “Soft skills and engineering education”, “Soft skills in virtual environments”, “Soft skills in machine learning”, “Serious games in teaching soft skills”, “Soft skills for problem-based learning”, “Soft skills for project management”, “Soft skills and technical skills”, “Project-based learning for the assessment of soft skills” and “Soft leadership skills”. Five potential areas for future research were derived: soft skills in collaborative work (CSCL), soft skills in computer-aided collaborative work (CSCW), facial expressions as a mirror of soft skills, soft skills for employability and Professional Development Plan (PDP) to assess soft skills. In conclusion, this Review type document on soft skills in personnel training helped to identify the most studied topics during the evaluated period, as well as to identify the little explored topics for future research. © 2023 Elsevier B.V., All rights reserved.","Employment; Human resource management; Soft skills; Staff training; Technical skills","","","","The authors thank all the participants, who collaborated selflessly in the study.","da Silva, Leonardo Breno Pessoa, Evaluation of Soft Skills Through Educational Testbed 4.0, Communications in Computer and Information Science, 1488 CCIS, pp. 678-690, (2021); Kupryaeva, Maria N., Soft skills development in personnel training, E3S Web of Conferences, 273, (2021); Fareri, Silvia, SkillNER: Mining and mapping soft skills from any text, Expert Systems with Applications, 184, (2021); Rodriguez, Jose Manriquez, Portfolio of Soft Skills Monitoring, Proceedings - International Conference of the Chilean Computer Science Society, SCCC, 2018-November, (2018); Schwartz-Chassidim, Hadas, Fostering soft skills in project-oriented learning within an agile atmosphere, European Journal of Engineering Education, 43, 4, pp. 638-650, (2018); Joseph, Damien, Soft skills and creativity in IS professionals, Proceedings of the Hawaii International Conference on System Science, (1999); Gudkova, Svetlana Anatolyevna, Soft Skills Simulation and Assessment: Qualimetric Approach for Smart University, Smart Innovation, Systems and Technologies, 188, pp. 527-537, (2020); Fejes, Csilla, Enhancement and assessment of engineering soft skills in a game-based learning environment, Proceedings of the European Conference on Games-based Learning, 2015-January, pp. 178-185, (2015); Espina Romero, Lorena Del Carmen, Neuroscience and its applications in Education: a bibliometric review, Revista Venezolana de Gerencia, 27, 98, pp. 512-529, (2022); Guerrero Alcedo, Jesús Manuel, Gamification in the University Context: Bibliometric Review in Scopus (2012-2022), International Journal of Learning, Teaching and Educational Research, 21, 5, pp. 309-325, (2022)","","Elsevier Ltd","","","","","","24058440","","","","English","Review","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85152270571"
"Sajjad, M.; Ullah, F.U.M.; Ullah, M.; Christodoulou, G.; Sudarshana, F.; Hijji, M.; Muhammad, K.; Rodrigues, J.J.P.C.","Sajjad, Muhammad (57215455402); Ullah, Fath U.Min (57203189784); Ullah, Mohib (7006278145); Christodoulou, Georgia (58100912100); Sudarshana, K. (42461035200); Hijji, Mohammad (56543747000); Muhammad, Khan (8942252200); Rodrigues, Joel J.P.C. (25930566300)","57215455402; 57203189784; 7006278145; 58100912100; 42461035200; 56543747000; 8942252200; 25930566300","A comprehensive survey on deep facial expression recognition: challenges, applications, and future guidelines","2023","Alexandria Engineering Journal","68","","","817","840","0","104","10.1016/j.aej.2023.01.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147953411&doi=10.1016%2Fj.aej.2023.01.017&partnerID=40&md5=66b3c51a18815134485cdf13449ed348","Norges Teknisk-Naturvitenskapelige Universitet, Trondheim, Norway; Sejong University, Seoul, South Korea; CATALINK LIMITED, Nicosia, Cyprus; University of Tabuk, Tabuk, Saudi Arabia; Department of Artificial Intelligence, Sungkyunkwan University, Seoul, South Korea; College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; Instituto de Telecomunicações, Aveiro, Portugal","Sajjad, Muhammad, Norges Teknisk-Naturvitenskapelige Universitet, Trondheim, Norway; Ullah, Fath U.Min, Sejong University, Seoul, South Korea; Ullah, Mohib, Norges Teknisk-Naturvitenskapelige Universitet, Trondheim, Norway; Christodoulou, Georgia, CATALINK LIMITED, Nicosia, Cyprus; Sudarshana, K., Norges Teknisk-Naturvitenskapelige Universitet, Trondheim, Norway; Hijji, Mohammad, University of Tabuk, Tabuk, Saudi Arabia; Muhammad, Khan, Department of Artificial Intelligence, Sungkyunkwan University, Seoul, South Korea; Rodrigues, Joel J.P.C., College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China, Instituto de Telecomunicações, Aveiro, Portugal","Facial expression recognition (FER) is an emerging and multifaceted research topic. Applications of FER in healthcare, security, safe driving, and so forth have contributed to the credibility of these methods and their adoption in human-computer interaction for intelligent outcomes. Computational FER mimics human facial expression coding skills and conveys important cues that complement speech to assist listeners. Similarly, FER methods based on deep learning and artificial intelligence (AI) techniques have been developed with edge modules to ensure efficiency and real-time processing. To this end, numerous studies have explored different aspects of FER. Surveys of FER have focused on the literature on hand-crafted techniques, with a focus on general methods for local servers but largely neglecting edge vision-inspired deep learning and AI-based FER technologies. To consider these missing aspects, in this study, the existing literature on FER is thoroughly analyzed and surveyed, and the working flow of FER methods, their integral and intermediate steps, and pattern structures are highlighted. Further, the limitations in existing FER surveys are discussed. Next, FER datasets are investigated in depth, and the associated challenges and problems are discussed. In contrast to existing surveys, FER methods are considered for edge vision (on e.g., smartphone or Raspberry Pi, devices, etc.), and different measures to evaluate the performance of FER methods are comprehensively discussed. Finally, recommendations and some avenues for future research are suggested to facilitate further development and implementation of FER technologies. © 2023 Elsevier B.V., All rights reserved.","Artificial intelligence; Deep learning; Edge vision; Facial expression recognition; Health care; Machine learning; Security","Deep learning; Face recognition; Learning systems; mHealth; Coding skills; Edge vision; Facial expression recognition; Human facial expressions; Machine-learning; Recognition methods; Research topics; Safe driving; Security; Human computer interaction","","","This research was funded by the European Union through the Horizon 2020 Research and Innovation Program, in the context of the ALAMEDA (Bridging the Early Diagnosis and Treatment Gap of Brain Diseases via Smart, Connected, Proactive and Evidence-based Technological Interventions) project under grant agreement No GA 101017558. This work is also partially funded by FCT/MCTES through national funds and when applicable co-funded EU funds under the Project UIDB/50008/2020; and by the Brazilian National Council for Scientific and Technological Development-CNPq, via Grant No. 313036/2020-9.","Nan, Yahui, A-MobileNet: An approach of facial expression recognition, Alexandria Engineering Journal, 61, 6, pp. 4435-4444, (2022); Li, Zhe, Facial expression-based analysis on emotion correlations, hotspots, and potential occurrence of urban crimes, Alexandria Engineering Journal, 60, 1, pp. 1411-1420, (2021); Mannepalli, Kasiprasad, A novel Adaptive Fractional Deep Belief Networks for speaker emotion recognition, Alexandria Engineering Journal, 56, 4, pp. 485-497, (2017); Tonguc, Güray, Automatic recognition of student emotions from facial expressions during a lecture, Computers and Education, 148, (2020); Yun, Sangseok, Social skills training for children with autism spectrum disorder using a robotic behavioral intervention system, Autism Research, 10, 7, pp. 1306-1323, (2017); undefined, (2021); Visual Computer, (2022); Jeong, Mira, Driver’s facial expression recognition in real-time for safe driving, Sensors, 18, 12, (2018); Kaulard, Kathrin, The MPI facial expression database - a validated database of emotional and conversational facial expressions, PLOS ONE, 7, 3, (2012); Ali, Mohammad Rafayet, Facial expressions can detect Parkinson’s disease: preliminary evidence from videos collected online, npj Digital Medicine, 4, 1, (2021)","","Elsevier B.V.","","","","","","11100168","","","","English","Review","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85147953411"
"Bibbò, L.; Cotroneo, F.; Vellasco, M.","Bibbò, Luigi (55817450700); Cotroneo, Francesco (25640699500); Vellasco, Marley Maria Bernardes Rebuzzi (56200580000)","55817450700; 25640699500; 56200580000","Emotional Health Detection in HAR: New Approach Using Ensemble SNN","2023","Applied Sciences (Switzerland)","13","5","3259","","","0","8","10.3390/app13053259","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149946301&doi=10.3390%2Fapp13053259&partnerID=40&md5=5cf0222d47c448f9ff4f01cbc6db1868","Department of Information Engineering, Infrastructure and Sustainable Energy (DIIES), Università degli Studi di Reggio Calabria, Reggio Calabria, Italy; Nophys S.r.l.s., Rome, Italy; Department of Electrical Engineering, Pontifícia Universidade Católica do Rio de Janeiro, Rio de Janeiro, Brazil","Bibbò, Luigi, Department of Information Engineering, Infrastructure and Sustainable Energy (DIIES), Università degli Studi di Reggio Calabria, Reggio Calabria, Italy; Cotroneo, Francesco, Nophys S.r.l.s., Rome, Italy; Vellasco, Marley Maria Bernardes Rebuzzi, Department of Electrical Engineering, Pontifícia Universidade Católica do Rio de Janeiro, Rio de Janeiro, Brazil","Computer recognition of human activity is an important area of research in computer vision. Human activity recognition (HAR) involves identifying human activities in real-life contexts and plays an important role in interpersonal interaction. Artificial intelligence usually identifies activities by analyzing data collected using different sources. These can be wearable sensors, MEMS devices embedded in smartphones, cameras, or CCTV systems. As part of HAR, computer vision technology can be applied to the recognition of the emotional state through facial expressions using facial positions such as the nose, eyes, and lips. Human facial expressions change with different health states. Our application is oriented toward the detection of the emotional health of subjects using a self-normalizing neural network (SNN) in cascade with an ensemble layer. We identify the subjects’ emotional states through which the medical staff can derive useful indications of the patient’s state of health. © 2023 Elsevier B.V., All rights reserved.","computer vision; deep learning; ensemble; face detection; face emotion recognition; HAR; SNN; vectorflow","","","","This work is supported by the Italian MIUR Project under GRANT PON Research and Innovation 2014–2020 Project Code C35E19000020001, AIM 1839112-1: Technologies for the living environment.","Aggarwal, Jagdishkumar Kumar K., Human activity analysis: A review, ACM Computing Surveys, 43, 3, (2011); Li, Ming, Facial Expression Recognition with Identity and Emotion Joint Learning, IEEE Transactions on Affective Computing, 12, 2, pp. 544-550, (2021); Adjabi, Insaf, Past, present, and future of face recognition: A review, Electronics (Switzerland), 9, 8, pp. 1-53, (2020); Tan, Lianzhi, Group emotion recognition with individual facial emotion CNNs and global image based CNNs, 2017-January, pp. 549-552, (2017); Song, Zhenjie, Facial Expression Emotion Recognition Model Integrating Philosophy and Machine Learning Theory, Frontiers in Psychology, 12, (2021); Sun, Ai, Facial expression recognition using optimized active regions, Human-centric Computing and Information Sciences, 8, 1, (2018); de Risi, Marco, Facial emotion decoding in patients with Parkinson's disease, International Journal of Neuroscience, 128, 1, pp. 71-78, (2018); Liu, Hui, Automatic Facial Recognition of Williams-Beuren Syndrome Based on Deep Convolutional Neural Networks, Frontiers in Pediatrics, 9, (2021); Zhao, Mengyang, VoxelEmbed: 3D Instance Segmentation and Tracking with Voxel Embedding based Deep Learning, Lecture Notes in Computer Science, 12966 LNCS, pp. 437-446, (2021); Zheng, Qinghe, Improvement of Generalization Ability of Deep CNN via Implicit Regularization in Two-Stage Training Process, IEEE Access, 6, pp. 15844-15869, (2018)","","MDPI","","","","","","20763417","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85149946301"
"Baldassarri, S.; García de Quirós, J.; Beltrán, J.R.; Álvarez, P.","Baldassarri, Sandra (6508141889); García de Quirós, Jorge (57211518930); Beltrán, José Ramón (7102677859); Álvarez, Pedro J. (35271271700)","6508141889; 57211518930; 7102677859; 35271271700","Wearables and Machine Learning for Improving Runners’ Motivation from an Affective Perspective","2023","Sensors","23","3","1608","","","0","4","10.3390/s23031608","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147893353&doi=10.3390%2Fs23031608&partnerID=40&md5=12c5a3a232343a36d2473614b03e5f50","Department of Computer Science and Systems Engineering, Universidad de Zaragoza, Zaragoza, Spain; Electronic Engineering and Communications Department, Universidad de Zaragoza, Zaragoza, Spain","Baldassarri, Sandra, Department of Computer Science and Systems Engineering, Universidad de Zaragoza, Zaragoza, Spain; García de Quirós, Jorge, Department of Computer Science and Systems Engineering, Universidad de Zaragoza, Zaragoza, Spain; Beltrán, José Ramón, Electronic Engineering and Communications Department, Universidad de Zaragoza, Zaragoza, Spain; Álvarez, Pedro J., Department of Computer Science and Systems Engineering, Universidad de Zaragoza, Zaragoza, Spain","Wearable technology is playing an increasing role in the development of user-centric applications. In the field of sports, this technology is being used to implement solutions that improve athletes’ performance, reduce the risk of injury, or control fatigue, for example. Emotions are involved in most of these solutions, but unfortunately, they are not monitored in real-time or used as a decision element that helps to increase the quality of training sessions, nor are they used to guarantee the health of athletes. In this paper, we present a wearable and a set of machine learning models that are able to deduce runners’ emotions during their training. The solution is based on the analysis of runners’ electrodermal activity, a physiological parameter widely used in the field of emotion recognition. As part of the DJ-Running project, we have used these emotions to increase runners’ motivation through music. It has required integrating the wearable and the models into the DJ-Running mobile application, which interacts with the technological infrastructure of the project to select and play the most suitable songs at each instant of the training. © 2023 Elsevier B.V., All rights reserved.","emotion recognition; machine learning; music recommendation; running; wearable devices","Machine learning; Motivation; Physiological models; Speech recognition; Wearable technology; Decision element; Emotion recognition; Machine-learning; Music recommendation; Performance; Real- time; Running; Training sessions; User-centric; Wearable devices; Emotion Recognition; athletic performance; electronic device; human; machine learning; mobile application; motivation; Athletic Performance; Humans; Machine Learning; Mobile Applications; Wearable Electronic Devices","","","This research has been supported by the PDC2021-121072-C22, TED2021-130374B-C22 and RTI2018-096986-B-C31 projects, granted by the Spanish Ministerio de Economía y Competitividad, and the DisCo-T21-20R and Affective-Lab-T60-20R projects, granted by the Aragonese Government.","Vijayan, Vini, Review of wearable devices and data collection considerations for connected health, Sensors, 21, 16, (2021); Wang, Huizhong, Research on the Application of Wireless Wearable Sensing Devices in Interactive Music, Journal of Sensors, 2021, (2021); Moj Appl Bionics Biomech, (2018); Seshadri, Dhruv R., Wearable sensors for monitoring the physiological and biochemical profile of the athlete, npj Digital Medicine, 2, 1, (2019); Liu, Lei, A Focused Review on the Flexible Wearable Sensors for Sports: From Kinematics to Physiologies, Micromachines, 13, 8, (2022); Nithya, N., Role of Wearables in Sports based on Activity recognition and biometric parameters: A Survey, pp. 1700-1705, (2021); Mencarini, Eleonora, Designing Wearable Systems for Sports: A Review of Trends and Opportunities in Human-Computer Interaction, IEEE Transactions on Human-Machine Systems, 49, 4, pp. 314-325, (2019); Duarte, Manuela Brito, Wearable Inertial Sensor Approach for Postural Adjustment Assessments during Predictable Perturbations in Sport, Sensors, 22, 21, (2022); McCarthy, Paul Joseph, Positive emotion in sport performance: Current status and future directions, International Review of Sport and Exercise Psychology, 4, 1, pp. 50-69, (2011); Lane, Andrew M., How should I regulate my emotions if I want to run faster?, European Journal of Sport Science, 16, 4, pp. 465-472, (2016)","","MDPI","","","","","","14248220","","","36772647","English","Article","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85147893353"
"Neggaz, I.; Neggaz, N.; Fizazi, H.","Neggaz, Imene (57471872000); Neggaz, Nabil (35095357200); Fizazi, Hadria (54997931900)","57471872000; 35095357200; 54997931900","Boosting Archimedes optimization algorithm using trigonometric operators based on feature selection for facial analysis","2023","Neural Computing and Applications","35","5","","3903","3923","0","8","10.1007/s00521-022-07925-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139934395&doi=10.1007%2Fs00521-022-07925-8&partnerID=40&md5=f7ca8b4d3449dac2c103cacb886f704e","Université des Sciences et de la Technologie d’Oran Mohamed-Boudiaf, Oran, Algeria","Neggaz, Imene, Université des Sciences et de la Technologie d’Oran Mohamed-Boudiaf, Oran, Algeria; Neggaz, Nabil, Université des Sciences et de la Technologie d’Oran Mohamed-Boudiaf, Oran, Algeria; Fizazi, Hadria, Université des Sciences et de la Technologie d’Oran Mohamed-Boudiaf, Oran, Algeria","Due to technical advancements and the proliferation of mobile applications, facial analysis (FA) of humans has recently become an important area for computer vision research. FA investigates a variety of difficulties, including gender recognition, facial expression recognition, age and race recognition, with the goal of automatically comprehending social interactions. Due to the dimensional challenge posed by pre-trained CNN networks, the scientific community has developed numerous techniques inspired by biology, swarm intelligence theory, physics, and mathematical rules. This article presents a gender recognition system based on scAOA, that is a modified version of the Archimedes optimization algorithm (AOA). The latest variant (scAOA) enhances the exploitation stage by using trigonometric operators inspired by the sine cosine algorithm (SCA) in order to prevent local optima and to accelerate the convergence. The main purpose of this paper is to apply scAOA to select the relevant deep features provided by two pretrained models of CNN (AlexNet & ResNet) to recognize the gender of a human person categorized into two classes (men and women). Two datasets are used to evaluate the proposed approach (scAOA): the Brazilian FEI dataset and the Georgia Tech Face dataset (GT). In terms of accuracy, Fscore and statistical test, the comparison analysis demonstrates that scAOA outperforms other modern and competitive optimizers such as AOA, SCA, Ant lion optimizer (ALO), Salp swarm algorithm (SSA), Grey wolf optimizer (GWO), Simple genetic algorithm (SGA), Grasshopper optimization algorithm (GOA) and Particle swarm optimizer (PSO). © 2023 Elsevier B.V., All rights reserved.","Facial analysis; Gender recognition; Pretrained CNN; Sine cosine archimedes optimization algorithm (scAOA); Trigonometric operators; Wrapper feature selection (FS)","Feature Selection; Genetic algorithms; Particle swarm optimization (PSO); Facial analysis; Features selection; Gender recognition; Optimization algorithms; Optimizers; Pretrained CNN; Sine cosine archimede optimization algorithm; Sine-cosine algorithm; Trigonometric operator; Wrapper feature selection; Face recognition","","","","Age Gender Prediction and Emotion Recognition Using Convolutional Neural Network, (2021); Peimankar, Abdolrahman, DENS-ECG: A deep learning approach for ECG signal delineation, Expert Systems with Applications, 165, (2021); Yu, Hang, Convolutional neural networks for medical image analysis: State-of-the-art, comparisons, improvement and perspectives, Neurocomputing, 444, pp. 92-110, (2021); Peker, Musa, Classification of hyperspectral imagery using a fully complex-valued wavelet neural network with deep convolutional features, Expert Systems with Applications, 173, (2021); Alhichri, Haikel Salem, Classification of Remote Sensing Images Using EfficientNet-B3 CNN Model with Attention, IEEE Access, 9, pp. 14078-14094, (2021); Hieu, Trung Huynh, Joint Age Estimation and Gender Classification of Asian Faces Using Wide ResNet, SN Computer Science, 1, 5, (2020); Savchenko, Andrey V., Efficient facial representations for age, gender and identity recognition in organizing photo albums using multi-output ConvNet, PeerJ Computer Science, 2019, 6, (2019); Samek, Wojciech, Understanding and Comparing Deep Neural Networks for Age and Gender Classification, 2018-January, pp. 1629-1638, (2017); Age and Gender Classification A Proposed System, (2019); Abirami, B., Gender and age prediction from real time facial images using CNN, Materials Today: Proceedings, 33, pp. 4708-4712, (2020)","","Springer Science and Business Media Deutschland GmbH","","","","","","14333058; 09410643","","","","English","Article","Final","All Open Access; Bronze Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85139934395"
"Piazzalunga, C.; Molino, P.; Giangregorio, C.; Fontolan, S.; Termine, C.; Ferrante, S.","Piazzalunga, Chiara (57364813100); Molino, Pierpaolo (58882671600); Giangregorio, Chiara (58682984900); Fontolan, Stefania (57190278320); Termine, Cristiano (55941156800); Ferrante, Simona (23003986400)","57364813100; 58882671600; 58682984900; 57190278320; 55941156800; 23003986400","Development and Validation of an iPad-based Serious Game for Emotion Recognition and Attention Tracking towards Early Identification of Autism","2023","","","","","","","0","1","10.1109/ACIIW59127.2023.10388145","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184820650&doi=10.1109%2FACIIW59127.2023.10388145&partnerID=40&md5=b503ec6a342de2463232fb98a0c758cc","Politecnico di Milano, Milan, Italy; Department of Medicine and Surgery, Università degli Studi dell'Insubria, Varese, Italy","Piazzalunga, Chiara, Politecnico di Milano, Milan, Italy; Molino, Pierpaolo, Politecnico di Milano, Milan, Italy; Giangregorio, Chiara, Politecnico di Milano, Milan, Italy; Fontolan, Stefania, Department of Medicine and Surgery, Università degli Studi dell'Insubria, Varese, Italy; Termine, Cristiano, Department of Medicine and Surgery, Università degli Studi dell'Insubria, Varese, Italy; Ferrante, Simona, Politecnico di Milano, Milan, Italy","The diagnosis of Autism Spectrum Disorder (ASD) and Attention Deficit Hyperactivity Disorder (ADHD) can be challenging due to limited accessibility and subjective assessments. Autistic individuals often present difficulties in emotional regulation, emotion recognition and imitation, and in maintaining focus. Emotional expressions and attention are thus hallmarks of ASD and ADHD and can be analyzed to identify these conditions. In this study, we developed and validated a serious game that integrates emotion recognition and attention tracking as a novel tool for identification of ASD and ADHD. Leveraging the TrueDepth camera capabilities, our game provides a cost-effective and user-friendly alternative to current face-tracking technologies. We compared the accuracy of emotion recognition using Euclidean distance with calibrated reference expressions and a calibration-free system based on a machine learning model using Random Forest. We also identified children at risk of ADHD using the Bells test and constructed a machine learning model, utilizing Support Vector Machine and Leave-One-Out Cross Validation, trained on attention data and game data to predict this risk. Our game was tested on 20 adults to validate the emotion recognition system, and then on 17 children of the primary school to assess usability and test the constructed models. The emotion recognition system achieved an accuracy of 0.78 for adults and 0.45 for children, while the machine learning model predicted seven emotions in children with an accuracy of 0.50, suggesting the potential for eliminating the need for calibration. The model also obtained good results in predicting valence and arousal values. The attention model showed excellent validation scores (accuracy: 0.94), indicating the possibility of extending it to a larger cohort. The System Usability Score was excellent (85.0), and children found the game enjoyable, making it a promising tool for ASD and ADHD identification. © 2024 Elsevier B.V., All rights reserved.","ADHD; attention; autism; emotion; machine learning; serious game","Calibration; Cost effectiveness; Emotion Recognition; Forestry; Learning systems; Serious games; Software design; Speech recognition; Statistical methods; Support vector machines; Attention; Attention deficit hyperactivity disorder; Autism; Autism spectrum disorders; Emotion; Emotion recognition; Machine learning models; Machine-learning; Recognition systems; Subjective assessments; Diseases","","","This work is funded by European H2020 project ESSENCE (SC1-PHE-CORONAVIRUS-2020-2B; G.A. 101016112).","Morris‑Rosendahl, Deborah, Neurodevelopmental disorders-the history and future of a diagnostic concept, Dialogues in Clinical Neuroscience, 22, 1, pp. 65-72, (2020); Bryson, Susan E., The early detection of autism in clinical practice, Paediatrics and Child Health (Canada), 9, 4, pp. 219-221, (2004); Hus, Yvette, Challenges surrounding the diagnosis of autism in children, Neuropsychiatric Disease and Treatment, 17, pp. 3509-3529, (2021); Malik-Soni, Natasha, Tackling healthcare access barriers for individuals with autism from diagnosis to adulthood, Pediatric Research, 91, 5, pp. 1028-1035, (2022); Lai, Meng Chuan, A behavioral comparison of male and female adults with high functioning autism spectrum conditions, PLOS ONE, 6, 6, (2011); Loomes, Rachel, What Is the Male-to-Female Ratio in Autism Spectrum Disorder? A Systematic Review and Meta-Analysis, Journal of the American Academy of Child and Adolescent Psychiatry, 56, 6, pp. 466-474, (2017); Ginsberg, Ylva, Underdiagnosis of attention-deficit/hyperactivity disorder in adult patients: A review of the literature, Primary Care Companion to the Journal of Clinical Psychiatry, 16, 3, (2014); Austerman, Joseph, ADHD and behavioral disorders: Assessment, management, and an update from DSM-5, Cleveland Clinic Journal of Medicine, 82, pp. 2-7, (2015); Psychiatry, (2022); An Approach to Diagnose Cognitive Deficits Gamifying Adhd Children Diagnosis Questionnaire, (2018)","","Institute of Electrical and Electronics Engineers Inc.","","11th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos, ACIIW 2023","","Cambridge; MA","196762","","9798350327458","","","English","Conference paper","Final","All Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85184820650"
"Aghakhani, S.; Carre, N.; Mostovoy, K.; Shafer, R.; Baeza-Hernandez, K.; Entenberg, G.; Testerman, A.; Bunge, E.L.","Aghakhani, Shirin (57226024692); Carre, Nicole (58082121400); Mostovoy, Karin (58081947600); Shafer, Rebecca (57278926200); Baeza-Hernandez, Katerina (57226011793); Entenberg, Guido A. (57211920375); Testerman, Alanna (57226006381); Bunge, Eduardo Liniers (35078041400)","57226024692; 58082121400; 58081947600; 57278926200; 57226011793; 57211920375; 57226006381; 35078041400","Qualitative analysis of mental health conversational agents messages about autism spectrum disorder: a call for action","2023","Frontiers in Digital Health","5","","1251016","","","0","5","10.3389/fdgth.2023.1251016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180131501&doi=10.3389%2Ffdgth.2023.1251016&partnerID=40&md5=f34b325ab4bf86113726cf5b79976bea","Department of Psychology, Pacific Graduate School of Psychology, Palo Alto, United States; Fundación ETCI, Buenos Aires, Argentina","Aghakhani, Shirin, Department of Psychology, Pacific Graduate School of Psychology, Palo Alto, United States; Carre, Nicole, Department of Psychology, Pacific Graduate School of Psychology, Palo Alto, United States; Mostovoy, Karin, Department of Psychology, Pacific Graduate School of Psychology, Palo Alto, United States; Shafer, Rebecca, Department of Psychology, Pacific Graduate School of Psychology, Palo Alto, United States; Baeza-Hernandez, Katerina, Department of Psychology, Pacific Graduate School of Psychology, Palo Alto, United States; Entenberg, Guido A., Fundación ETCI, Buenos Aires, Argentina; Testerman, Alanna, Department of Psychology, Pacific Graduate School of Psychology, Palo Alto, United States; Bunge, Eduardo Liniers, Department of Psychology, Pacific Graduate School of Psychology, Palo Alto, United States","Background: Conversational agents (CA's) have shown promise in increasing accessibility to mental health resources. This study aimed to identify common themes of messages sent to a mental health CA (Wysa) related to ASD by general users and users that identify as having ASD. Methods: This study utilized retrospective data. Two thematic analyses were conducted, one focusing on user messages including the keywords (e.g., ASD, autism, Asperger), and the second one with messages from users who self-identified as having ASD. Results: For the sample of general users, the most frequent themes were “others having ASD,” “ASD diagnosis,” and “seeking help.” For the users that self-identified as having ASD (n = 277), the most frequent themes were “ASD diagnosis or symptoms,” “negative reaction from others,” and “positive comments.” There were 3,725 emotion words mentioned by users who self-identified as having ASD. The majority had negative valence (80.3%), and few were positive (14.8%) or ambivalent (4.9%). Conclusion: Users shared their experiences and emotions surrounding ASD with a mental health CA. Users asked about the ASD diagnosis, sought help, and reported negative reactions from others. CA's have the potential to become a source of support for those interested in ASD and/or identify as having ASD. © 2023 Elsevier B.V., All rights reserved.","autism spectrum disorder; chatbots; digital interventions; digital mental health; technology","adolescent; adult; anxiety; Article; artificial intelligence; autism; consensus; data analysis; data extraction; depression; emotion; experience; female; human; major clinical study; male; mental health; physiological stress; qualitative analysis; retrospective study; self esteem; sleep; thematic analysis; young adult","","","The authors would like to acknowledge Chaitali Sinha, Madhavi Roy, and Madhura Kadaba from Wysa for their patience, advice, and support throughout the process of the manuscript.","Maenner, Matthew J., Prevalence of autism spectrum disorder among children aged 8 Years-Autism and developmental disabilities monitoring network, 11 Sites, United States, 2016, MMWR Surveillance Summaries, 69, 4, pp. 1-12, (2020); Salari, Nader, The global prevalence of autism spectrum disorder: a comprehensive systematic review and meta-analysis, Italian Journal of Pediatrics, 48, 1, (2022); Grzadzinski, Rebecca L., DSM-5 and autism spectrum disorders (ASDs): An opportunity for identifying ASD subtypes, Molecular Autism, 4, 1, (2013); Lord, Catherine E., Recent advances in autism research as reflected in DSM-5 criteria for autism spectrum disorder, Annual Review of Clinical Psychology, 11, pp. 53-70, (2015); Camm-Crosbie, Louise, ‘People like me don’t get support’: Autistic adults’ experiences of support and treatment for mental health difficulties, self-injury and suicidality, Autism, 23, 6, pp. 1431-1441, (2019); Kanne, Stephen M., Editorial Perspective: The autism waitlist crisis and remembering what families need, Journal of Child Psychology and Psychiatry and Allied Disciplines, 62, 2, pp. 140-142, (2021); Buescher, Ariane V.S., Costs of autism spectrum disorders in the United Kingdom and the United States, JAMA Pediatrics, 168, 8, pp. 721-728, (2014); Lipinski, Silke, A blind spot in mental healthcare? Psychotherapists lack education and expertise for the support of adults on the autism spectrum, Autism, 26, 6, pp. 1509-1521, (2022); Khanlou, Nazilla, Digital Literacy, Access to Technology and Inclusion for Young Adults with Developmental Disabilities, Journal of Developmental and Physical Disabilities, 33, 1, (2021); Valencia, Katherine, The impact of technology on people with autism spectrum disorder: A systematic literature review, Sensors, 19, 20, (2019)","","Frontiers Media SA","","","","","","2673253X","","","","English","Article","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85180131501"
"Saisanthiya, D.; Supraja, P.","Saisanthiya, D. (58360202800); Supraja, P. (57191530051)","58360202800; 57191530051","Heterogeneous Convolutional Neural Networks for Emotion Recognition Combined with Multimodal Factorised Bilinear Pooling and Mobile Application Recommendation","2023","International Journal of Interactive Mobile Technologies","17","16","","129","142","0","4","10.3991/ijim.v17i16.42735","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170413311&doi=10.3991%2Fijim.v17i16.42735&partnerID=40&md5=7176101fb0e29e590a445c588ef28a4d","Department of Networking and Communications, SRM Institute of Science and Technology, Kattankulathur, India; SRM Institute of Science and Technology, Kattankulathur, India","Saisanthiya, D., Department of Networking and Communications, SRM Institute of Science and Technology, Kattankulathur, India; Supraja, P., SRM Institute of Science and Technology, Kattankulathur, India","The field of emotion recognition has garnered considerable interest due to its diverse applications in mental health, personalised advertising and enhancing user experiences. This research paper introduces a unique and innovative method for emotion recognition by integrating heterogeneous convolutional neural networks (CNNs) with multimodal factorised bilinear pooling. Furthermore, the paper also incorporates the integration of mobile application recommendations as part of the overall approach. The proposed method leverages the power of CNNs to extract high-level features from different modalities, including facial expressions, speech signals and physiological signals. By using heterogeneous CNNs, each modality is processed independently to capture modality-specific emotional cues effectively. To fuse the extracted features, multimodal factorised bilinear pooling is employed, which captures the complex interactions between different modalities while reducing the computational complexity. This pooling technique efficiently combines the modality-specific features, resulting in a compact and discriminative representation of the emotional state. In addition to emotion recognition, this paper also introduces the integration of mobile app recommendations. By leveraging the recognised emotion, the system recommends relevant mobile applications that are tailored to the user’s emotional state. This integration enhances user experience and facilitates emotion regulation through the utilisation of appropriate mobile apps. Experimental evaluations are conducted on benchmark emotion recognition datasets, including the DEAP and MAHNOB_HCI datasets. The findings of the study highlight the effectiveness of the proposed methodology in terms of accuracy and robustness, surpassing existing approaches in the field. Additionally, the integration of the mobile app recommendation system showcases encouraging outcomes by offering personalised recommendations tailored to the user’s emotional state. © 2023 Elsevier B.V., All rights reserved.","bilinear pooling; heterogeneous CNN; mobile application; multimodal data; recommendation system","","","","Networking and Communications in SRM Institute of Science and Technology Kattankulathur, India. She is a recipient of AICTE Visvesvaraya Best Teacher Award 2020. She completed Indo-US WISTEMM Research fellowship at University of Southern California, Los Angeles, USA, funded by IUSSTF and DST Govt., of India. She served as a Post-Doctoral Research Associate at Northumbria University, Newcastle, UK and completed her Ph.D. from Anna University in 2017. She has published more than 50 research papers in reputed national and international level journals/ conferences. She received university-level Best Research Paper Award in 2019 and 2022. Also, she has received funding from AICTE for conducting STTP. Her research interests include Cognitive Computing, Optimization algorithms, Machine learning, Deep Learning, Wireless Communication, and IoT. She is a reviewer in IEEE, Interscience, Elsevier and Springer journals. She is also a member of several national and international professional bodies including IEEE, ACM, ISTE, etc. In addition, she has received the young women in Engineering award and Distinguished Young Researcher award from various international organizations (E-mail: suprajap@ srmist.edu.in).","Computer Vision and Image Understanding, (2020); Eyben, Florian, Recent developments in openSMILE, the munich open-source multimedia feature extractor, pp. 835-838, (2013); IEEE Transactions on Affective Computing, (2019); Lin, Tsungyu, Bilinear CNN models for fine-grained visual recognition, Proceedings of the IEEE International Conference on Computer Vision, 2015 International Conference on Computer Vision, ICCV 2015, pp. 1449-1457, (2015); Karpathy, Andrej, Deep visual-semantic alignments for generating image descriptions, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 07-12-June-2015, pp. 3128-3137, (2015); Ng, Joe Yue Hei, Beyond short snippets: Deep networks for video classification, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 07-12-June-2015, pp. 4694-4702, (2015); Litjens, Geert J.S., A survey on deep learning in medical image analysis, Medical Image Analysis, 42, pp. 60-88, (2017); Nemati, Shahla, A Hybrid Latent Space Data Fusion Method for Multimodal Emotion Recognition, IEEE Access, 7, pp. 172948-172964, (2019); Huang, Haiping, Multimodal Emotion Recognition Based on Ensemble Convolutional Neural Network, IEEE Access, 8, pp. 3265-3271, (2020); Zhang, Yong, Multimodal Emotion Recognition Using a Hierarchical Fusion Convolutional Neural Network, IEEE Access, 9, pp. 7943-7951, (2021)","","International Association of Online Engineering","","","","","","18657923","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85170413311"
"Haoues, M.; Mokni, R.","Haoues, Mariem (55916585600); Mokni, Raouia (56582114700)","55916585600; 56582114700","Toward an autism-friendly environment based on mobile apps user feedback analysis using deep learning and machine learning models","2023","PeerJ Computer Science","9","","e1442","","","0","6","10.7717/peerj-cs.1442","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170252841&doi=10.7717%2Fpeerj-cs.1442&partnerID=40&md5=b9c9cc84072d87d604fe96584d824cbf","Department of Software Engineering, Prince Sattam Bin Abdulaziz University, Al Kharj, Saudi Arabia; University of Carthage, Faculté des Sciences de Bizerte, Bizerte, Tunisia; Department of Information Systems, Prince Sattam Bin Abdulaziz University, Al Kharj, Saudi Arabia; Higher Institute of Management of Gabès, Université de Gabès, Gabes, Tunisia","Haoues, Mariem, Department of Software Engineering, Prince Sattam Bin Abdulaziz University, Al Kharj, Saudi Arabia, University of Carthage, Faculté des Sciences de Bizerte, Bizerte, Tunisia; Mokni, Raouia, Department of Information Systems, Prince Sattam Bin Abdulaziz University, Al Kharj, Saudi Arabia, Higher Institute of Management of Gabès, Université de Gabès, Gabes, Tunisia","Autistic people are often disadvantaged in employment, education, etc. In fact, autistic students/employees face several challenges navigating and communicating with their superiors and colleagues. Mobile applications for people with Autism Spectrum Disorder (ASD apps for short) have been increasingly being adapted to help autistic people manage their conditions and daily activities. User feedback analysis is an effective method that can be used to improve ASD apps’ services. In this article, we investigate the usage of ASD apps to improve the quality of life for autistic students/employees based on user feedback analysis. For this purpose, we analyze user reviews suggested on highly ranked ASD apps for college students, and workers. A total of 97,051 reviews have been collected from 13 ASD apps available on Google Play and Apple App stores. The collected reviews have been classified into negative, positive, and neutral opinions. This analysis has been performed using machine learning and deep learning models. The best performances were provided by combining RNN and LSTM models with an accuracy of 96.58% and an AUC of 99.41%. Finally, we provide some recommendations to improve ASD apps to assist developers in upgrading the main services provided by their apps. © 2023 Elsevier B.V., All rights reserved.","Autism spectrum disorder; Autism-friendly environment; Deep learning models; LSTM; Mobile apps; RNN; Sentiment analysis; User feedback","Diseases; Learning systems; Long short-term memory; Quality control; Students; Autism spectrum disorders; Autism-friendly environment; Deep learning model; Feedback analysis; Learning models; LSTM; Mobile app; RNN; Sentiment analysis; User feedback","","","The authors received funding for this research from the Deputyship for Research & Innovation, Ministry of Education in Saudi Arabia through the project number (IF2/ PSAU/2022/01/21674). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.","Aslam, Naila, Convolutional neural network based classification of app reviews, IEEE Access, 8, pp. 185619-185628, (2020); Bury, Simon M., Workplace Social Challenges Experienced by Employees on the Autism Spectrum: An International Exploratory Study Examining Employee and Supervisor Perspectives, Journal of Autism and Developmental Disorders, 51, 5, pp. 1614-1627, (2021); Camacho-Rivera, Marlene, Evaluating asthma mobile apps to improve asthma self-management: User ratings and sentiment analysis of publicly available apps, JMIR mHealth and uHealth, 8, 10, (2020); Workplace Challenges for Individuals with Autism, (2020); Diagnostic and Statistical Manual of Mental Disorders, (2014); de Lima, Vitor Mesaque Alves, Temporal dynamics of requirements engineering from mobile app reviews, PeerJ Computer Science, 8, (2022); Gallardo-Montes, Carmen Del Pilar, Technologies in the education of children and teenagers with autism: evaluation and classification of apps by work areas, Education and Information Technologies, 27, 3, pp. 4087-4115, (2022); Gao, Cuiyun, Emerging App Issue Identification from User Feedback: Experience on WeChat, pp. 279-288, (2019); Gurbuz, Emine, University Students with Autism: The Social and Academic Experiences of University in the UK, Journal of Autism and Developmental Disorders, 49, 2, pp. 617-631, (2019); Hogan, Abigail L., Screening and diagnosis of autism spectrum disorder in preschool-aged children, pp. 323-345, (2020)","","PeerJ Inc.","","","","","","23765992","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85170252841"
"Thabet, Z.; Ansari, H.; Albashtawi, S.; Siyam, N.; Abdallah, S.","Thabet, Zeina (57848419100); Ansari, Hurmat (58298356600); Albashtawi, Sara (58298730300); Siyam, Nur (60007580900); Abdallah, Sherief (8318330700)","57848419100; 58298356600; 58298730300; 60007580900; 8318330700","Using Deep Learning and Native Mobile App to Assist Autistic Students' Educational Experience","2023","International Conference on Computer Supported Education, CSEDU - Proceedings","2","","","96","103","0","1","10.5220/0011748800003470","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160867305&doi=10.5220%2F0011748800003470&partnerID=40&md5=f87f183f715a548f2e0aa3acb13d3a4e","Faculty of Engineering and IT, British University in Dubai, Dubai, United Arab Emirates","Thabet, Zeina, Faculty of Engineering and IT, British University in Dubai, Dubai, United Arab Emirates; Ansari, Hurmat, Faculty of Engineering and IT, British University in Dubai, Dubai, United Arab Emirates; Albashtawi, Sara, Faculty of Engineering and IT, British University in Dubai, Dubai, United Arab Emirates; Siyam, Nur, Faculty of Engineering and IT, British University in Dubai, Dubai, United Arab Emirates; Abdallah, Sherief, Faculty of Engineering and IT, British University in Dubai, Dubai, United Arab Emirates","Apart from difficulties with social communication, children with autism spectrum disorder (ASD) tend to have limited interest in academic activities. The challenges faced by the educators of these students are abundant, including selecting motivating items or activities that can prompt them to complete a task. In addition to this challenge, the educators also face the issue of the lack of coordination between the teachers, therapists, and parents. This issue is imperative as significant learning opportunities are lost for lack of communication. To address these two issues, we have created a distributed system consisting of a mobile application that tracks the academic objectives and behavioural progress of the students which allows for a centralized place of information for easier coordination between educators, as well as suggesting effective motivators using a Deep Neural Network (DNN), specifically a Deep Q Network, to help autistic students regain their focus in the class. The Deep Q Network is constructed with a custom environment that takes in the state as input and then, based on the current state, calculates the best motivator to suggest. The mobile application was created with an aim of assisting school educators in tracking a student’s progress. Moreover, the system includes a staff dashboard to manage users and provide visualizations depicting students’ progress. This project is the first of its kind and will help educators select effective motivators in moments that the students need them as well as aid the flow of information between the stakeholders. © 2023 Elsevier B.V., All rights reserved.","Autism; Behaviour Intervention; Deep Reinforcement Learning; Mobile Application; Special Education","Deep neural networks; Diseases; E-learning; Mobile computing; Reinforcement learning; Autism; Behavior intervention; Children with autisms; Deep reinforcement learning; Educational experiences; Mobile app; Mobile applications; Reinforcement learnings; Social communications; Special education; Students","","","","Alkashri, Zainab, A detailed survey of Artificial Intelligence and Spftware Engineering: Emergent Issues, pp. 666-672, (2020); Bellman, Richard E., Dynamic programming, Science, 153, 3731, pp. 34-37, (1966); Data Statistics on Autism Spectrum Disorder, (2020); Chasson, Gregory S., Cost comparison of early intensive behavioral intervention and special education for children with autism, Journal of Child and Family Studies, 16, 3, pp. 401-413, (2007); Dawson, Geraldine, Early behavioral intervention is associated with normalized brain activity in young children with autism, Journal of the American Academy of Child and Adolescent Psychiatry, 51, 11, pp. 1150-1159, (2012); Durstewitz, Daniel, Deep neural networks in psychiatry, Molecular Psychiatry, 24, 11, pp. 1583-1598, (2019); Youssef, Fenjiro, Deep reinforcement learning overview of the state of the art, Journal of Automation, Mobile Robotics and Intelligent Systems, 12, 3, pp. 20-39, (2018); Koegel, Lynn Kern, Improving motivation for academics in children with autism, Journal of Autism and Developmental Disorders, 40, 9, pp. 1057-1066, (2010); Kosmicki, Jack A., Searching for a minimal set of behaviors for autism detection through feature selection-based machine learning, Translational Psychiatry, 5, 2, (2015); Lazaridis, Aristotelis, Deep reinforcement learning: A state-of-the-art walkthrough, Journal of Artificial Intelligence Research, 69, pp. 1421-1471, (2021)","Jovanovic, J.; Chounta, I.-A.; Uhomoibhi, J.; McLaren, B.","Science and Technology Publications, Lda","Institute for Systems and Technologies of Information, Control and Communication (INSTICC)","15th International Conference on Computer Supported Education, CSEDU 2023","","Prague","188800","21845026","9789897585623; 9789897586972; 9789897587467; 9789897586415; 9789897585029","","","English","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85160867305"
"Mai, N.-D.; Nguyen, H.-T.; Chung, W.-Y.","Mai, Ngoc Dau (57222326137); Nguyen, Ha Trung (57552473100); Chung, Wan-young (7401982175)","57222326137; 57552473100; 7401982175","Real-Time On-Chip Machine-Learning-Based Wearable Behind-The-Ear Electroencephalogram Device for Emotion Recognition","2023","IEEE Access","11","","","47258","47271","0","17","10.1109/ACCESS.2023.3276244","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160221555&doi=10.1109%2FACCESS.2023.3276244&partnerID=40&md5=3049dde8143a2a124b0dbe42d39b90e5","Pukyong National University, Busan, South Korea","Mai, Ngoc Dau, Pukyong National University, Busan, South Korea; Nguyen, Ha Trung, Pukyong National University, Busan, South Korea; Chung, Wan-young, Pukyong National University, Busan, South Korea","In this study, we propose an end-to-end emotion recognition system using an ear-electroencephalogram (EEG)-based on-chip device that is enabled using the machine-learning model. The system has an integrated device that gathers EEG signals from electrodes positioned behind the ear; it is more practical than the conventional scalp-EEG method. The relative power spectral density (PSD), which is the feature used in this study, is derived using the fast Fourier transform over five frequency bands. Directly on the embedded device, data preprocessing and feature extraction were carried out. Three standard machine learning models, namely, support vector machine (SVM), multilayer perceptron (MLP), and one-dimensional convolutional neural network (1D-CNN), were trained on these rich emotion classification features. The traditional approach, which integrates a model into the application software on a personal computer (PC), is cumbersome and lacks mobility, which makes it challenging to use in real-life applications. Besides, the PC-based system is not sufficiently real-time because of the connection latency from the EEG data acquisition device. To overcome these limitations, we propose a wearable device capable of performing on-chip machine learning and signal processing on the EEG data immediately after the acquisition task for the real-time result. In order to perform on-chip machine learning for the real-time prediction of emotions, 1D-CNN was chosen as a pre-trained model using the relative PSD characteristics as input based on the evaluation of the set results. Additionally, we developed a smartphone application that alerted the user whenever a negative emotional state was identified and displayed the information in real life. Our test results demonstrated the feasibility and practicability of our embedded system for real-time emotion recognition. © 2023 Elsevier B.V., All rights reserved.","Electroencephalogram (EEG); emotion recognition; multilayer perceptron (MLP); one-dimensional convolutional neural network (1D-CNN); power spectral density (PSD); real-time EEG system; support vector machine (SVM); tiny machine learning","Application programs; Biomedical signal processing; Convolution; Data acquisition; Electrodes; Electroencephalography; Electrophysiology; Fast Fourier transforms; Interactive computer systems; Learning systems; Multilayers; Personal computers; Power spectral density; Real time systems; Speech recognition; Support vector machines; Brain modeling; Convolutional neural network; Electroencephalogram; Emotion recognition; Machine-learning; Multilayer perceptron; Multilayers perceptrons; One-dimensional; One-dimensional convolutional neural network (1d-CNN); Real - Time system; Real- time; Real-time electroencephalogram system; Support vector machine; Support vectors machine; Tiny machine learning; Emotion Recognition","","","This work was supported by the National Research Foundation of Korea (NRF) Grant funded by the Korea Government through Ministry of Science and ICT (MSIT) under Grant NRF-2019R1A2C1089139.","Etkin, Amit, The neural bases of emotion regulation, Nature Reviews Neuroscience, 16, 11, pp. 693-700, (2015); Slovic, Paul, Risk as Analysis and Risk as Feelings: Some Thoughts about Affect, Reason, Risk, and Rationality, Risk Analysis, 24, 2, pp. 311-322, (2004); Handbook of Face Recognition, (2005); Haag, Andreas, Emotion recognition using bio-sensors: First steps towards an automatic system, Lecture Notes in Computer Science, 3068, pp. 36-48, (2004); Kumari, Preeti, Increasing trend of wearables and multimodal interface for human activity monitoring: A review, Biosensors and Bioelectronics, 90, pp. 298-307, (2017); Petrantonakis, Panagiotis C., Emotion recognition from EEG using higher order crossings, IEEE Transactions on Information Technology in Biomedicine, 14, 2, pp. 186-197, (2010); Pivik, Rudolph Terry, Guidelines for the recording and quantitative analysis of electroencephalographic activity in research contexts, Psychophysiology, 30, 6, pp. 547-558, (1993); Mikkelsen, Kaare B., EEG recorded from the ear: Characterizing the Ear-EEG Method, Frontiers in Neuroscience, 9, NOV, (2015); Norton, James J.S., Soft, curved electrode systems capable of integration on the auricle as a persistent brain-computer interface, Proceedings of the National Academy of Sciences of the United States of America, 112, 13, pp. 3920-3925, (2015); Gu, Ying, Comparison between scalp EEG and behind-the-ear EEG for development of a wearable seizure detection system for patients with focal epilepsy, Sensors, 18, 1, (2018)","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21693536","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85160221555"
"Alam, H.; Burhan, M.; Gillani, A.; Haq, I.U.; Arshed, M.A.; Shafi, M.; Ahmed, S.","Alam, Hina (57557473500); Burhan, Muhammad (57188740220); Gillani, Anusha (58220846700); Haq, Ihtisham Ul (58709712400); Arshed, Muhammad Asad (36839969700); Shafi, Muhammad (55029162200); Ahmed, Saeed (57213511614)","57557473500; 57188740220; 58220846700; 58709712400; 36839969700; 55029162200; 57213511614","IoT Based Smart Baby Monitoring System with Emotion Recognition Using Machine Learning","2023","Wireless Communications and Mobile Computing","2023","","1175450","","","0","26","10.1155/2023/1175450","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85156180566&doi=10.1155%2F2023%2F1175450&partnerID=40&md5=7775488560f1959b976d8e885dd30d75","School of Systems and Technology, University of Management and Technology Lahore, Lahore, Pakistan; Department of Information Technology, University of the Punjab, Lahore, Pakistan; Office of Research, University of Management and Technology Lahore, Lahore, Pakistan; Faculty of Computing and Information Technology, Sohar University, Sohar, Oman","Alam, Hina, School of Systems and Technology, University of Management and Technology Lahore, Lahore, Pakistan; Burhan, Muhammad, Department of Information Technology, University of the Punjab, Lahore, Pakistan; Gillani, Anusha, Office of Research, University of Management and Technology Lahore, Lahore, Pakistan; Haq, Ihtisham Ul, School of Systems and Technology, University of Management and Technology Lahore, Lahore, Pakistan; Arshed, Muhammad Asad, School of Systems and Technology, University of Management and Technology Lahore, Lahore, Pakistan; Shafi, Muhammad, Faculty of Computing and Information Technology, Sohar University, Sohar, Oman; Ahmed, Saeed, School of Systems and Technology, University of Management and Technology Lahore, Lahore, Pakistan","Child care is necessary for parents, but nowadays taking care of a child has become a lot more challenging, especially for working mothers. It has become increasingly difficult for parents to continuously monitor their baby's condition. Thus, a smart baby monitoring system based on IoT and machine learning is implemented to overcome the monitoring issues and provide intimation to parents in real-time. In the proposed system, the necessary monitoring features like room temperature and humidity, cry detection, and face detection were monitored by exploiting different sensors. The sensor data is transferred to the Blynk server via controllers with an Internet connection. The system is also capable of detecting the facial emotions of the registered babies by using a machine learning model. Parents can monitor the live activities and emotions of their child through the external web camera and can swing the baby cradle remotely upon cry detection using their mobile application. They can also check the real-time room temperature and humidity level. In case an abnormal action is detected, a notification is sent to the parent's mobile application to take action thus, making the baby monitoring system a relief for all working parents to manage their time efficiently while taking care of their babies simultaneously. © 2023 Elsevier B.V., All rights reserved.","","Emotion Recognition; Face recognition; Internet of things; Mobile computing; Room temperature; Child care; Condition; Emotion recognition; Faces detection; Machine-learning; Mobile applications; Monitoring system; Real- time; Sensors data; Temperature and humidities; Machine learning","","","","Hemalatha, P., Smart Digital Parenting Using Internet of Things, (2018); Burhan, Muhammad, IoT elements, layered architectures and security issues: A comprehensive survey, Sensors, 18, 9, (2018); Atzori, Luigi, The Internet of Things: A survey, Computer Networks, 54, 15, pp. 2787-2805, (2010); Ubaid, Muhammad Talha, COVID-19 SOP's Violations Detection in Terms of Face Mask Using Deep Learning, (2021); Arshed, Muhammad Asad, Symptoms Based Covid-19 Disease Diagnosis Using Machine Learning Approach, (2021); Arshed, Muhammad Asad, Comparison of Machine Learning Classifiers for Breast Cancer Diagnosis, (2021); Arshed, Muhammad Asad, A Light Weight Deep Learning Model for Real World Plant Identification, pp. 40-45, (2022); Int J Adv Res Comput Commun Eng, (2019); Shahadi, Haider Ismael, Design and Implementation of a Smart Baby Crib, IOP Conference Series: Materials Science and Engineering, 671, 1, (2020); Joseph, Senoj, IOT Based Baby Monitoring System Smart Cradle, pp. 748-751, (2021)","","Hindawi Limited","","","","","","15308677; 15308669","","","","English","Article","Final","All Open Access; Green Final Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85156180566"
"Banerjee, A.; Mutlu, O.C.; Kline, A.; Surabhi, S.; Washington, P.; Wall, D.P.","Banerjee, Agnik (57244392900); Mutlu, Onur Cezmi (16043006700); Kline, Aaron (57191505004); Surabhi, Saimourya (57997449100); Washington, Peter Yigitcan (57191498961); Wall, Dennis Paul (7202196193)","57244392900; 16043006700; 57191505004; 57997449100; 57191498961; 7202196193","Training and Profiling a Pediatric Facial Expression Classifier for Children on Mobile Devices: Machine Learning Study","2023","JMIR Formative Research","7","","e39917","","","0","12","10.2196/39917","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151374366&doi=10.2196%2F39917&partnerID=40&md5=30f3d9595ce3678194ba10796ca9bf66","Department of Pediatrics (Systems Medicine), Stanford University, Stanford, United States; Stanford Engineering, Stanford, United States; Department of Information and Computer Sciences, Honolulu, United States; Department of Biomedical Data Science, Stanford University, Stanford, United States; Department of Psychiatry and Behavioral Sciences, Stanford University, Stanford, United States","Banerjee, Agnik, Department of Pediatrics (Systems Medicine), Stanford University, Stanford, United States; Mutlu, Onur Cezmi, Stanford Engineering, Stanford, United States; Kline, Aaron, Department of Pediatrics (Systems Medicine), Stanford University, Stanford, United States; Surabhi, Saimourya, Department of Pediatrics (Systems Medicine), Stanford University, Stanford, United States; Washington, Peter Yigitcan, Department of Information and Computer Sciences, Honolulu, United States; Wall, Dennis Paul, Department of Pediatrics (Systems Medicine), Stanford University, Stanford, United States, Department of Biomedical Data Science, Stanford University, Stanford, United States, Department of Psychiatry and Behavioral Sciences, Stanford University, Stanford, United States","Background: Implementing automated facial expression recognition on mobile devices could provide an accessible diagnostic and therapeutic tool for those who struggle to recognize facial expressions, including children with developmental behavioral conditions such as autism. Despite recent advances in facial expression classifiers for children, existing models are too computationally expensive for smartphone use. Objective: We explored several state-of-the-art facial expression classifiers designed for mobile devices, used posttraining optimization techniques for both classification performance and efficiency on a Motorola Moto G6 phone, evaluated the importance of training our classifiers on children versus adults, and evaluated the models’ performance against different ethnic groups. Methods: We collected images from 12 public data sets and used video frames crowdsourced from the GuessWhat app to train our classifiers. All images were annotated for 7 expressions: neutral, fear, happiness, sadness, surprise, anger, and disgust. We tested 3 copies for each of 5 different convolutional neural network architectures: MobileNetV3-Small 1.0x, MobileNetV2 1.0x, EfficientNetB0, MobileNetV3-Large 1.0x, and NASNetMobile. We trained the first copy on images of children, second copy on images of adults, and third copy on all data sets. We evaluated each model against the entire Child Affective Facial Expression (CAFE) set and by ethnicity. We performed weight pruning, weight clustering, and quantize-aware training when possible and profiled each model’s performance on the Moto G6. Results: Our best model, a MobileNetV3-Large network pretrained on ImageNet, achieved 65.78% accuracy and 65.31% F<inf>1</inf>-score on the CAFE and a 90-millisecond inference latency on a Moto G6 phone when trained on all data. This accuracy is only 1.12% lower than the current state of the art for CAFE, a model with 13.91x more parameters that was unable to run on the Moto G6 due to its size, even when fully optimized. When trained solely on children, this model achieved 60.57% accuracy and 60.29% F<inf>1</inf>-score. When trained only on adults, the model received 53.36% accuracy and 53.10% F<inf>1</inf>-score. Although the MobileNetV3-Large trained on all data sets achieved nearly a 60% F<inf>1</inf>-score across all ethnicities, the data sets for South Asian and African American children achieved lower accuracy (as much as 11.56%) and F<inf>1</inf>-score (as much as 11.25%) than other groups. Conclusions: With specialized design and optimization techniques, facial expression classifiers can become lightweight enough to run on mobile devices and achieve state-of-the-art performance. There is potentially a “data shift” phenomenon between facial expressions of children compared with adults; our classifiers performed much better when trained on children. Certain underrepresented ethnic groups (e.g., South Asian and African American) also perform significantly worse than groups such as European Caucasian despite similar data quality. Our models can be integrated into mobile health therapies to help diagnose autism spectrum disorder and provide targeted therapeutic treatment to children. © 2023 Elsevier B.V., All rights reserved.","affective computing; algorithm; ASD; autism; autism spectrum disorder; child; classification; classifier; computer vision; deep learning; developmental disorder; diagnostic tool; digital therapy; edge computing; emotion recognition; image analysis; Image classification; machine learning; machine learning for health; mHealth; mobile health; model; neural network; pediatrics; smartphone","","","","The work was supported in part by funds to DPW from the National Institutes of Health (1R01EB025025-01, 1R01LM013364-01, 1R21HD091500-01, 1R01LM013083); the National Science Foundation (Award 2014232); The Hartwell Foundation; the Bill and Melinda Gates Foundation; the Coulter Foundation; the Lucile Packard Foundation; the Auxiliaries Endowment; the Islamic Development Bank Transform Fund; the Weston Havens Foundation; program grants from Stanford’s Human Centered Artificial Intelligence Program, Precision Health and Integrated Diagnostics Center, Beckman Center, Bio-X Center, Predictives and Diagnostics Accelerator, Spectrum, Spark Program in Translational Research, and MediaX; and the Wu Tsai Neurosciences Institute's Neuroscience:Translate Program. We also acknowledge generous support from David Orr, Imma Calvo, Bobby Dekesyer, and Peter Sullivan. PW would like to acknowledge support from Mr. Schroeder and the Stanford Interdisciplinary Graduate Fellowship (SIGF) as the Schroeder Family Goldman Sachs Graduate Fellow.","Maenner, Matthew J., Prevalence and Characteristics of Autism Spectrum Disorder Among Children Aged 8 Years — Autism and Developmental Disabilities Monitoring Network, 11 Sites, United States, 2018, MMWR Surveillance Summaries, 70, 11, pp. 1-16, (2021); Zeidan, Jinan, Global prevalence of autism: A systematic review update, Autism Research, 15, 5, pp. 778-790, (2022); Dawson, Geraldine, Early behavioral intervention, brain plasticity, and the prevention of autism spectrum disorder, Development and Psychopathology, 20, 3, pp. 775-803, (2008); Landa, Rebecca J., Social and communication development in toddlers with early and later diagnosis of autism spectrum disorders, Archives of General Psychiatry, 64, 7, pp. 853-864, (2007); Battle, Dolores Elaine, Diagnostic and Statistical Manual of Mental Disorders (DSM)., CODAS, 25, 2, pp. 191-192, (2013); Kogan, Michael D., The prevalence of parent-reported autism spectrum disorder among US children, Pediatrics, 142, 6, (2018); Mazurek, Micah Osborne, Age at first autism spectrum disorder diagnosis: The role of birth cohort, demographic factors, and clinical features, Journal of Developmental and Behavioral Pediatrics, 35, 9, pp. 561-569, (2014); Siklos, Susan, Assessing the diagnostic experiences of a small sample of parents of children with autism spectrum disorders, Research in Developmental Disabilities, 28, 1, pp. 9-22, (2007); Lord, Catherine E., Autism Diagnostic Interview-Revised: A revised version of a diagnostic interview for caregivers of individuals with possible pervasive developmental disorders, Journal of Autism and Developmental Disorders, 24, 5, pp. 659-685, (1994); Leblanc, Emilie, Feature replacement methods enable reliable home video analysis for machine learning detection of autism, Scientific Reports, 10, 1, (2020)","","JMIR Publications Inc.","","","","","","2561326X","","","","English","Article","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85151374366"
"Avenoğlu, B.; Koeman, V.J.; Hindriks, K.V.","Avenoğlu, Bilgin (55806652500); Koeman, Vincent J. (56566677500); Hindriks, Koen V. (55904124900)","55806652500; 56566677500; 55904124900","A cloud-based middleware for multi-modal interaction services and applications","2022","Journal of Ambient Intelligence and Smart Environments","14","6","","455","481","0","2","10.3233/AIS-220161","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145659785&doi=10.3233%2FAIS-220161&partnerID=40&md5=287a0a397f78d14198208e74ae9fa29d","Faculty of Engineering, Ankara Üniversitesi, Ankara, Turkey; Faculty of Science, Vrije Universiteit Amsterdam, Amsterdam, Netherlands","Avenoğlu, Bilgin, Faculty of Engineering, Ankara Üniversitesi, Ankara, Turkey; Koeman, Vincent J., Faculty of Science, Vrije Universiteit Amsterdam, Amsterdam, Netherlands; Hindriks, Koen V., Faculty of Science, Vrije Universiteit Amsterdam, Amsterdam, Netherlands","Smart devices, such as smart phones, voice assistants and social robots, provide users with a range of input modalities, e.g., speech, touch, gestures, and vision. In recent years, advancements in processing of these input channels enable more natural interaction (e.g., automated speech, face, and gesture recognition, dialog generation, emotion expression etc.) experiences for users. However, there are several important challenges that need to be addressed to create these user experiences. One challenge is that most smart devices do not have sufficient computing resources to execute the Artificial Intelligence (AI) techniques locally. Another challenge is that users expect responses in near real-time when they interact with these devices. Moreover, users also want to be able to seamlessly switch between devices and services any time and from anywhere and expect personalized and privacy-aware services. To address these challenges, we design and develop a cloud-based middleware (CMI) which helps to develop multi-modal interaction applications and easily integrate applications to AI services. In this middleware, services developed by different producers with different protocols and smart devices with different capabilities and protocols can be integrated easily. In CMI, applications stream data from devices to cloud services for processing and consume the results. It supports data streaming from multiple devices to multiple services (and vice versa). CMI provides an integration framework for decoupling the services and devices and enabling application developers to concentrate on 'interaction' instead of AI techniques. We provide simple examples to illustrate the conceptual ideas incorporated in CMI. © 2023 Elsevier B.V., All rights reserved.","cloud computing; integration framework; multi-modal interaction; smart devices; Software architectures for AI","Application programs; Emotion Recognition; Smartphones; Speech recognition; Artificial intelligence techniques; Assistant robot; Cloud-based; Cloud-computing; Integration frameworks; Multimodal Interaction; Services and applications; Smart devices; Smart phones; Software architecture for artificial intelligence; Middleware","","","Bilgin Avenoğlu is supported by The Scientific and Technological Research Council of Turkey (TUBITAK) under the name of 2219 – International Postdoctoral Research Fellowship Program for Turkish Citizens.","Aazam, Mohammad, Offloading in fog computing for IoT: Review, enabling technologies, and research opportunities, Future Generation Computer Systems, 87, pp. 278-289, (2018); Almeida, Nuno, The am4i architecture and framework formultimodal interaction and its application to smart environments, Sensors, 19, 11, (2019); Andrist, Sean, Demonstrating a Framework for Rapid Development of Physically Situated Interactive Systems, ACM/IEEE International Conference on Human-Robot Interaction, 2019-March, (2019); Emotion Recognition Python Source Code; International Journal of Systems and Service Oriented Engineering, (2015); Bohus, Dan, Rapid development of multimodal interactive systems: A demonstration of platform for situated intelligence, 2017-January, pp. 493-494, (2017); Handbook of Multimodal Multisensor Interfaces Language Processing Software Commercialization and Emerging Directions Vol 3 Pp 105143, (2019); Brudy, Frederik, Cross-device taxonomy: Survey, opportunities and challenges of interactions spanning across multiple devices, Conference on Human Factors in Computing Systems - Proceedings, (2019); Bernheim Brush, A. J., Home automation in the wild: Challenges and opportunities, Conference on Human Factors in Computing Systems - Proceedings, pp. 2115-2124, (2011); Cesta, Amedeo, User needs and preferences on AAL systems that support older adults and their carers, Journal of Ambient Intelligence and Smart Environments, 10, 1, pp. 49-70, (2018)","","IOS Press BV","","","","","","18761364","","","","English","Article","Final","All Open Access; Bronze Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85145659785"
"Saganowski, S.; Miszczyk, J.; Kunc, D.; Lisouski, D.; Kazienko, P.","Saganowski, Stanisław (50262928000); Miszczyk, Jan (57759534200); Kunc, Dominika (57224443139); Lisouski, Dzmitry (58093915300); Kazienko, Przemysław (35615668400)","50262928000; 57759534200; 57224443139; 58093915300; 35615668400","Lessons Learned from Developing Emotion Recognition System for Everyday Life","2022","","","","","1047","1054","0","5","10.1145/3560905.3567759","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147539941&doi=10.1145%2F3560905.3567759&partnerID=40&md5=c0112c4ce86833e5f7bd40b2a586ea48","Politechnika Wrocławska, Wroclaw, Poland","Saganowski, Stanisław, Politechnika Wrocławska, Wroclaw, Poland; Miszczyk, Jan, Politechnika Wrocławska, Wroclaw, Poland; Kunc, Dominika, Politechnika Wrocławska, Wroclaw, Poland; Lisouski, Dzmitry, Politechnika Wrocławska, Wroclaw, Poland; Kazienko, Przemysław, Politechnika Wrocławska, Wroclaw, Poland","Recognizing emotions in everyday life requires a user-friendly and reliable system based on a smartphone and wearables. For over a year, we have been developing the Emognition system, which enables collecting emotionally annotated physiological signals in real-life scenarios. In this work, we describe the system architecture, the components and libraries used, as well as the development, testing, and implementation strategies. We explain in detail the integration with wearables - smartwatch Samsung Galaxy Watch 3 and chest strap Polar H10. The encountered problems and developed solutions are thoroughly discussed. We also provide the advantages and limitations of several frameworks for embedding machine learning models into a resource-restricted mobile application. © 2023 Elsevier B.V., All rights reserved.","emognition system; emotional data; emotionally annotated physiology; wearables","Internet of things; Physiology; Emognition system; Emotion recognition; Emotional data; Emotionally annotated physiology; Recognition systems; Recognizing emotions; Reliable systems; Smart phones; User friendly; Wearables; Emotion Recognition","","","This work was partially supported by National Science Centre, Poland, project no. 2020/37/B/ST6/03806; by the statutory funds of the Department of Artificial Intelligence, Wroclaw University of Science and Technology; by the Polish Ministry of Education and Science, National Information Processing Institute — the CLARIN-PL Project.","Albraikan, Amani, IAware: A Real-Time Emotional Biofeedback System Based on Physiological Signals, IEEE Access, 6, pp. 78780-78789, (2018); Álvarez, Pedro J., Mobile music recommendations for runners based on location and emotions: The DJ-Running system, Pervasive and Mobile Computing, 67, (2020); IEEE Transactions on Affective Computing, (2022); E Methodology, (2020); Dzieżyc, Maciej, How to catch them all? Enhanced data collection for emotion recognition in the field, pp. 348-351, (2021); Feng, Huanghao, A wavelet-based approach to emotion classification using EDA signals, Expert Systems with Applications, 112, pp. 77-86, (2018); Fernández-Aguilar, Luz, Emotion Detection in Aging Adults Through Continuous Monitoring of Electro-Dermal Activity and Heart-Rate Variability, Lecture Notes in Computer Science, 11486 LNCS, pp. 252-261, (2019); Gloor, Peter A., Aristotle Said “Happiness is a State of Activity” — Predicting Mood Through Body Sensing with Smartwatches, Journal of Systems Science and Systems Engineering, 27, 5, pp. 586-612, (2018); He, Cheng, An emotion recognition system based on physiological signals obtained by wearable sensors, Lecture Notes in Electrical Engineering, 399, pp. 15-25, (2017); Hernández, Javier, Guidelines for Assessing and Minimizing Risks of Emotion Recognition Applications, (2021)","","Association for Computing Machinery, Inc","ACM SIGARCH; ACM SIGBED; ACM SIGCOMM; ACM SIGMETRICS; ACM SIGMOBILE; ACM SIGOPS","20th ACM Conference on Embedded Networked Sensor Systems, SenSys 2022","","Boston; MA","186204","","9781450398862","","","English","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85147539941"
"Popescu, A.-L.; Popescu, N.","Popescu, Aura Loredana (57221333045); Popescu, Nirvana Alina (9240753400)","57221333045; 9240753400","Drawing Interpretation Using Neural Networks and Accessibility Implementation in Mobile Application","2022","Computation","10","11","202","","","0","2","10.3390/computation10110202","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149469065&doi=10.3390%2Fcomputation10110202&partnerID=40&md5=dbaadd1b0344598f144a22420f6586dd","Department of Computer Science, University Politehnica of Bucharest, Bucharest, Romania","Popescu, Aura Loredana, Department of Computer Science, University Politehnica of Bucharest, Bucharest, Romania; Popescu, Nirvana Alina, Department of Computer Science, University Politehnica of Bucharest, Bucharest, Romania","This paper continues the research of the previous work, regarding PandaSays mobile application, having its main purpose to detect the affective state of the child from his drawings, using MobileNet neural network. Children diagnosed with autism spectrum disorder, have difficulties in expressing their feelings and communicating with others. The purpose of PandaSays mobile application, is to help parents and tutors that have children diagnosed with autism, to communicate better with them and to understand their feelings. The main goal was to improve the model’s accuracy, trained with MobileNet neural network, which reached the value of 84.583%. For training the model, it was used Python programming language. The study focuses further on accessibility and its importance to children diagnosed with autism. Relevant screenshots of the mobile application are presented, in order to indicate that the application follows the accessibility guidelines and rules. Finally, there is presented the interaction with Marty robot and the efficiency of mobile application’s drawing prediction. © 2023 Elsevier B.V., All rights reserved.","accessibility; autism; drawing interpretation; humanoid robots; machine learning; mobile application; neural networks","","","","This research was funded by University Politehnica of Bucharest, Pub Art Project.","undefined; undefined; Miskam, Mohd Azfar, Humanoid robot NAO as a teaching tool of emotion recognition for children with autism using the Android app, (2014); Farhan, Sk Adif, Improvement of Verbal and Non-Verbal Communication Skills of Children with Autism Spectrum Disorder using Human Robot Interaction, pp. 356-359, (2021); Akalin, Neziha, Non-verbal communication with a social robot peer: Towards robot assisted interactive sign language tutoring, IEEE-RAS International Conference on Humanoid Robots, 2015-February, pp. 1122-1127, (2015); Abou El-Seoud, M. Samir, A pictorial mobile-based communication application for non-verbal people with autism, pp. 529-534, (2014); undefined; Wainer, Joshua, Using the humanoid robot KASPAR to autonomously play triadic games and facilitate collaborative play among children with autism, IEEE Transactions on Autonomous Mental Development, 6, 3, pp. 183-199, (2014); Yussof, Hanafiah Bin, IQ level assessment methodology in robotic intervention with children with autism, (2015); Shamsuddin, Syamimi A., Initial response of autistic children in human-robot interaction therapy with humanoid robot NAO, pp. 188-193, (2012)","","MDPI","","","","","","20793197","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85149469065"
"Miranda-Calero, J.A.; Rituerto-González, E.; Luis-Mingueza, C.; Canabal, M.F.; Ramirez-Barcenas, A.R.; Lanza-Gutierrez, J.M.; Peláez-Moreno, C.; Lopez-Ongil, C.","Miranda-Calero, José Angel (57196354696); Rituerto-González, Esther (57209294977); Luis-Mingueza, Clara (57545903000); Canabal, Manuel Felipe (57203965293); Ramirez-Barcenas, Alberto (57545135900); Lanza-Gutierrez, Jose M. (54784705200); Peláez-Moreno, Carmen (6507551724); Lopez-Ongil, Celia (6508213987)","57196354696; 57209294977; 57545903000; 57203965293; 57545135900; 54784705200; 6507551724; 6508213987","Bindi: Affective Internet of Things to Combat Gender-Based Violence","2022","IEEE Internet of Things Journal","9","21","","21174","21193","0","31","10.1109/JIOT.2022.3177256","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130841154&doi=10.1109%2FJIOT.2022.3177256&partnerID=40&md5=44ddf5ed7928c353f4b2c6bd78b2018b","Department of Electronics, Universidad Carlos III de Madrid, Getafe, Spain; Department of Signal Theory and Communications, Universidad Carlos III de Madrid, Getafe, Spain; Department of Computer Science, Universidad de Alcalá, Alcala de Henares, Spain","Miranda-Calero, José Angel, Department of Electronics, Universidad Carlos III de Madrid, Getafe, Spain; Rituerto-González, Esther, Department of Signal Theory and Communications, Universidad Carlos III de Madrid, Getafe, Spain; Luis-Mingueza, Clara, Department of Signal Theory and Communications, Universidad Carlos III de Madrid, Getafe, Spain; Canabal, Manuel Felipe, Department of Electronics, Universidad Carlos III de Madrid, Getafe, Spain; Ramirez-Barcenas, Alberto, Department of Electronics, Universidad Carlos III de Madrid, Getafe, Spain; Lanza-Gutierrez, Jose M., Department of Computer Science, Universidad de Alcalá, Alcala de Henares, Spain; Peláez-Moreno, Carmen, Department of Signal Theory and Communications, Universidad Carlos III de Madrid, Getafe, Spain; Lopez-Ongil, Celia, Department of Electronics, Universidad Carlos III de Madrid, Getafe, Spain","The main research motivation of this article is the fight against gender-based violence and achieving gender equality from a technological perspective. The solution proposed in this work goes beyond currently existing panic buttons, needing to be manually operated by the victims under difficult circumstances. Instead, Bindi, our end-to-end autonomous multimodal system, relies on artificial intelligence methods to automatically identify violent situations, based on detecting fear-related emotions, and trigger a protection protocol, if necessary. To this end, Bindi integrates modern state-of-the-art technologies, such as the Internet of Bodies, affective computing, and cyber-physical systems, leveraging: 1) affective Internet of Things (IoT) with auditory and physiological commercial off-the-shelf smart sensors embedded in wearable devices; 2) hierarchical multisensorial information fusion; and 3) the edge-fog-cloud IoT architecture. This solution is evaluated using our own data set named WEMAC, a very recently collected and freely available collection of data comprising the auditory and physiological responses of 47 women to several emotions elicited by using a virtual reality environment. On this basis, this work provides an analysis of multimodal late fusion strategies to combine the physiological and speech data processing pipelines to identify the best intelligence engine strategy for Bindi. In particular, the best data fusion strategy reports an overall fear classification accuracy of 63.61% for a subject-independent approach. Both a power consumption study and an audio data processing pipeline to detect violent acoustic events complement this analysis. This research is intended as an initial multimodal baseline that facilitates further work with real-life elicited fear in women. © 2022 Elsevier B.V., All rights reserved.","Artificial intelligence of things; edge computing; fear recognition; microelectromechanical systems; multimodal data fusion; smart sensors","Computer architecture; Edge computing; Electromechanical devices; Embedded systems; Emotion Recognition; Hierarchical systems; Interactive computer systems; Internet of things; Physiological models; Physiology; Pipelines; Sensor data fusion; Smartphones; Virtual reality; Wearable sensors; Artificial intelligence of thing.; Data processing pipelines; Emotion recognition; Fear recognition; Gender equality; Multi-modal; Multimodal data fusion; Research motivations; Technological perspective; MEMS","","","","Gender Based Violence in Emergencies, (2019); Sardinha, Lynnmarie, Global, regional, and national prevalence estimates of physical or sexual, or both, intimate partner violence against women in 2018, The Lancet, 399, 10327, pp. 803-813, (2022); Gender Based Violence Victims Killed in Spain by their Partners or Former Partners; Costs of Gender Based Violence in the European Union European Institute for Gender Equality Eige Eur, (2021); Jewkes, R. K., More research is needed on digital technologies in violence against women, The Lancet Public Health, 4, 6, pp. e270-e271, (2019); Alertcops Smart Phone Application; Karusala, Naveena, Women's safety in public spaces: Examining the efficacy of panic buttons in New Delhi, Conference on Human Factors in Computing Systems - Proceedings, 2017-May, pp. 3340-3351, (2017); Revista De Estudios Socioeducativos, (2019); Portilla, J., The Extreme Edge at the Bottom of the Internet of Things: A Review, IEEE Sensors Journal, 19, 9, pp. 3179-3190, (2019); Machine Learning, (1997)","","Institute of Electrical and Electronics Engineers Inc.","","","","","","23274662","9781728176055","","","English","Article","Final","All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85130841154"
"Liu, I.; Zhong, Q.; Liu, F.; Xu, H.; Chen, W.; Zhu, X.; Ma, Y.; Ni, S.","Liu, Ivan Shih Chun (57216199846); Zhong, Qi (58849658400); Liu, Fangyuan (58797924100); Xu, Hushan (58894657100); Chen, Wenxi (58894457400); Zhu, Xiaoqing (58894332900); Ma, Yingbo (57911760200); Ni, Shiguang (55908625400)","57216199846; 58849658400; 58797924100; 58894657100; 58894457400; 58894332900; 57911760200; 55908625400","Assessing Mental Health During Covid-19 Lockdown: A Smartphone-Based Multimodal Emotion Recognition Approach","2022","","","","","262","269","0","0","10.1145/3565698.3565795","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185409061&doi=10.1145%2F3565698.3565795&partnerID=40&md5=e1240dcb5ba58303f0b35b36767f9791","Faculty of Arts and Sciences, Beijing Normal University, Beijing, China; Faculty of Psychology, Beijing Normal University, Beijing, China; Shenzhen International Graduate School, Tsinghua University, Beijing, China","Liu, Ivan Shih Chun, Faculty of Arts and Sciences, Beijing Normal University, Beijing, China; Zhong, Qi, Faculty of Psychology, Beijing Normal University, Beijing, China; Liu, Fangyuan, Faculty of Arts and Sciences, Beijing Normal University, Beijing, China; Xu, Hushan, Faculty of Psychology, Beijing Normal University, Beijing, China; Chen, Wenxi, Faculty of Psychology, Beijing Normal University, Beijing, China; Zhu, Xiaoqing, Faculty of Psychology, Beijing Normal University, Beijing, China; Ma, Yingbo, Faculty of Psychology, Beijing Normal University, Beijing, China; Ni, Shiguang, Shenzhen International Graduate School, Tsinghua University, Beijing, China","The large-scale lockdowns that occurred during the coronavirus pandemic have caused a severe public mental health crisis. Consequently, a fast and effective method for assessing the mental state of the public is essential. As artificial intelligence technology advances, smartphone-based multimodal emotion recognition technology can provide potential solutions to this problem. Fifty-five participants who lived in lockdown areas participated in this study. We extracted facial expressions, acoustic features, and text information from five-minute self-recorded videos - as well as participants' cardiovascular information - using smartphone photoplethysmography (PPG) technology, which converted videos of their fingertips into heart-rate information. Support vector machine (SVM) and random forest (RF) algorithms were used for each modality - as well as in the final fusion stage - to determine the best multi-modality prediction model for mental health. Our results showed that the prediction accuracy of various mental health indicators using the single-modality models was between 0.31 and 0.52. However, the multi-modality model provided better, more stable results. This study validated the feasibility of using smartphones in mental health assessments of residents under lockdown. Our data also supported the use of multi-modality models and confirmed their superiority in emotion recognition. © 2024 Elsevier B.V., All rights reserved.","Acoustic feature analysis; Facial expression recognition; Heart rate variability; Human-computer interaction; Mental health; Multimodal emotion recognition; Natural language processing; Smartphone photoplethysmography","Emotion Recognition; Face recognition; Forestry; Heart; Human computer interaction; mHealth; Photoplethysmography; Speech recognition; Support vector machines; Acoustic feature analyse; Acoustic features; Facial expression recognition; Feature analysis; Heart rate variability; Language processing; Mental health; Multimodal emotion recognition; Natural language processing; Natural languages; Smart phones; Smartphone photoplethysmography; Smartphones","","","Funding text 1: This study was funded by the Shenzhen Key Laboratory of next generation interactive media innovative technology (Grant No. ZDSYS20210623092001004), Chinese National Social Science Foundation (Grant No.BBA210042), the Shenzhen Key Research Base of Humanities and Social Sciences for People s Well-being Benchmarking Study(Grant No. 202003); Funding text 2: This study was funded by the Shenzhen Key Laboratory of next generation interactive media innovative technology (Grant No. ZDSYS20210623092001004), Chinese National Social Science Foundation (Grant No.BBA210042), the Shenzhen Key Research Base of Humanities and Social Sciences for People’s Well-being Benchmarking Study(Grant No. 202003)","Anikin, Andrey, Static and dynamic formant scaling conveys body size and aggression, Royal Society Open Science, 9, 1, (2022); Antonovsky, Aaron, The life cycle, mental health and the sense of coherence, Israel Journal of Psychiatry and Related Sciences, 22, 4, pp. 273-280, (1985); Arya, Resham, A survey of multidisciplinary domains contributing to affective computing, Computer Science Review, 40, (2021); Atalan, Abdulkadir, Is the lockdown important to prevent the COVID-19 pandemic? Effects on psychology, environment and economy-perspective, Annals of Medicine and Surgery, 56, pp. 38-42, (2020); Baumeister, Roy F., Psychology as the Science of Self-Reports and Finger Movements: Whatever Happened to Actual Behavior?, Perspectives on Psychological Science, 2, 4, pp. 396-403, (2007); Speak and Unspeak with Praat, (2001); Brawner, Keith W., Establishing ground truth on pyschophysiological models for training machine learning algorithms: Options for ground truth proxies, Lecture Notes in Computer Science, 10284 11th International Conference, AC 2017, Held as Part of HCI International 2017, Vancouver, BC, Canada, July 9-14, 2017, Proceedings, Part I, pp. 468-477, (2017); Brockmeyer, Timo, Me, myself, and I: Self-referent word use as an indicator of self-focused attention in relation to depression and anxiety, Frontiers in Psychology, 6, OCT, (2015); Chanel, Guillaume, Short-term emotion assessment in a recall paradigm, International Journal of Human Computer Studies, 67, 8, pp. 607-627, (2009); Constantine, Layale, A survey of ground-truth in emotion data annotation, pp. 697-702, (2012)","","Association for Computing Machinery","","10th International Symposium of Chinese CHI, Chinese CHI 2022","","Hybrid, Guangzhou","197194","","9781450398695","","","English","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85185409061"
"Li, J.; Zhang, X.; Huang, L.; Li, F.; Duan, S.; Sun, Y.","Li, Juan (57929782300); Zhang, Xueying (56048917100); Huang, Lixia (55492512500); Li, Fenglian (55494500300); Duan, Shufei (57188670694); Sun, Ying (56902003700)","57929782300; 56048917100; 55492512500; 55494500300; 57188670694; 56902003700","Speech Emotion Recognition Using a Dual-Channel Complementary Spectrogram and the CNN-SSAE Neutral Network","2022","Applied Sciences (Switzerland)","12","19","9518","","","0","17","10.3390/app12199518","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139967529&doi=10.3390%2Fapp12199518&partnerID=40&md5=dacdcb83cbee75ac4dd53cfd9e062b75","College of Information and Computer, Taiyuan University of Technology, Taiyuan, China; Department of Physics and Electronic Engineering, Yuncheng University, Yuncheng, China","Li, Juan, College of Information and Computer, Taiyuan University of Technology, Taiyuan, China, Department of Physics and Electronic Engineering, Yuncheng University, Yuncheng, China; Zhang, Xueying, College of Information and Computer, Taiyuan University of Technology, Taiyuan, China; Huang, Lixia, College of Information and Computer, Taiyuan University of Technology, Taiyuan, China; Li, Fenglian, College of Information and Computer, Taiyuan University of Technology, Taiyuan, China; Duan, Shufei, College of Information and Computer, Taiyuan University of Technology, Taiyuan, China; Sun, Ying, College of Information and Computer, Taiyuan University of Technology, Taiyuan, China","Featured Application: Emotion recognition is the computer’s automatic recognition of the emotional state of input speech. It is a hot research field, resulting from the mutual infiltration and interweaving of phonetics, psychology, digital signal processing, pattern recognition, and artificial intelligence. At present, speech emotion recognition has been widely used in the fields of intelligent signal processing, smart medical care, business intelligence, assistant lie detection, criminal investigation, the service industry, self-driving cars, voice assistants of smartphones, and human psychoanalysis, etc. In the background of artificial intelligence, the realization of smooth communication between people and machines has become the goal pursued by people. Mel spectrograms is a common method used in speech emotion recognition, focusing on the low-frequency part of speech. In contrast, the inverse Mel (IMel) spectrogram, which focuses on the high-frequency part, is proposed to comprehensively analyze emotions. Because the convolutional neural network-stacked sparse autoencoder (CNN-SSAE) can extract deep optimized features, the Mel-IMel dual-channel complementary structure is proposed. In the first channel, a CNN is used to extract the low-frequency information of the Mel spectrogram. The other channel extracts the high-frequency information of the IMel spectrogram. This information is transmitted into an SSAE to reduce the number of dimensions, and obtain the optimized information. Experimental results show that the highest recognition rates achieved on the EMO-DB, SAVEE, and RAVDESS datasets were 94.79%, 88.96%, and 83.18%, respectively. The conclusions are that the recognition rate of the two spectrograms was higher than that of each of the single spectrograms, which proves that the two spectrograms are complementary. The SSAE followed the CNN to get the optimized information, and the recognition rate was further improved, which proves the effectiveness of the CNN-SSAE network. © 2022 Elsevier B.V., All rights reserved.","deep learning; IMel spectrogram; Mel spectrogram; speech emotion recognition","","","","Funding text 1: This work was supported in part by the National Nature Science Foundation of China under Grant 62271342, in part by “Project 1331” Quality Enhancement and Efficiency Construction Plan National First-class Major Construction Project of Electronic Science and Technology, in part by the National Natural Science Foundation of China Youth Science Foundation under Grant 12004275, in part by the Natural Science Foundation of Shanxi Province, China, under Grant 201901D111096, in part by Research Project Supported by Shanxi Scholarship Council of China HGKY2019025.; Funding text 2: This research was supported by the Hunan Provincial Key Laboratory of Intelligent Disaster Prevention-Mitigation and Ecological Restoration in Civil Engineering, Key Investigation and Application of Intelligent Disaster Prevention-Mitigation and Ecological Restoration in Civil Engineering.","Yildirim, Serdar, A modified feature selection method based on metaheuristic algorithms for speech emotion recognition, Applied Acoustics, 173, (2021); Fahad, Md Shah, A survey of speech emotion recognition in natural environment, Digital Signal Processing: A Review Journal, 110, (2021); Wang, S. Hua, Intelligent facial emotion recognition based on stationary wavelet entropy and Jaya algorithm, Neurocomputing, 272, pp. 668-676, (2018); Gunes, Hatice, Bi-modal emotion recognition from expressive face and body gestures, Journal of Network and Computer Applications, 30, 4, pp. 1334-1345, (2007); Noroozi, Fatemeh, Survey on Emotional Body Gesture Recognition, IEEE Transactions on Affective Computing, 12, 2, pp. 505-523, (2021); Islam, Md Rabiul, Emotion Recognition from EEG Signal Focusing on Deep Learning and Shallow Learning Techniques, IEEE Access, 9, pp. 94601-94624, (2021); Abbaschian, Babak Joze, Deep learning techniques for speech emotion recognition, from databases to models, Sensors, 21, 4, pp. 1-27, (2021); Zhang, Huiyun, A novel heterogeneous parallel convolution Bi-LSTM for speech emotion recognition, Applied Sciences (Switzerland), 11, 21, (2021); El Ayadi, Moataz M.H., Survey on speech emotion recognition: Features, classification schemes, and databases, Pattern Recognition, 44, 3, pp. 572-587, (2011); Akçay, Mehmet Berkehan, Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers, Speech Communication, 116, pp. 56-76, (2020)","","MDPI","","","","","","20763417","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85139967529"
"Neggaz, I.; Fizazi, H.","Neggaz, Imene (57471872000); Fizazi, Hadria (54997931900)","57471872000; 54997931900","An Intelligent handcrafted feature selection using Archimedes optimization algorithm for facial analysis","2022","Soft Computing","26","19","","10435","10464","0","23","10.1007/s00500-022-06886-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125540868&doi=10.1007%2Fs00500-022-06886-3&partnerID=40&md5=6eb7fa24ffc872a7dd93ab38b1d58f10","Département d’Informatique, Université des Sciences et de la Technologie d’Oran Mohamed-Boudiaf, Oran, Algeria","Neggaz, Imene, Département d’Informatique, Université des Sciences et de la Technologie d’Oran Mohamed-Boudiaf, Oran, Algeria; Fizazi, Hadria, Département d’Informatique, Université des Sciences et de la Technologie d’Oran Mohamed-Boudiaf, Oran, Algeria","Human facial analysis (HFA) has recently become an attractive topic for computer vision research due to technological progress and mobile applications. HFA explores several issues as gender recognition (GR), facial expression, age, and race recognition for automatically understanding social life. This study explores HFA from the angle of recognizing a person’s gender from their face. Several hard challenges are provoked, such as illumination, occlusion, facial emotions, quality, and angle of capture by cameras, making gender recognition more difficult for machines. The Archimedes optimization algorithm (AOA) was recently designed as a metaheuristic-based population optimization method, inspired by the Archimedes theory’s physical notion. Compared to other swarm algorithms in the realm of optimization, this method promotes a good balance between exploration and exploitation. The convergence area is increased By incorporating extra data into the solution, such as volume and density. Because of the preceding benefits of AOA and the fact that it has not been used to choose the best area of the face, we propose utilizing a wrapper feature selection technique, which is a real motivation in the field of computer vision and machine learning. The paper’s primary purpose is to automatically determine the optimal face area using AOA to recognize the gender of a human person categorized by two classes (Men and women). In this paper, the facial image is divided into several subregions (blocks), where each area provides a vector of characteristics using one method from handcrafted techniques as the local binary pattern (LBP), histogram-oriented gradient (HOG), or gray-level co-occurrence matrix (GLCM). Two experiments assess the proposed method (AOA): The first employs two benchmarking datasets: the Georgia Tech Face dataset (GT) and the Brazilian FEI dataset. The second experiment represents a more challenging large dataset that uses Gallagher’s uncontrolled dataset. The experimental results show the good performance of AOA compared to other recent and competitive optimizers for all datasets. In terms of accuracy, the AOA-based LBP outperforms the state-of-the-art deep convolutional neural network (CNN) with 96.08% for the Gallagher’s dataset. © 2023 Elsevier B.V., All rights reserved.","Archimedes optimization algorithm (AOA); Automatic selection; Handcrafted methods; Human facial analysis (HFA); Wrapper feature selection (FS)","Convolutional neural networks; Deep neural networks; Emotion Recognition; Face recognition; Feature extraction; Local binary pattern; Optimization; Archimede optimization algorithm; Automatic selection; Facial analysis; Features selection; Gender recognition; Handcrafted method; Human facial analyse; Local binary patterns; Optimization algorithms; Wrapper feature selection; Large dataset","","","","Abdalrady, Nihad A., Fusion of Multiple Simple Convolutional Neural Networks for Gender Classification, pp. 251-256, (2020); Abirami, B., Gender and age prediction from real time facial images using CNN, Materials Today: Proceedings, 33, pp. 4708-4712, (2020); Acien, Alejandro, Measuring the gender and ethnicity bias in deep models for face recognition, Lecture Notes in Computer Science, 11401 LNCS, pp. 584-593, (2019); Afifi, Mahmoud, 11K Hands: Gender recognition and biometric identification using a large dataset of hand images, Multimedia Tools and Applications, 78, 15, pp. 20835-20854, (2019); International Conference on Sustainable and Innovative Solutions for Current Challenges in Engineering Technology, (2019); Alhichri, Haikel Salem, Classification of Remote Sensing Images Using EfficientNet-B3 CNN Model with Attention, IEEE Access, 9, pp. 14078-14094, (2021); Evolutionary Machine Learning Techniques, (2020); Althnian, Alhanoof, Face gender recognition in the wild: An extensive performance comparison of deep-learned, hand-crafted, and fused features with deep and traditional models, Applied Sciences (Switzerland), 11, 1, pp. 1-16, (2021); Multimedia Research, (2021); Wirel Pers Commun, (2021)","","Springer Science and Business Media Deutschland GmbH","","","","","","14327643; 14337479","","","","English","Article","Final","All Open Access; Bronze Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85125540868"
"Cheng, W.K.; Leong, W.C.; Tan, J.S.; Hong, Z.-W.; Chen, Y.-L.","Cheng, Wai Khuen (57207770147); Leong, Waichun (57897590300); Tan, Joi San (57005878300); Hong, Zengwei (35208291200); Chen, Yen Lin (35322122400)","57207770147; 57897590300; 57005878300; 35208291200; 35322122400","Affective Recommender System for Pet Social Network","2022","Sensors","22","18","6759","","","0","8","10.3390/s22186759","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138421510&doi=10.3390%2Fs22186759&partnerID=40&md5=0009c773637ae9a3a870d0f49f131a4a","Faculty of Information and Communication Technology, Universiti Tunku Abdul Rahman, Kajang, Malaysia; Department of Computer Science and Information Engineering, Feng Chia University, Taichung, Taiwan; Department of Computer Science and Information Engineering, National Taipei University of Technology, Taipei, Taiwan","Cheng, Wai Khuen, Faculty of Information and Communication Technology, Universiti Tunku Abdul Rahman, Kajang, Malaysia; Leong, Waichun, Faculty of Information and Communication Technology, Universiti Tunku Abdul Rahman, Kajang, Malaysia; Tan, Joi San, Faculty of Information and Communication Technology, Universiti Tunku Abdul Rahman, Kajang, Malaysia; Hong, Zengwei, Department of Computer Science and Information Engineering, Feng Chia University, Taichung, Taiwan; Chen, Yen Lin, Department of Computer Science and Information Engineering, National Taipei University of Technology, Taipei, Taiwan","In this new era, it is no longer impossible to create a smart home environment around the household. Moreover, users are not limited to humans but also include pets such as dogs. Dogs need long-term close companionship with their owners; however, owners may occasionally need to be away from home for extended periods of time and can only monitor their dogs’ behaviors through home security cameras. Some dogs are sensitive and may develop separation anxiety, which can lead to disruptive behavior. Therefore, a novel smart home solution with an affective recommendation module is proposed by developing: (1) an application to predict the behavior of dogs and, (2) a communication platform using smartphones to connect with dog friends from different households. To predict the dogs’ behaviors, the dog emotion recognition and dog barking recognition methods are performed. The ResNet model and the sequential model are implemented to recognize dog emotions and dog barks. The weighted average is proposed to combine the prediction value of dog emotion and dog bark to improve the prediction output. Subsequently, the prediction output is forwarded to a recommendation module to respond to the dogs’ conditions. On the other hand, the Real-Time Messaging Protocol (RTMP) server is implemented as a platform to contact a dog’s friends on a list to interact with each other. Various tests were carried out and the proposed weighted average led to an improvement in the prediction accuracy. Additionally, the proposed communication platform using basic smartphones has successfully established the connection between dog friends. © 2022 Elsevier B.V., All rights reserved.","affective recommendation; deep learning; dog barking recognition; emotion recognition model; pet social network","Automation; Deep learning; Forecasting; Smartphones; Social networking (online); Speech recognition; Statistical methods; Affective recommendation; Communication platforms; Dog barking recognition; Emotion recognition; Emotion recognition model; Pet social network; Recognition models; Smart homes; Smart phones; Emotion Recognition; animal; dog; emotion; human; human-animal bond; information processing; social network; Animals; Data Collection; Dogs; Emotions; Human-Animal Bond; Humans; Social Networking","","","This work was funded by the Ministry of Science and Technology in Taiwan, under grant numbers MOST-109-2628-E-027-004-MY3, MOST-111-2218-E-027-003, and MOST-110-2622-8-027-006.","Rasch, Katharina, An unsupervised recommender system for smart homes, Journal of Ambient Intelligence and Smart Environments, 6, 1, pp. 21-37, (2014); Ojagh, Soroush, A location-based orientation-aware recommender system using IoT smart devices and Social Networks, Future Generation Computer Systems, 108, pp. 97-118, (2020); Mishra, Prabhat, Alternate Action Recommender System Using Recurrent Patterns of Smart Home Users, (2020); Gladence, L. Mary, RETRACTED ARTICLE: Recommender system for home automation using IoT and artificial intelligence, Journal of Ambient Intelligence and Humanized Computing, 15, 2, (2024); Altulyan, May S., A Survey on Recommender Systems for Internet of Things: Techniques, Applications and Future Directions, Computer Journal, 65, 8, pp. 2098-2132, (2022); Liu, Hai, EDMF: Efficient Deep Matrix Factorization With Review Feature Learning for Industrial Recommender System, IEEE Transactions on Industrial Informatics, 18, 7, pp. 4361-4371, (2022); Liu, Hai, Multi-perspective social recommendation method with graph representation learning, Neurocomputing, 468, pp. 469-481, (2022); Li, Duantengchuan, CARM: Confidence-aware recommender model via review representation learning and historical rating behavior in the online platforms, Neurocomputing, 455, pp. 283-296, (2021); Rodríguez Fernández, María, Using the Big Data generated by the Smart Home to improve energy efficiency management, Energy Efficiency, 9, 1, pp. 249-260, (2016); Hossain, M. Shamim, Cyber–physical cloud-oriented multi-sensory smart home framework for elderly people: An energy efficiency perspective, Journal of Parallel and Distributed Computing, 103, pp. 11-21, (2017)","","MDPI","","","","","","14248220","","","36146109","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85138421510"
"Ali, N.; Abdrazaq, A.; Shah, Z.; Househ, M.","Ali, Nashva (57221715291); Abdrazaq, Alaa (57730885000); Shah, Zubair (56428700200); Househ, Mowafa Said (8667908000)","57221715291; 57730885000; 56428700200; 8667908000","Artificial Intelligence-Based Mobile Application for Emotion Sensing for Children Through Art","2022","Studies in Health Technology and Informatics","290","","","1130","1131","0","2","10.3233/SHTI220302","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131476162&doi=10.3233%2FSHTI220302&partnerID=40&md5=28c0e6b064b2f17a0a4c3041f2753113","Division of Information and Computing Technology, Hamad Bin Khalifa University, College of Science and Engineering, Doha, Qatar","Ali, Nashva, Division of Information and Computing Technology, Hamad Bin Khalifa University, College of Science and Engineering, Doha, Qatar; Abdrazaq, Alaa, Division of Information and Computing Technology, Hamad Bin Khalifa University, College of Science and Engineering, Doha, Qatar; Shah, Zubair, Division of Information and Computing Technology, Hamad Bin Khalifa University, College of Science and Engineering, Doha, Qatar; Househ, Mowafa Said, Division of Information and Computing Technology, Hamad Bin Khalifa University, College of Science and Engineering, Doha, Qatar","In this paper, we develop an artificial intelligence (A.I.) based Emotion Sensing Recognition App (ESRA) to help parents and teachers understand the emotions of children by analyzing their drawings. Four different experiments were conducted using a combination of two datasets. The deep learning model was trained using the Fastai library in Python. The model classifies the drawings into positive or negative emotions. The model accuracy ranged from 55% to 79% in the four experiments. © 2022 Elsevier B.V., All rights reserved.","","Emotion Recognition; Medical informatics; Python; Learning models; Mobile applications; Modeling accuracy; Teachers'; Deep learning; artificial intelligence; child; conference paper; deep learning; drawing; emotion; female; human; human experiment; male; mobile application; teacher; child parent relation; physiotherapy; Artificial Intelligence; Child; Emotions; Humans; Mobile Applications; Parents; Physical Therapy Modalities","","","","How Artificial Intelligence can Detect Emotions in Children S Drawings, (2019)","Otero, P.; Scott, P.; Martin, S.Z.; Huesing, E.","IOS Press BV","","18th World Congress on Medical and Health Informatics: One World, One Health - Global Partnership for Digital Innovation, MEDINFO 2021","","Virtual, Online","179966","18798365; 09269630","9781643685939; 9781643685946; 9781643683409; 1586031430; 9781607508052; 9781586037512; 1586036475; 1586032038; 9051993625; 9781614996521","","35673240","English","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85131476162"
"Sayis, B.; Ramirez-Melendez, R.; Pares, N.","Sayis, Batuhan (57203517032); Ramirez-Melendez, Rafael (35280935600); Pares, Narcis (55939174600)","57203517032; 35280935600; 55939174600","Mixed reality or LEGO game play? Fostering social interaction in children with Autism","2022","Virtual Reality","26","2","","771","787","0","15","10.1007/s10055-021-00580-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115379231&doi=10.1007%2Fs10055-021-00580-9&partnerID=40&md5=076ca81edb3251fde5c1eb407cae04db","Universitat Pompeu Fabra Barcelona, Barcelona, Spain","Sayis, Batuhan, Universitat Pompeu Fabra Barcelona, Barcelona, Spain; Ramirez-Melendez, Rafael, Universitat Pompeu Fabra Barcelona, Barcelona, Spain; Pares, Narcis, Universitat Pompeu Fabra Barcelona, Barcelona, Spain","This study extends the previous research in which it has been shown that a mixed reality (MR) system fosters social interaction behaviours (SIBs) in children with Autism Spectrum Condition (ASC). When comparing this system to a LEGO-based non-digital intervention, it has been observed that an MR system effectively mediates a face-to-face play session between a child with ASC and a child without ASC providing new specific advantageous properties (e.g. not being a passive tool, not needing to be guided by the therapist). Considering the newly collected multimodal data totaling to 72 children (36 trials of dyads, child with ASC/child without ASC), a first goal of the present study is to apply detailed statistical inference and machine learning techniques to extensively evaluate the overall effect of this MR system, when compared to the LEGO condition. This goal also includes the analysis of psychophysiological data and allows the context-driven triangulation of the multimodal data which is operationalized by (i) video-coding of SIBs, (ii) psychophysiological data, and (iii) system logs of user-system events. A second goal is to show how SIBs, taking place in these experiences, are influenced by the internal states of the users and the system. SIBs were measured by video-coding overt behaviours (Initiation, Response and Externalization) and with self-reports. Internal states were measured using a wearable device designed by the FuBIntLab (Full-Body Interaction Lab) to acquire: Electrocardiogram (ECG) and Electrodermal Activity (EDA) data. Affective sliders and State Trait Anxiety Scale questionnaires were used as self-reports. Repeated-measures design was chosen with two conditions, the MR environment and the traditional therapy LEGO. The results show that the MR system has a positive effect on SIBs when compared to the LEGO condition, with an added advantage of being more flexible. © 2022 Elsevier B.V., All rights reserved.","Children with Autism; Embodied interaction; Mixed reality; Multi-modal evaluation; Psychophysiology; Social initiation","Codes (symbols); Electrocardiography; Electrophysiology; Image coding; Learning systems; Mixed reality; Surveys; Video signal processing; Children with autisms; Condition; Embodied interaction; Interaction behavior; Multi-modal; Multi-modal evaluation; Social initiation; Social interactions; Spectra's; Diseases","","","This work has been funded by Spanish Ministry of Economy and Competitiveness under the Maria de Maeztu Units of Excellence Program (MDM-2015-0502).","Bauminger-Zviely, Nirit, The Facilitation of Social-Emotional Understanding and Social Interaction in High-Functioning Children with Autism: Intervention Outcomes, Journal of Autism and Developmental Disorders, 32, 4, pp. 283-298, (2002); Benedek, Mathias, Decomposition of skin conductance data by means of nonnegative deconvolution, Psychophysiology, 47, 4, pp. 647-658, (2010); Bernard-Opitz, Vera, Enhancing Social Problem Solving in Children with Autism and Normal Children Through Computer-Assisted Instruction, Journal of Autism and Developmental Disorders, 31, 4, pp. 377-384, (2001); Bernardini, Sara, ECHOES: An intelligent serious game for fostering social communication in children with autism, Information Sciences, 264, pp. 41-60, (2014); Berthoz, Sylvie, The validity of using self-reports to assess emotion regulation abilities in adults with autism spectrum disorder, European Psychiatry, 20, 3, pp. 291-298, (2005); Betella, Alberto, The affective slider: A digital self-assessment scale for the measurement of human emotions, PLOS ONE, 11, 2, (2016); Does Body Movement Engage You More in Digital Game Play and Why in, (2007); Boettger, Silke, Heart rate variability, QT variability, and electrodermal activity during exercise, Medicine and Science in Sports and Exercise, 42, 3, pp. 443-448, (2010); Borghi, Anna Maria, Embodied cognition and beyond: Acting and sensing the body, Neuropsychologia, 48, 3, pp. 763-773, (2010); Boucsein, Wolfram, Electrodermal activity: Second edition, 9781461411260, pp. 1-618, (2012)","","Springer Science and Business Media Deutschland GmbH","","","","","","13594338","9781614702467","","","English","Article","Final","All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85115379231"
"Washington, P.; Kalantarian, H.; Kent, J.; Husic, A.; Kline, A.; Leblanc, E.; Hou, C.; Mutlu, O.C.; Dunlap, K.; Penev, Y.; Varma, M.; Stockham, N.T.; Chrisman, B.; Paskov, K.; Sun, M.W.; Jung, J.-Y.; Voss, C.; Haber, N.; Wall, D.P.","Washington, Peter Yigitcan (57191498961); Kalantarian, Haik (55820889400); Kent, John (57358005000); Husic, Arman (57220907587); Kline, Aaron (57191505004); Leblanc, Emilie (57218776057); Hou, Cathy (57221872574); Mutlu, Onur Cezmi (16043006700); Dunlap, Kaitlyn L. (57208624014); Penev, Yordan (57218775787); Varma, Maya (57194055814); Stockham, Nathaniel Tyler (57202464121); Chrisman, Brianna Sierra (57207466293); Paskov, Kelley Marie (55991998800); Sun, Min-woo (57202463700); Jung, Jae-yoon (36647501900); Voss, Catalin (57190141053); Haber, Nick (56353182800); Wall, Dennis Paul (7202196193)","57191498961; 55820889400; 57358005000; 57220907587; 57191505004; 57218776057; 57221872574; 16043006700; 57208624014; 57218775787; 57194055814; 57202464121; 57207466293; 55991998800; 57202463700; 36647501900; 57190141053; 56353182800; 7202196193","Improved Digital Therapy for Developmental Pediatrics Using Domain-Specific Artificial Intelligence: Machine Learning Study","2022","JMIR Pediatrics and Parenting","5","2","e26760","","","0","22","10.2196/26760","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128447984&doi=10.2196%2F26760&partnerID=40&md5=c501d6a5ea2a9e480c8aaad03d337b20","Departments of Pediatrics (Systems Medicine) and Biomedical Data Science, Stanford University, Stanford, United States","Washington, Peter Yigitcan, Departments of Pediatrics (Systems Medicine) and Biomedical Data Science, Stanford University, Stanford, United States; Kalantarian, Haik, Departments of Pediatrics (Systems Medicine) and Biomedical Data Science, Stanford University, Stanford, United States; Kent, John, Departments of Pediatrics (Systems Medicine) and Biomedical Data Science, Stanford University, Stanford, United States; Husic, Arman, Departments of Pediatrics (Systems Medicine) and Biomedical Data Science, Stanford University, Stanford, United States; Kline, Aaron, Departments of Pediatrics (Systems Medicine) and Biomedical Data Science, Stanford University, Stanford, United States; Leblanc, Emilie, Departments of Pediatrics (Systems Medicine) and Biomedical Data Science, Stanford University, Stanford, United States; Hou, Cathy, Departments of Pediatrics (Systems Medicine) and Biomedical Data Science, Stanford University, Stanford, United States; Mutlu, Onur Cezmi, Departments of Pediatrics (Systems Medicine) and Biomedical Data Science, Stanford University, Stanford, United States; Dunlap, Kaitlyn L., Departments of Pediatrics (Systems Medicine) and Biomedical Data Science, Stanford University, Stanford, United States; Penev, Yordan, Departments of Pediatrics (Systems Medicine) and Biomedical Data Science, Stanford University, Stanford, United States; Varma, Maya, Departments of Pediatrics (Systems Medicine) and Biomedical Data Science, Stanford University, Stanford, United States; Stockham, Nathaniel Tyler, Departments of Pediatrics (Systems Medicine) and Biomedical Data Science, Stanford University, Stanford, United States; Chrisman, Brianna Sierra, Departments of Pediatrics (Systems Medicine) and Biomedical Data Science, Stanford University, Stanford, United States; Paskov, Kelley Marie, Departments of Pediatrics (Systems Medicine) and Biomedical Data Science, Stanford University, Stanford, United States; Sun, Min-woo, Departments of Pediatrics (Systems Medicine) and Biomedical Data Science, Stanford University, Stanford, United States; Jung, Jae-yoon, Departments of Pediatrics (Systems Medicine) and Biomedical Data Science, Stanford University, Stanford, United States; Voss, Catalin, Departments of Pediatrics (Systems Medicine) and Biomedical Data Science, Stanford University, Stanford, United States; Haber, Nick, Departments of Pediatrics (Systems Medicine) and Biomedical Data Science, Stanford University, Stanford, United States; Wall, Dennis Paul, Departments of Pediatrics (Systems Medicine) and Biomedical Data Science, Stanford University, Stanford, United States","Background: Automated emotion classification could aid those who struggle to recognize emotions, including children with developmental behavioral conditions such as autism. However, most computer vision emotion recognition models are trained on adult emotion and therefore underperform when applied to child faces. Objective: We designed a strategy to gamify the collection and labeling of child emotion-enriched images to boost the performance of automatic child emotion recognition models to a level closer to what will be needed for digital health care approaches. Methods: We leveraged our prototype therapeutic smartphone game, GuessWhat, which was designed in large part for children with developmental and behavioral conditions, to gamify the secure collection of video data of children expressing a variety of emotions prompted by the game. Independently, we created a secure web interface to gamify the human labeling effort, called HollywoodSquares, tailored for use by any qualified labeler. We gathered and labeled 2155 videos, 39,968 emotion frames, and 106,001 labels on all images. With this drastically expanded pediatric emotion-centric database (>30 times larger than existing public pediatric emotion data sets), we trained a convolutional neural network (CNN) computer vision classifier of happy, sad, surprised, fearful, angry, disgust, and neutral expressions evoked by children. Results: The classifier achieved a 66.9% balanced accuracy and 67.4% F1-score on the entirety of the Child Affective Facial Expression (CAFE) as well as a 79.1% balanced accuracy and 78% F1-score on CAFE Subset A, a subset containing at least 60% human agreement on emotions labels. This performance is at least 10% higher than all previously developed classifiers evaluated against CAFE, the best of which reached a 56% balanced accuracy even when combining ""anger"" and ""disgust"" into a single class. Conclusions: This work validates that mobile games designed for pediatric therapies can generate high volumes of domain-relevant data sets to train state-of-the-art classifiers to perform tasks helpful to precision health efforts. © 2022 Elsevier B.V., All rights reserved.","affective computing; artificial intelligence; autism spectrum disorder; computer vision; convolutional neural network; digital therapy; emotion recognition; machine learning; mobile health; pediatrics","","","","We would like to acknowledge all the nine high school and undergraduate emotion annotators: Natalie Park, Chris Harjadi, Meagan Tsou, Belle Bankston, Hadley Daniels, Sky Ng-Thow-Hing, Bess Olshen, Courtney McCormick, and Jennifer Yu. The work was supported in part by funds to DPW from the National Institutes of Health (grants 1R01EB025025-01, 1R01LM013364-01, 1R21HD091500-01, and 1R01LM013083), the National Science Foundation (Award 2014232), The Hartwell Foundation, Bill and Melinda Gates Foundation, Coulter Foundation, Lucile Packard Foundation, Auxiliaries Endowment, the Islamic Development Bank Transform Fund, the Weston Havens Foundation, and program grants from Stanford’s Human-Centered Artificial Intelligence Program, Precision Health and Integrated Diagnostics Center, Beckman Center, Bio-X Center, Predictives and Diagnostics Accelerator, Spectrum, Spark Program in Translational Research, MediaX, and from the Wu Tsai Neurosciences Institute's Neuroscience:Translate Program. We also acknowledge generous support from David Orr, Imma Calvo, Bobby Dekesyer, and Peter Sullivan. PW would like to acknowledge support from Mr. Schroeder and the Stanford Interdisciplinary Graduate Fellowship (SIGF) as the Schroeder Family Goldman Sachs Graduate Fellow.","Harms, Madeline B., Facial emotion recognition in autism spectrum disorders: A review of behavioral and neuroimaging studies, Neuropsychology Review, 20, 3, pp. 290-322, (2010); Hobson, R. Peter, Emotion recognition in autism: Coordinating faces and voices, Psychological Medicine, 18, 4, pp. 911-923, (1988); Rieffe, Carolien, Emotion regulation and internalizing symptoms in children with autism spectrum disorders, Autism, 15, 6, pp. 655-670, (2011); De Carolis, Berardina Nadja, Cognitive emotions recognition in e-learning: Exploring the role of age differences and personality traits, Advances in Intelligent Systems and Computing, 1007, pp. 97-104, (2020); De Carolis, Berardina Nadja, Socio-affective technologies [SI 1156 T]: Insights from psychological and computational sciences, Multimedia Tools and Applications, 79, 47-48, pp. 35779-35783, (2020); Franzoni, Valentina, Enhancing mouth-based emotion recognition using transfer learning, Sensors, 20, 18, pp. 1-15, (2020); Daniels, Jena, Feasibility Testing of a Wearable Behavioral Aid for Social Learning in Children with Autism, Applied Clinical Informatics, 9, 1, pp. 129-140, (2018); Daniels, Jena, Exploratory study examining the at-home feasibility of a wearable tool for social-affective learning in children with autism, npj Digital Medicine, 1, 1, (2018); Haber, Nick, A practical approach to real-time neutral feature subtraction for facial expression recognition, (2016); Haber, Nick, Making emotions transparent: Google Glass helps autistic kids understand facial expressions through augmented-reaiity therapy, IEEE Spectrum, 57, 4, pp. 46-52, (2020)","","JMIR Publications Inc.","","","","","","25616722","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85128447984"
"Popescu, A.-L.; Popescu, N.; Dobre, C.; Apostol, E.-S.; Popescu, D.","Popescu, Aura Loredana (57221333045); Popescu, Nirvana Alina (9240753400); Dobre, C. Mihai (24437773100); Apostol, Elena Simona (55365937600); Popescu, Decebal Gheorghe (36901948800)","57221333045; 9240753400; 24437773100; 55365937600; 36901948800","‘IoT and AI-Based Application for Automatic Interpretation of the Affective State of Children Diagnosed with Autism","2022","Sensors","22","7","2528","","","0","9","10.3390/s22072528","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126731643&doi=10.3390%2Fs22072528&partnerID=40&md5=084360f43ee2a9d5ee63786a228cb3ee","Department of Computer Science, University Politehnica of Bucharest, Bucharest, Romania","Popescu, Aura Loredana, Department of Computer Science, University Politehnica of Bucharest, Bucharest, Romania; Popescu, Nirvana Alina, Department of Computer Science, University Politehnica of Bucharest, Bucharest, Romania; Dobre, C. Mihai, Department of Computer Science, University Politehnica of Bucharest, Bucharest, Romania; Apostol, Elena Simona, Department of Computer Science, University Politehnica of Bucharest, Bucharest, Romania; Popescu, Decebal Gheorghe, Department of Computer Science, University Politehnica of Bucharest, Bucharest, Romania","In the context in which it was demonstrated that humanoid robots are efficient in helping children diagnosed with autism in exploring their affective state, this paper underlines and proves the efficiency of a previously developed machine learning-based mobile application called Pan-daSays, which was improved and integrated with an Alpha 1 Pro robot, and discusses performance evaluations using deep convolutional neural networks and residual neural networks. The model trained with MobileNet convolutional neural network had an accuracy of 56.25%, performing better than ResNet50 and VGG16. A strategy for commanding the Alpha 1 Pro robot without its native application was also established and a robot module was developed that includes the communication protocols with the application PandaSays. The output of the machine learning algorithm involved in PandaSays is sent to the humanoid robot to execute some actions as singing, dancing, and so on. Alpha 1 Pro has its own programming language—Blockly—and, in order to give the robot specific commands, Bluetooth programming is used, with the help of a Raspberry Pi. Therefore, the robot motions can be controlled based on the corresponding protocols. The tests have proved the robustness of the whole solution. © 2022 Elsevier B.V., All rights reserved.","Autism spectrum disorder; Drawing interpretation; Humanoid robots; Neural networks","Anthropomorphic robots; Convolutional neural networks; Deep neural networks; Diseases; Internet of things; Learning algorithms; Robot programming; Affective state; Autism spectrum disorders; Communications protocols; Convolutional neural network; Drawing interpretation; Humanoid robot; Machine learning algorithms; Mobile applications; Neural-networks; Performances evaluation; Convolution; autism; child; emotion; human; machine learning; robotics; Autistic Disorder; Child; Emotions; Humans; Machine Learning; Neural Networks, Computer; Robotics","","","","Autism and Health A Special Report by Autism Speaks; Villano, Michael A., DOMER: A Wizard of Oz interface for using interactive robots to scaffold social skills for children with autism spectrum disorders, pp. 279-280, (2011); Shamsuddin, Syamimi A., Initial response of autistic children in human-robot interaction therapy with humanoid robot NAO, pp. 188-193, (2012); Krithiga, R., Socially assistive robot for children with autism spectrum disorder, (2019); Alcorn, Alyssa M., Educators' Views on Using Humanoid Robots With Autistic Learners in Special Education Settings in England, Frontiers in Robotics and AI, 6, (2019); Documentation Regarding Robots Used for Helping Children with Autism; undefined; Documentation Regarding Kaspar Robot; Miskam, Mohd Azfar, Humanoid robot NAO as a teaching tool of emotion recognition for children with autism using the Android app, (2014); Beer, Jenay M., Robot assisted music therapy: A case study with children diagnosed with autism, ACM/IEEE International Conference on Human-Robot Interaction, 2016-April, pp. 419-420, (2016)","","MDPI","","","","","","14248220","","","35408139","English","Article","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85126731643"
"Wetcho, S.; Nasongkhla, J.","Wetcho, Suthanit (57208164571); Nasongkhla, Jaitip (56896058300)","57208164571; 56896058300","An Investigation of Pre-Service Teachers Using Mobile and Wearable Devices for Emotion Recognition and Social Sharing of Emotion to Support Emotion Regulation in mCSCL Environments","2022","Contemporary Educational Technology","14","2","ep359","","","0","14","10.30935/cedtech/11668","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126272531&doi=10.30935%2Fcedtech%2F11668&partnerID=40&md5=a26622d641f902f55f3193eaa3ec638c","Faculty of Education, Chulalongkorn University, Bangkok, Thailand","Wetcho, Suthanit, Faculty of Education, Chulalongkorn University, Bangkok, Thailand; Nasongkhla, Jaitip, Faculty of Education, Chulalongkorn University, Bangkok, Thailand","In the era of a workforce driven by automation and artificial intelligence, social and emotional skills are becoming increasingly relevant to online learning environments. Since social-emotional learning may be defined as a vital component of the learning process in professional instructional design practices, online learners not only need to develop the ability to apply their knowledge, attitudes, and skills but also to understand and manage their emotions. In which setting and achieving positive goals through social interaction, sharing feelings, and developing empathy for others can help with the process. This paper outlines the possibility of using emotion recognition, and social sharing of emotion techniques to support the regulation of emotion in pre-service teacher education. This study aimed to investigate pre-service teachers’ emotion recognition tools acquired by emotion tracker and physiological signals based on their perceptions (without a concrete experience and knowledge). Moreover, the predictive ability was examined along with the relationships between emotion recognition, social sharing of emotion, and emotion regulation. Finally, we investigated emotion adjustment techniques that can be adapted into mobile computer-supported collaborative learning (mCSCL). In this study, 183 pre-service teachers from three different teacher-education institutions in Thailand, were voluntarily participated based on convenience sampling. The results of a self-report via online survey revealed that most pre-service teachers own at least one of the mobile technologies e.g., smartphones, tablets, or laptops. However, there is an increasing number of additional gadgets and wearable devices like EarPods and smartwatches. At the current time, it is nearly impossible to use of the IoT and other wearable devices. According to their subjective impressions in which corresponded to emotion recognition in the scientific literature, Heart rate (HR) and Heart rate variability (HRV) have recognized the most possibilities for emotion detection among physiological signals. Regarding regression analysis, the two-predictor models of emotion recognition and the social sharing of emotion were also able to account for 31% of the variance in emotion regulation, p<.001, R2=.31, and 95% CI [.70, .77]. In addition, the mCSCL applications and the importance of these variables in different collaboration levels are also discussed. © 2022 Elsevier B.V., All rights reserved.","Emotion recognition; Emotion regulation; MCSCL; Social emotional learning; Social sharing of emotion","","","","Author contributions: SW: Original draft, formal analysis, writing, review, and editing; JN-S: Review and editing. All authors approve final version of the article. Author notes: This work was under the Disruptive Innovation Technology in Education, Chulalongkorn University. The research results were previously presented at AECT 2020 Virtual Convention. Funding: This research was financially supported by the Royal Golden Jubilee PhD Programme of the Thailand Research Fund (TRF) (Grant No. PHD/0210/2560). Declaration of interest: Authors declare no competing interest. Data availability: Data generated or analysed during this study are available from the authors on request.","Abramson, Lior, Social interaction context shapes emotion recognition through body language, not facial expressions., Emotion, 21, 3, pp. 557-568, (2021); Augustsson, Gunnar, Web 2.0, pedagogical support for reflexive and emotional social interaction among Swedish students, Internet and Higher Education, 13, 4, pp. 197-205, (2010); Bucich, Micaela, Emotional Intelligence and Day-To-Day Emotion Regulation Processes: Examining Motives for Social Sharing, Personality and Individual Differences, 137, pp. 22-26, (2019); Butler, Emily A., Emotion and Emotion Regulation: Integrating Individual and Social Levels of Analysis, Emotion Review, 1, 1, pp. 86-87, (2009); Carstensen, Laura L., Social and emotional patterns in adulthood: support for socioemotional selectivity theory., Psychology and Aging, 7, 3, pp. 331-338, (1992); Carstensen, Laura L., Socioemotional selectivity theory and the regulation of emotion in the second half of life, Motivation and Emotion, 27, 2, pp. 103-123, (2003); Castillo, José Carlos, A framework for recognizing and regulating emotions in the elderly, Lecture Notes in Computer Science, 8868, pp. 320-327, (2014); Connolly, Hannah L., Emotion recognition ability: Evidence for a supramodal factor and its links to social cognition, Cognition, 197, (2020); Cowie, Roddy, Emotion recognition in human-computer interaction, IEEE Signal Processing Magazine, 18, 1, pp. 32-80, (2001); Dores, Artemisa Rocha, Recognizing emotions through facial expressions: A largescale experimental study, International Journal of Environmental Research and Public Health, 17, 20, pp. 1-13, (2020)","","Bastas","","","","","","1309517X","","","","English","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85126272531"
"Amiriparian, S.; Hübner, T.; Karas, V.; Gerczuk, M.; Ottl, S.; Schuller, B.W.","Amiriparian, Shahin (56964296000); Hübner, Tobias (57223824436); Karas, Vincent (57212027280); Gerczuk, Maurice (57200079240); Ottl, Sandra (57200084423); Schuller, Björn W. (6603767415)","56964296000; 57223824436; 57212027280; 57200079240; 57200084423; 6603767415","DeepSpectrumLite: A Power-Efficient Transfer Learning Framework for Embedded Speech and Audio Processing From Decentralized Data","2022","Frontiers in Artificial Intelligence","5","","856232","","","0","26","10.3389/frai.2022.856232","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127963258&doi=10.3389%2Ffrai.2022.856232&partnerID=40&md5=a4a38e294f61c4318dece03e9a9121e6","Chair of Embedded Intelligence for Health Care and Wellbeing, Universität Augsburg, Augsburg, Germany; and Music, Imperial College London, London, United Kingdom","Amiriparian, Shahin, Chair of Embedded Intelligence for Health Care and Wellbeing, Universität Augsburg, Augsburg, Germany; Hübner, Tobias, Chair of Embedded Intelligence for Health Care and Wellbeing, Universität Augsburg, Augsburg, Germany; Karas, Vincent, Chair of Embedded Intelligence for Health Care and Wellbeing, Universität Augsburg, Augsburg, Germany; Gerczuk, Maurice, Chair of Embedded Intelligence for Health Care and Wellbeing, Universität Augsburg, Augsburg, Germany; Ottl, Sandra, Chair of Embedded Intelligence for Health Care and Wellbeing, Universität Augsburg, Augsburg, Germany; Schuller, Björn W., Chair of Embedded Intelligence for Health Care and Wellbeing, Universität Augsburg, Augsburg, Germany, and Music, Imperial College London, London, United Kingdom","Deep neural speech and audio processing systems have a large number of trainable parameters, a relatively complex architecture, and require a vast amount of training data and computational power. These constraints make it more challenging to integrate such systems into embedded devices and utilize them for real-time, real-world applications. We tackle these limitations by introducing DeepSpectrumLite, an open-source, lightweight transfer learning framework for on-device speech and audio recognition using pre-trained image Convolutional Neural Networks (CNNs). The framework creates and augments Mel spectrogram plots on the fly from raw audio signals which are then used to finetune specific pre-trained CNNs for the target classification task. Subsequently, the whole pipeline can be run in real-time with a mean inference lag of 242.0 ms when a DenseNet121 model is used on a consumer-grade Motorola moto e7 plus smartphone. DeepSpectrumLite operates decentralized, eliminating the need for data upload for further processing. We demonstrate the suitability of the proposed transfer learning approach for embedded audio signal processing by obtaining state-of-the-art results on a set of paralinguistic and general audio tasks, including speech and music emotion recognition, social signal processing, COVID-19 cough and COVID-19 speech analysis, and snore sound classification. We provide an extensive command-line interface for users and developers which is comprehensively documented and publicly available at https://github.com/DeepSpectrum/DeepSpectrumLite. © 2022 Elsevier B.V., All rights reserved.","audio processing; computational paralinguistics; deep spectrum; embedded devices; transfer learning","","","","This study presented has received funding from the BMW Group. Further, we acknowledge funding from the DFG's Reinhart Koselleck project No. 442218748 (AUDI0NOMOUS) and the DFG project No. 421613952 (ParaStiChaD).","Deep Representation Learning Techniques for Audio Signal Processing, (2019); Amiriparian, Shahin, Are You Playing a Shooter Again?!' Deep Representation Learning for Audio-Based Video Game Genre Recognition, IEEE Transactions on Games, 12, 2, pp. 145-154, (2020); Amiriparian, Shahin, Sentiment analysis using image-based deep spectrum features, 2018-January, pp. 26-29, (2017); Proc of the Dcase 2017 Workshop, (2017); Amiriparian, Shahin, Snore sound classification using image-based deep spectrum features, Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH, 2017-August, pp. 3512-3516, (2017); Amiriparian, Shahin, Bag-of-Deep-Features: Noise-Robust Deep Feature Representations for Audio Analysis, Proceedings of the International Joint Conference on Neural Networks, 2018-July, (2018); Amiriparian, Shahin, Towards cross-modal pre-training and learning tempo-spatial characteristics for audio recognition with convolutional and recurrent neural networks, Eurasip Journal on Audio, Speech, and Music Processing, 2020, 1, (2020); Baird, Alice E., Can Deep Generative Audio be Emotional? Towards an Approach for Personalised Emotional Audio Generation, (2019); Bartl-Pokorny, Katrin D., The voice of COVID-19: Acoustic correlates of infection in sustained vowels, Journal of the Acoustical Society of America, 149, 6, pp. 4377-4383, (2021); Busso, Carlos, IEMOCAP: Interactive emotional dyadic motion capture database, Language Resources and Evaluation, 42, 4, pp. 335-359, (2008)","","Frontiers Media S.A.","","","","","","26248212","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85127963258"
"Varma, M.; Washington, P.; Chrisman, B.; Kline, A.; Leblanc, E.; Paskov, K.; Stockham, N.T.; Jung, J.-Y.; Sun, M.W.; Wall, D.P.","Varma, Maya (57194055814); Washington, Peter Yigitcan (57191498961); Chrisman, Brianna Sierra (57207466293); Kline, Aaron (57191505004); Leblanc, Emilie (57218776057); Paskov, Kelley Marie (55991998800); Stockham, Nathaniel Tyler (57202464121); Jung, Jae-yoon (36647501900); Sun, Min-woo (57202463700); Wall, Dennis Paul (7202196193)","57194055814; 57191498961; 57207466293; 57191505004; 57218776057; 55991998800; 57202464121; 36647501900; 57202463700; 7202196193","Identification of Social Engagement Indicators Associated With Autism Spectrum Disorder Using a Game-Based Mobile App: Comparative Study of Gaze Fixation and Visual Scanning Methods","2022","Journal of Medical Internet Research","24","2","e31830","","","0","26","10.2196/31830","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124636673&doi=10.2196%2F31830&partnerID=40&md5=ed28d10a955f730e8e8ff02249b91ff4","Stanford Engineering, Stanford, United States; Department of Bioengineering, Stanford, United States; Department of Biomedical Data Science, Stanford University, Stanford, United States; Department of Biomedical Data Science, Stanford University, Stanford, United States; Department of Neuroscience, Stanford University, Stanford, United States","Varma, Maya, Stanford Engineering, Stanford, United States; Washington, Peter Yigitcan, Department of Bioengineering, Stanford, United States; Chrisman, Brianna Sierra, Department of Bioengineering, Stanford, United States; Kline, Aaron, Department of Biomedical Data Science, Stanford University, Stanford, United States; Leblanc, Emilie, Department of Biomedical Data Science, Stanford University, Stanford, United States; Paskov, Kelley Marie, Department of Biomedical Data Science, Stanford University, Stanford, United States; Stockham, Nathaniel Tyler, Department of Neuroscience, Stanford University, Stanford, United States; Jung, Jae-yoon, Department of Biomedical Data Science, Stanford University, Stanford, United States; Sun, Min-woo, Department of Biomedical Data Science, Stanford University, Stanford, United States; Wall, Dennis Paul, Department of Biomedical Data Science, Stanford University, Stanford, United States","Background: Autism spectrum disorder (ASD) is a widespread neurodevelopmental condition with a range of potential causes and symptoms. Standard diagnostic mechanisms for ASD, which involve lengthy parent questionnaires and clinical observation, often result in long waiting times for results. Recent advances in computer vision and mobile technology hold potential for speeding up the diagnostic process by enabling computational analysis of behavioral and social impairments from home videos. Such techniques can improve objectivity and contribute quantitatively to the diagnostic process. Objective: In this work, we evaluate whether home videos collected from a game-based mobile app can be used to provide diagnostic insights into ASD. To the best of our knowledge, this is the first study attempting to identify potential social indicators of ASD from mobile phone videos without the use of eye-tracking hardware, manual annotations, and structured scenarios or clinical environments. Methods: Here, we used a mobile health app to collect over 11 hours of video footage depicting 95 children engaged in gameplay in a natural home environment. We used automated data set annotations to analyze two social indicators that have previously been shown to differ between children with ASD and their neurotypical (NT) peers: (1) gaze fixation patterns, which represent regions of an individual’s visual focus and (2) visual scanning methods, which refer to the ways in which individuals scan their surrounding environment. We compared the gaze fixation and visual scanning methods used by children during a 90-second gameplay video to identify statistically significant differences between the 2 cohorts; we then trained a long short-term memory (LSTM) neural network to determine if gaze indicators could be predictive of ASD. Results: Our results show that gaze fixation patterns differ between the 2 cohorts; specifically, we could identify 1 statistically significant region of fixation (P<.001). In addition, we also demonstrate that there are unique visual scanning patterns that exist for individuals with ASD when compared to NT children (P<.001). A deep learning model trained on coarse gaze fixation annotations demonstrates mild predictive power in identifying ASD. Conclusions: Ultimately, our study demonstrates that heterogeneous video data sets collected from mobile devices hold potential for quantifying visual patterns and providing insights into ASD. We show the importance of automated labeling techniques in generating large-scale data sets while simultaneously preserving the privacy of participants, and we demonstrate that specific social engagement indicators associated with ASD can be identified and characterized using such data. © 2022 Elsevier B.V., All rights reserved.","App; Autism; Autism spectrum disorder; Computer vision; Diagnostic; Engagement; Gaming; Gaze; Insight; Mobile diagnostics; Mobile health; Pattern; Pattern recognition; Social phenotyping; Video; Vision","accuracy; algorithm; Article; autism; child; comparative study; computer vision; controlled study; deep learning; diagnosis; eye tracking; female; game; gaze; home environment; human; long short term memory network; major clinical study; male; mobile application; social interaction; videorecording; eye fixation; personal digital assistant; social participation; Autism Spectrum Disorder; Child; Computers, Handheld; Fixation, Ocular; Humans; Mobile Applications; Social Participation","","","This work was supported in part by funds to DPW from the National Institutes of Health (1R01EB025025-01, 1R01LM013364-01, 1R21HD091500-01, 1R01LM013083), the National Science Foundation (Award 2014232), The Hartwell Foundation, Bill and Melinda Gates Foundation, Coulter Foundation, Lucile Packard Foundation, Auxiliaries Endowment, the ISDB Transform Fund, the Weston Havens Foundation, and program grants from Stanford\u2019s Human Centered Artificial Intelligence Program, Precision Health and Integrated Diagnostics Center, Beckman Center, Bio-X Center, Predictives and Diagnostics Accelerator, Spectrum, Spark Program in Translational Research, MediaX, and from the Wu Tsai Neurosciences Institute's Neuroscience: Translate Program. We also acknowledge generous support from David Orr, Imma Calvo, Bobby Dekesyer and Peter Sullivan. MV is supported by the Knight-Hennessy Scholars program at Stanford University and the National Defense Science and Engineering Graduate (NDSEG) Fellowship. PW would like to acknowledge support from Mr. Schroeder and the Stanford Interdisciplinary Graduate Fellowship (SIGF) as the Schroeder Family Goldman Sachs Graduate Fellow.","Fombonne, Eric J., Editorial: The rising prevalence of autism, Journal of Child Psychology and Psychiatry and Allied Disciplines, 59, 7, pp. 717-720, (2018); Maenner, Matthew J., Prevalence of autism spectrum disorder among children aged 8 Years-Autism and developmental disabilities monitoring network, 11 Sites, United States, 2016, MMWR Surveillance Summaries, 69, 4, pp. 1-12, (2020); Bisgaier, Joanna, Access to autism evaluation appointments with developmental-behavioral and neurodevelopmental subspecialists, Archives of Pediatrics and Adolescent Medicine, 165, 7, pp. 673-674, (2011); Gordon-Lipkin, Eliza M., Whittling Down the Wait Time: Exploring Models to Minimize the Delay from Initial Concern to Diagnosis and Treatment of Autism Spectrum Disorder, Pediatric Clinics of North America, 63, 5, pp. 851-859, (2016); Crane, Laura, Experiences of autism diagnosis: A survey of over 1000 parents in the United Kingdom, Autism, 20, 2, pp. 153-162, (2016); Washington, Peter Yigitcan, Data-Driven Diagnostics and the Potential of Mobile Artificial Intelligence for Digital Therapeutic Phenotyping in Computational Psychiatry, Biological Psychiatry: Cognitive Neuroscience and Neuroimaging, 5, 8, pp. 759-769, (2020); Kalantarian, Haik, Guess What?: Towards Understanding Autism from Structured Video Using Facial Affect, Journal of Healthcare Informatics Research, 3, 1, pp. 43-66, (2019); Kalantarian, Haik, A Mobile Game for Automatic Emotion-Labeling of Images, IEEE Transactions on Games, 12, 2, pp. 213-218, (2020); Kalantarian, Haik, A gamified mobile system for crowdsourcing video for autism research, pp. 350-352, (2018); Kalantarian, Haik, The performance of emotion classifiers for children with parent-reported autism: Quantitative feasibility study, JMIR Mental Health, 7, 4, (2020)","","JMIR Publications Inc.","","","","","","14388871","","","35166683","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85124636673"
"Lee, Y.-S.; Park, W.-H.","Lee, Youngshin (57487822500); Park, Wonhyung (59797846400)","57487822500; 59797846400","Diagnosis of Depressive Disorder Model on Facial Expression Based on Fast R-CNN","2022","Diagnostics","12","2","317","","","0","43","10.3390/diagnostics12020317","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124089266&doi=10.3390%2Fdiagnostics12020317&partnerID=40&md5=c2b410688b64cb10ef37a1dfaf9827b4","Department of Nursing, Far East University, Eumseong-gun, South Korea; Department of Information Security Protection, Sangmyung University, Jongno-gu, South Korea","Lee, Youngshin, Department of Nursing, Far East University, Eumseong-gun, South Korea; Park, Wonhyung, Department of Information Security Protection, Sangmyung University, Jongno-gu, South Korea","This study examines related literature to propose a model based on artificial intelligence (AI), that can assist in the diagnosis of depressive disorder. Depressive disorder can be diagnosed through a self-report questionnaire, but it is necessary to check the mood and confirm the consistency of subjective and objective descriptions. Smartphone-based assistance in diagnosing depressive disorders can quickly lead to their identification and provide data for intervention provision. Through fast region-based convolutional neural networks (R-CNN), a deep learning method that recognizes vector-based information, a model to assist in the diagnosis of depressive disorder can be devised by checking the position change of the eyes and lips, and guessing emotions based on accumulated photos of the participants who will repeatedly participate in the diagnosis of depressive disorder. © 2024 Elsevier B.V., All rights reserved.","Deep learning; Depressive disorder; Diagnosis; Facial expression; Fast R-CNN","Article; artificial intelligence; convolutional neural network; deep learning; depression; emotion; emotional appraisal; facial expression; fast region convolutional neural network; human; learning; mental health; mental health center; mood change; Patient Health Questionnaire 9; psychology; questionnaire","","","","Graham, Sarah Anne, Artificial Intelligence for Mental Health and Mental Illnesses: an Overview, Current Psychiatry Reports, 21, 11, (2019); Freeman, Daniel H., Virtual reality in the assessment, understanding, and treatment of mental health disorders, Psychological Medicine, 47, 14, pp. 2393-2400, (2017); undefined; Gromatsky, Molly A., Ecological momentary assessment (EMA) of mental health outcomes in veterans and servicemembers: A scoping review, Psychiatry Research, 292, (2020); Robinaugh, Donald J., Towards a precision psychiatry approach to anxiety disorders with ecological momentary assessment: The example of panic disorder, General Psychiatry, 33, 1, (2020); Triantafillou, Sofia, Relationship between sleep quality and mood: Ecological momentary assessment study, JMIR Mental Health, 6, 3, (2019); Versluis, Anke, Changing mental health and positive psychological well-being using ecological momentary interventions: A systematic review and meta-analysis, Journal of Medical Internet Research, 18, 6, (2016); Schueller, Stephen M., Ecological momentary interventions for depression and anxiety, Depression and Anxiety, 34, 6, pp. 540-545, (2017); Hanssen, Esther, An ecological momentary intervention incorporating personalised feedback to improve symptoms and social functioning in schizophrenia spectrum disorders, Psychiatry Research, 284, (2020); undefined","","Multidisciplinary Digital Publishing Institute (MDPI)","","","","","","20754418","","","","English","Article","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85124089266"
"Izountar, Y.; Benbelkacem, S.; Otmane, S.; Khababa, A.; Masmoudi, M.; Zenati, N.","Izountar, Yousra (57212382487); Benbelkacem, Samir (24528529600); Otmane, Samir (13808173500); Khababa, Abdallah (10840008500); Masmoudi, Mostefa (57218657017); Zenati, Nadia (24529284800)","57212382487; 24528529600; 13808173500; 10840008500; 57218657017; 24529284800","VR-PEER: A Personalized Exer-Game Platform Based on Emotion Recognition","2022","Electronics (Switzerland)","11","3","455","","","0","21","10.3390/electronics11030455","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123820686&doi=10.3390%2Felectronics11030455&partnerID=40&md5=a4b66fddcf296bf2f2522054f2acb53c","Department of Computer Science, Université Ferhat Abbas Sétif 1, Setif, Algeria; Equipe Interaction Homme Système Réalité Virtuelle & Augmentée-IRVA, Centre de Développement des Technologies Avancées, Algiers, Algeria; Informatique, BioInformatique, Systèmes Complexes, Evry, France","Izountar, Yousra, Department of Computer Science, Université Ferhat Abbas Sétif 1, Setif, Algeria, Equipe Interaction Homme Système Réalité Virtuelle & Augmentée-IRVA, Centre de Développement des Technologies Avancées, Algiers, Algeria; Benbelkacem, Samir, Equipe Interaction Homme Système Réalité Virtuelle & Augmentée-IRVA, Centre de Développement des Technologies Avancées, Algiers, Algeria; Otmane, Samir, Informatique, BioInformatique, Systèmes Complexes, Evry, France; Khababa, Abdallah, Department of Computer Science, Université Ferhat Abbas Sétif 1, Setif, Algeria; Masmoudi, Mostefa, Equipe Interaction Homme Système Réalité Virtuelle & Augmentée-IRVA, Centre de Développement des Technologies Avancées, Algiers, Algeria; Zenati, Nadia, Equipe Interaction Homme Système Réalité Virtuelle & Augmentée-IRVA, Centre de Développement des Technologies Avancées, Algiers, Algeria","Motor rehabilitation exercises require recurrent repetitions to enhance patients’ gestures. However, these repetitive gestures usually decrease the patients’ motivation and stress them. Virtual Reality (VR) exer-games (serious games in general) could be an alternative solution to address the problem. This innovative technology encourages patients to train different gestures with less effort since they are totally immersed in an easy to play exer-game. Despite this evolution, patients, with available exer-games, still suffer in performing their gestures correctly without pain. The developed applications do not consider the patients psychological states when playing an exer-game. Therefore, we believe that is necessary to develop personalized and adaptive exer-games that take into consideration the patients’ emotions during rehabilitation exercises. This paper proposed a VR-PEER adaptive exer-game system based on emotion recognition. The platform contain three main modules: (1) computing and interpretation module, (2) emotion recognition module, (3) adaptation module. Furthermore, a virtual reality-based serious game is developed as a case study, that uses updated facial expression data and provides dynamically the patient’s appropriate game to play during rehabilitation exercises. An experimental study has been conducted on fifteen subjects who expressed the usefulness of the proposed system in motor rehabilitation process. © 2022 Elsevier B.V., All rights reserved.","3D Exer-game; Adaptation; CNN; Deep learning; Emotion recognition; Facial emotion recognition (FER); Rehabilitation; Virtual reality (VR)","","","","Funding: We would like to thank the IBISC Laboratory and the Ile de France region and CDTA research center for their funding support. This material is based upon work supported by the FEDER CESAAR-AVC project under Grant N◦ IF 001 1053.","Loading, (2012); Valenzuela, Trinidad, Adherence to Technology-Based Exercise Programs in Older Adults: A Systematic Review, Journal of Geriatric Physical Therapy, 41, 1, pp. 49-61, (2018); Meekes, Wytske M.A., Motivational determinants of exergame participation for older people in assisted living facilities: Mixed-methods study, Journal of Medical Internet Research, 19, 7, (2017); Alhagbani, Abdulrhman, Home-Based Exergames for Older Adults Balance and Falls Risk: A Systematic Review, Physical and Occupational Therapy in Geriatrics, 39, 3, pp. 241-257, (2021); Piech, Joanna, Virtual reality rehabilitation and exergames-physical and psychological impact on fall prevention among the elderly-a literature review, Applied Sciences (Switzerland), 11, 9, (2021); Cugusi, Lucia, Exergaming for Quality of Life in Persons Living with Chronic Diseases: A Systematic Review and Meta-analysis, PM and R, 13, 7, pp. 756-780, (2021); Pachêco, Thaiana Barbosa Ferreira, Effectiveness of exergames for improving mobility and balance in older adults: A systematic review and meta-analysis, Systematic Reviews, 9, 1, (2020); Yoo, Soojeong, VRun: Running-in-place virtual reality exergame, pp. 562-566, (2016); Mütterlein, Joschka, The three pillars of virtual reality? Investigating the roles of immersion, presence, and interactivity, Proceedings of the Annual Hawaii International Conference on System Sciences, 2018-January, pp. 1407-1415, (2018); Kappen, Dennis L., Older Adults’ Physical Activity and Exergames: A Systematic Review, International Journal of Human-Computer Interaction, 35, 2, pp. 140-167, (2019)","","MDPI","","","","","","20799292","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85123820686"
"Teja Chavali, S.; Tej Kandavalli, C.; Sugash, T.M.; Subramani, R.","Teja Chavali, Surya (57218396445); Tej Kandavalli, Charan (57639435200); Sugash, T. M. (57732723200); Subramani, R. (57144132100)","57218396445; 57639435200; 57732723200; 57144132100","Smart Facial Emotion Recognition with Gender and Age Factor Estimation","2022","Procedia Computer Science","218","","","113","123","0","21","10.1016/j.procs.2022.12.407","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163687390&doi=10.1016%2Fj.procs.2022.12.407&partnerID=40&md5=035904d0186731e79ff8bda35a614d1b","Amrita Vishwa Vidyapeetham University, Bangalore, Bengaluru, India; Department of Mathematics, Christ University, Bengaluru, India","Teja Chavali, Surya, Amrita Vishwa Vidyapeetham University, Bangalore, Bengaluru, India; Tej Kandavalli, Charan, Amrita Vishwa Vidyapeetham University, Bangalore, Bengaluru, India; Sugash, T. M., Department of Mathematics, Christ University, Bengaluru, India; Subramani, R., Department of Mathematics, Christ University, Bengaluru, India","Human-Computer Interaction (HCI) in an intelligent way, which aims at creating scalable and flexible solutions. Big tech firms and businesses believe in the success of HCI as it allows them to profit from on-demand technology and infrastructure for information-centric applications without having to use public clouds. Because of its capacity to imitate human coding abilities, facial expression recognition and software-based facial expression identification systems are crucial. This paper proposes a system of recognizing the emotional condition of humans, given a facial expression, and conveys two methods of predicting the age and gender factors from human faces. This research also aims in understanding the influences posed by gender and age of humans on their facial expressions. The model can currently detect 7 emotions based on the facial data of a person - (Anger, Disgust, Happy, Fear, Sad, Surprise, and Neutral state). The proposed system is divided into three segments: a.) Gender Detection b.) Age Detection c.) Emotion Recognition. The initial model is created using 2 algorithms - KNN, and SVM. We have also utilized the architectures of some of the deep learning models such as CNN and VGG - 16 pre-trained models (Transfer Learning). The evaluation metrics show the model performance regarding the accuracy of the Recognition system. Future enhancements of this work can include the deployment of the DL and ML model onto an android or a wearable device such as a smartphone or a watch for a real-time use case. © 2023 Elsevier B.V., All rights reserved.","Age recognition; CNN; Emotion recognition; Gender; HCI; KNN; SVM; VGG - 16","Deep learning; Face recognition; Human computer interaction; Speech recognition; Age factors; Age recognition; Emotion recognition; Facial emotions; Facial Expressions; Gender; KNN; On demands; SVM; VGG - 16; Emotion Recognition","","","","Shi-Qing, Zhang, Facial expression recognition based on local binary patterns and local fisher discriminant analysis, WSEAS Transactions on Signal Processing, 8, 1, pp. 21-31, (2012); Mlp Neural Network Based Approach for Facial Expression Analysis, (2016); J Softw Tools, (2008); Boureau, Y. Lan, A theoretical analysis of feature pooling in visual recognition, pp. 111-118, (2010); Yang, Xu, Deep Label Distribution Learning for Apparent Age Estimation, Proceedings of the IEEE International Conference on Computer Vision, 2015-February, pp. 344-350, (2016); Escalera, Sergio, ChaLearn Looking at People 2015: Apparent Age and Cultural Event Recognition Datasets and Results, Proceedings of the IEEE International Conference on Computer Vision, 2015-February, pp. 243-251, (2016); Xiaoxi, Ma, Facial emotion recognition, 2017-January, pp. 77-81, (2017); Huang, Dongyan, Speaker state classification based on fusion of asymmetric simple partial least squares (SIMPLS) and support vector machines, Computer Speech and Language, 28, 2, pp. 392-419, (2014); Teja Chavali, Surya, Grammar Detection for Sentiment Analysis through Improved Viterbi Algorithm, (2022); Gontumukkala, Sai Surya Teja, Analysis of Image Classification using SVM, (2021)","Singh, V.","Elsevier B.V.","","2022 International Conference on Machine Learning and Data Engineering, ICMLDE 2022","","Dehradun","188151","18770509","9781510849914","","","English","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85163687390"
"Mallikarjuna, B.; Ram, M.S.; Addanke, S.","Mallikarjuna, B. (57142279300); Ram, M. Sethu (57773277500); Addanke, Supriya (57693951100)","57142279300; 57773277500; 57693951100","An Improved Face-Emotion Recognition to Automatically Generate Human Expression With Emoticons","2022","International Journal of Reliable and Quality E-Healthcare","11","1","","","","0","0","10.4018/IJRQEH.314945","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154061066&doi=10.4018%2FIJRQEH.314945&partnerID=40&md5=b72e426494ee870d7b57e01f3f571c53","Galgotias University, Greater Noida, India; Sri Padmavati Mahila Visvavidyalayam, Tirupati, India","Mallikarjuna, B., Galgotias University, Greater Noida, India; Ram, M. Sethu, Sri Padmavati Mahila Visvavidyalayam, Tirupati, India; Addanke, Supriya, Sri Padmavati Mahila Visvavidyalayam, Tirupati, India","Any human face image expression naturally identifies expressions of happy, sad etc.; sometimes human facial image expression recognition is complex, and it is a combination of two emotions. The existing literature provides face emotion classification and image recognition, and the study on deep learning using convolutional neural networks (CNN), provides face emotion recognition most useful for healthcare and with the most complex of the existing algorithms. This paper improves the human face emotion recognition and provides feelings of interest for others to generate emoticons on their smartphone. Face emotion recognition plays a major role by using convolutional neural networks in the area of deep learning and artificial intelligence for healthcare services. Automatic facial emotion recognition consists of two methods, such as face detection with Ada boost classifier algorithm and emotional classification, which consists of feature extraction by using deep learning methods such as CNN to identify the seven emotions to generate emoticons. © 2023 Elsevier B.V., All rights reserved.","Artificial Intelligence; convolutional neural network (CNN); Deep learning; emoticons; emotion classification; face-recognition; feature extraction; pre-processing","","","","","Real Time Convolutional Neural Networks for Emotion and Gender Classification, (2017); Baliyan, Mohit, Prediction of decay modes of higgs boson using classification algorithms, Journal of Critical Reviews, 7, 7, pp. 300-306, (2020); I Managers Journal on Software Engineering, (2012); Bendjillali, Ridha Ilyas, Improved facial expression recognition based on DWT feature for deep CNN, Electronics (Switzerland), 8, 3, (2019); Tee, Connie, Facial expression recognition using a hybrid CNN-SIFT aggregator, Lecture Notes in Computer Science, 10607 LNAI, pp. 139-149, (2017); Gao, Zhongke, A GPSO-optimized convolutional neural networks for EEG-based emotion recognition, Neurocomputing, 380, pp. 225-235, (2020); Ghoneim, Ahmed M., Cervical cancer classification using convolutional neural networks and extreme learning machines, Future Generation Computer Systems, 102, pp. 643-649, (2020); Giménez, Maite, Semantic-based padding in convolutional neural networks for improving the performance in natural language processing. A case of study in sentiment analysis, Neurocomputing, 378, pp. 315-323, (2020); He, Xuanyu, Emotion recognition by assisted learning with convolutional neural networks, Neurocomputing, 291, pp. 187-194, (2018); Bendjillali, Ridha Ilyas, Facial expression recognition based on DWT feature for deep CNN, pp. 344-348, (2019)","","IGI Global","","","","","","21609551; 2160956X","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85154061066"
"Revathy, V.R.; Pillai, A.S.; Daneshfar, F.","Revathy, V. R. (57195526696); Pillai, Anitha S. (56642433900); Daneshfar, Fatemeh (35078447100)","57195526696; 56642433900; 35078447100","LyEmoBERT: Classification of lyrics' emotion and recommendation using a pre-trained model","2022","Procedia Computer Science","218","","","1196","1208","0","27","10.1016/j.procs.2023.01.098","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151808976&doi=10.1016%2Fj.procs.2023.01.098&partnerID=40&md5=830bda7f64381f2404e08aaeba5a73ea","School of Computing Sciences, Hindustan Institute of Technology and Science, Chennai, India; Department of Computer Engineering, University of Kurdistan, Sanandaj, Iran","Revathy, V. R., School of Computing Sciences, Hindustan Institute of Technology and Science, Chennai, India; Pillai, Anitha S., School of Computing Sciences, Hindustan Institute of Technology and Science, Chennai, India; Daneshfar, Fatemeh, Department of Computer Engineering, University of Kurdistan, Sanandaj, Iran","Music plays a significant role in evoking human emotions. Thanks to the quick proliferation of smartphones and mobile internet, music streaming applications and websites have made the music emotion recognition task even more active and exciting. However, music emotion recognition faces significant challenges too. These include inaccessibility of data, unavailability of large data volume, and the lack of other emotionally relevant features. While emotionally relevant features can be identified by analyzing lyrics and audio signals, the availability of datasets annotated with a lyrical emotion remains a constant challenge. This study uses the Music4All dataset to evaluate the lyrical features relevant for the identification of four important human emotions-happy, angry, relaxed, and sad. This was done with the help of several machine learning algorithms based on a semantic psychometric model. A transfer learning approach was also used to understand the feelings of the lyrics from an in-domain dataset and then predict the emotion of the target dataset. Further, it was observed that the BERT model improves the overall accuracy of the model (92%). A simple lyrics recommender system is also built using the Sentence Transformer model. © 2023 Elsevier B.V., All rights reserved.","BERT; Emotion classification; Emotion detection from lyrics; Music Information Retrieval; music lyrics recommendation; transformer approach","Classification (of information); Emotion Recognition; Learning algorithms; Machine learning; Semantics; Speech recognition; BERT; Emotion classification; Emotion detection; Emotion detection from lyric; Emotion recognition; Human emotion; Music emotions; Music information retrieval; Music lyric recommendation; Transformer approach; Music","","","","Agrawal, Yudhik, Transformer-Based Approach Towards Music Emotion Recognition from Lyrics, Lecture Notes in Computer Science, 12657 LNCS, pp. 167-175, (2021); Stanfort University, (2014); Barthet, Mathieu, Music emotion recognition: From content- to context-based models, Lecture Notes in Computer Science, 7900 LNCS, pp. 228-252, (2013); Text Based Sentiment Analysis and Music Emotion Recognition, (2018); Chang, Weicheng, Taming Pretrained Transformers for Extreme Multi-label Text Classification, pp. 3163-3171, (2020); Chen, Xituo Vito, Combining content and sentiment analysis on lyrics for a lightweight emotion-aware Chinese song recommendation system, ACM International Conference Proceeding Series, pp. 85-89, (2018); Chen, Tianqi, XGBoost: A scalable tree boosting system, 13-17-August-2016, pp. 785-794, (2016); Choi, Kahyun, Bimodal Music Subject Classification via Context-Dependent Language Models, Lecture Notes in Computer Science, 12645 LNCS, pp. 68-77, (2021); Cortes, Corinna, Support-Vector Networks, Machine Learning, 20, 3, pp. 273-297, (1995); Devlin, Jacob, BERT: Pre-training of deep bidirectional transformers for language understanding, 1, pp. 4171-4186, (2019)","Singh, V.","Elsevier B.V.","","2022 International Conference on Machine Learning and Data Engineering, ICMLDE 2022","","Dehradun","188151","18770509","9781510849914","","","English","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85151808976"
"Deveau, N.; Washington, P.; Leblanc, E.; Husic, A.; Dunlap, K.; Penev, Y.; Kline, A.; Mutlu, O.C.; Wall, D.P.","Deveau, Nicholas (57216830063); Washington, Peter Yigitcan (57191498961); Leblanc, Emilie (57218776057); Husic, Arman (57220907587); Dunlap, Kaitlyn L. (57208624014); Penev, Yordan (57218775787); Kline, Aaron (57191505004); Mutlu, Onur Cezmi (16043006700); Wall, Dennis Paul (7202196193)","57216830063; 57191498961; 57218776057; 57220907587; 57208624014; 57218775787; 57191505004; 16043006700; 7202196193","Machine learning models using mobile game play accurately classify children with autism","2022","Intelligence-Based Medicine","6","","100057","","","0","15","10.1016/j.ibmed.2022.100057","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139281320&doi=10.1016%2Fj.ibmed.2022.100057&partnerID=40&md5=d85247441ddc09981e7f03057256b1db","Department of Biomedical Data Science, Stanford University, Stanford, United States; Department of Bioengineering, Stanford, United States; PEDIATRICS, Stanford University, Stanford, United States; Stanford Engineering, Stanford, United States","Deveau, Nicholas, Department of Biomedical Data Science, Stanford University, Stanford, United States; Washington, Peter Yigitcan, Department of Bioengineering, Stanford, United States; Leblanc, Emilie, PEDIATRICS, Stanford University, Stanford, United States; Husic, Arman, PEDIATRICS, Stanford University, Stanford, United States; Dunlap, Kaitlyn L., PEDIATRICS, Stanford University, Stanford, United States; Penev, Yordan, PEDIATRICS, Stanford University, Stanford, United States; Kline, Aaron, PEDIATRICS, Stanford University, Stanford, United States; Mutlu, Onur Cezmi, Stanford Engineering, Stanford, United States; Wall, Dennis Paul, Department of Biomedical Data Science, Stanford University, Stanford, United States, PEDIATRICS, Stanford University, Stanford, United States","Digitally-delivered healthcare is well suited to address current inequities in the delivery of care due to barriers of access to healthcare facilities. As the COVID-19 pandemic phases out, we have a unique opportunity to capitalize on the current familiarity with telemedicine approaches and continue to advocate for mainstream adoption of remote care delivery. In this paper, we specifically focus on the ability of GuessWhat? a smartphone-based charades-style gamified therapeutic intervention for autism spectrum disorder (ASD) to generate a signal that distinguishes children with ASD from neurotypical (NT) children. We demonstrate the feasibility of using “in-the-wild”, naturalistic gameplay data to distinguish between ASD and NT by children by training a random forest classifier to discern the two classes (AU-ROC = 0.745, recall = 0.769). This performance demonstrates the potential for GuessWhat? to facilitate screening for ASD in historically difficult-to-reach communities. To further examine this potential, future work should expand the size of the training sample and interrogate differences in predictive ability by demographic. © 2022 Elsevier B.V., All rights reserved.","","","","","This work was supported in part by funds to DPW from the National Institutes of Health ( 1R01EB025025-01 , 1R21HD091500-01 , 1R01LM013083 , 1R01LM013364 ), the National Science Foundation (Award 2014232 ), The Hartwell Foundation , Bill and Melinda Gates Foundation , Coulter Foundation , Lucile Packard Foundation , the Weston Havens Foundation , and program grants from Stanford's Human Centered Artificial Intelligence Program , Stanford's Precision Health and Integrated Diagnostics Center (PHIND) , Stanford's Beckman Center , Stanford's Bio-X Center , Predictives and Diagnostics Accelerator (SPADA) Spectrum , Stanford's Spark Program in Translational Research , Stanford mediaX, and Stanford's Wu Tsai Neurosciences Institute's Neuroscience : Translate Program. We also acknowledge generous support from David Orr, Imma Calvo, Bobby Dekesyer and Peter Sullivan. P.W. would like to acknowledge support from Mr. Schroeder and the Stanford Interdisciplinary Graduate Fellowship (SIGF) as the Schroeder Family Goldman Sachs Graduate Fellow.","Telehealth and Medicine Today, (2020); Gunasekeran, Dinesh Visva, Digital health during COVID-19: lessons from operationalising new models of care in ophthalmology, The Lancet Digital Health, 3, 2, pp. e124-e134, (2021); Inkster, Becky S., Digital health management during and beyond the COVID-19 pandemic: Opportunities, barriers, and recommendations, JMIR Mental Health, 7, 7, (2020); Barbosa, William A., Improving Access to Care: Telemedicine across Medical Domains, Annual Review of Public Health, 42, pp. 463-481, (2020); Antezana, Ligia, Rural trends in diagnosis and services for autism spectrum disorder, Frontiers in Psychology, 8, APR, (2017); Ning, Michael, Identification and quantification of gaps in access to autism resources in the United States: An infodemiological study, Journal of Medical Internet Research, 21, 7, (2019); Elder, Jennifer Harrison, Clinical impact of early diagnosis of autism on the prognosis and parent-child relationships, Psychology Research and Behavior Management, 10, pp. 283-292, (2017); Greenhalgh, Trisha, The organising vision for telehealth and telecare: Discourse analysis, BMJ Open, 2, 4, (2012); Kosmicki, Jack A., Searching for a minimal set of behaviors for autism detection through feature selection-based machine learning, Translational Psychiatry, 5, 2, (2015); Duda, Marlena, Use of machine learning for behavioral distinction of autism and ADHD, Translational Psychiatry, 6, 2, (2016)","","Elsevier B.V.","","","","","","26665212","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85139281320"
"Fadlilah, U.; Prasetyo, R.A.R.; Mahamad, A.K.; Handaga, B.; Saon, S.; Sudarmilah, E.","Fadlilah, Umi (57202821436); Prasetyo, Raden Adrian Rafli (57810733100); Mahamad, Abd Kadir (26423181700); Handaga, Bana (43061222800); Saon, Sharifah (35280559600); Sudarmilah, Endah (56122440200)","57202821436; 57810733100; 26423181700; 43061222800; 35280559600; 56122440200","Modelling of basic Indonesian Sign Language translator based on Raspberry Pi technology","2022","Scientific and Technical Journal of Information Technologies, Mechanics and Optics","22","3","","574","584","0","2","10.17586/2226-1494-2022-22-3-574-584","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134495455&doi=10.17586%2F2226-1494-2022-22-3-574-584&partnerID=40&md5=afb437d3154752735a66b7aedb6670ff","Universitas Muhammadiyah Surakarta, Surakarta, Indonesia; Universiti Tun Hussein Onn Malaysia, Batu Pahat, Malaysia","Fadlilah, Umi, Universitas Muhammadiyah Surakarta, Surakarta, Indonesia, Universiti Tun Hussein Onn Malaysia, Batu Pahat, Malaysia; Prasetyo, Raden Adrian Rafli, Universitas Muhammadiyah Surakarta, Surakarta, Indonesia; Mahamad, Abd Kadir, Universiti Tun Hussein Onn Malaysia, Batu Pahat, Malaysia; Handaga, Bana, Universitas Muhammadiyah Surakarta, Surakarta, Indonesia; Saon, Sharifah, Universiti Tun Hussein Onn Malaysia, Batu Pahat, Malaysia; Sudarmilah, Endah, Universitas Muhammadiyah Surakarta, Surakarta, Indonesia","Deaf people have hearing loss from mild to very severe. Such people have difficulty processing language information both with and without hearing aids. Deaf people who do not use hearing aids use sign language in their everyday conversations. At the same time, it is difficult for general people to communicate with the deaf, so in order to communicate with the deaf they must know sign language. There are two sign languages in Indonesia, namely SIBI (Indonesian Sign Language System) and BISINDO (Indonesian Sign Language). To help with communication between deaf and normal people, we developed a model using the one-handed SIBI method as an example, and then further developed it using the one-handed and two-handed BISINDO. The main function of the method is the recognition of basic letters, words, sentences and numbers using a Raspberry Pi single-board computer and a camera which are designed to detect the movements of language gestures. With the help of a special program, images are translated into text on the monitor screen. The method used is image processing and machine learning using the Python programming language and Convolutional Neural Network techniques. The device prototype issues a warning to repeat the sign language if the translation fails, and delete the translation if it doesn’t match the database. The prototype of the device requires further development providing its flexibility: to provide reading of dynamic movements, facial expressions, to provide translation of words not included in the existing database. You need to add a database other than SIBI, such as BISINDO, or sign languages from other regions or countries. © 2022 Elsevier B.V., All rights reserved.","CNN; deaf people; droidcam; image processing; machine learning; Python; Raspberry Pi; SIBI; sign language; smartphone camera; webcam","","","","This work was supported by a research grant TIER 1, H917. The researchers thank all those who contributed to the preparation of the work, especially the UMS (Muhammadiyya University Surakarta) Electrical and Electronics Engineering Curriculum and Tun Hussein Onn Malaysia (UTHM) Electrical Engineering Curriculum, Research Management. We thank the Center, which contributed to the implementation and testing of this study.","Jurnal Jurusan Plb Fip Universitas Pendidikan Indonesia, (2007); Shiraishi, Yuhki, Crowdsourced real-time captioning of sign language by deaf and hard-of-hearing people, International Journal of Pervasive Computing and Communications, 13, 1, pp. 2-25, (2017); Islam, Md Shafiqul, Recognition bangla sign language using convolutional neural network, (2019); Myslang an Electronic Malaysian Sign Language Dictionary, (2008); Karpov, Alexey Anatolievich, Automatic Technologies for Processing Spoken Sign Languages, Procedia Computer Science, 81, pp. 201-207, (2016); Brour, Mourad, ATLASLang NMT: Arabic text language into Arabic sign language neural machine translation, Journal of King Saud University - Computer and Information Sciences, 33, 9, pp. 1121-1131, (2021); Latif, Ghazanfar, ArASL: Arabic Alphabets Sign Language Dataset, Data in Brief, 23, (2019); Suharjito, Suharjito, The Comparison of Some Hidden Markov Models for Sign Language Recognition, pp. 6-10, (2018); Muthu Mariappan, H., Real-time recognition of Indian sign language, (2019); Kadiyala, Akhil, Applications of Python to evaluate environmental data science problems, Environmental Progress and Sustainable Energy, 36, 6, pp. 1580-1586, (2017)","","ITMO University","","","","","","22261494; 25000373","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85134495455"
"Ali, N.; Abd-alrazaq, A.; Shah, Z.; Alajlani, M.; Alam, T.; Househ, M.","Ali, Nashva (57221715291); Abd-alrazaq, Alaa A. (57208188497); Shah, Zubair (56428700200); Alajlani, Mohannad (55644338900); Alam, Tanvir (56188785000); Househ, Mowafa Said (8667908000)","57221715291; 57208188497; 56428700200; 55644338900; 56188785000; 8667908000","Artificial Intelligence-Based Mobile Application for Sensing Children Emotion Through Drawings","2022","Studies in Health Technology and Informatics","295","","","118","121","0","5","10.3233/SHTI220675","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133298772&doi=10.3233%2FSHTI220675&partnerID=40&md5=e649b3837c2eef60d168b1ca7e30f00c","Division of Information and Computing Technology, Hamad Bin Khalifa University, College of Science and Engineering, Doha, Qatar; AI Center for Precision Health, Weill Cornell Medicine-Qatar, Doha, Qatar; Institute of Digital Healthcare, University of Warwick, Coventry, United Kingdom","Ali, Nashva, Division of Information and Computing Technology, Hamad Bin Khalifa University, College of Science and Engineering, Doha, Qatar, AI Center for Precision Health, Weill Cornell Medicine-Qatar, Doha, Qatar; Abd-alrazaq, Alaa A., Division of Information and Computing Technology, Hamad Bin Khalifa University, College of Science and Engineering, Doha, Qatar; Shah, Zubair, Division of Information and Computing Technology, Hamad Bin Khalifa University, College of Science and Engineering, Doha, Qatar; Alajlani, Mohannad, Institute of Digital Healthcare, University of Warwick, Coventry, United Kingdom; Alam, Tanvir, Division of Information and Computing Technology, Hamad Bin Khalifa University, College of Science and Engineering, Doha, Qatar; Househ, Mowafa Said, Division of Information and Computing Technology, Hamad Bin Khalifa University, College of Science and Engineering, Doha, Qatar","Children go through varied emotions such as happiness, sadness, and fear. At times, it may be difficult for children to express their emotions. Detecting and understanding the unexpressed emotions of children is very important to address their needs and prevent mental health issues. In this paper, we develop an artificial intelligence (AI) based Emotion Sensing Recognition App (ESRA) to help parents and teachers understand the emotions of children by analyzing their drawings. We collected 102 drawings from a local school in Doha and 521 drawings from Google and Instagram. Four different experiments were conducted using a combination of the two datasets. The deep learning model was trained using the Fastai library in Python. The model classifies the drawings into positive or negative emotions. The model accuracy ranged from 55% to 79% in the four experiments. This study showed that ESRA has the potential in identifying the emotions of children. However, the underlying algorithm needs to be trained and evaluated using more drawings to improve its current accuracy and to be able to identify more specific emotions. © 2022 Elsevier B.V., All rights reserved.","Artificial Intelligence; Children; Emotion Sensing; Mobile Application","Emotion Recognition; Mobile computing; Python; 'current; Child; Emotion sensing; Google+; Health issues; Learning models; Mental health; Mobile applications; Modeling accuracy; Teachers'; Deep learning; algorithm; artificial intelligence; child; conference paper; deep learning; drawing; emotion; female; human; human experiment; male; mobile application; social media; teacher; fear; Artificial Intelligence; Emotions; Fear; Humans; Mobile Applications","","","","Working with Children in Art Therapy, (1990); Clinical Art Therapy A Comprehensive Guide, (1981); International Journal of Child Adolescent Psychiatry, (1975); How Artificial Intelligence can Detect Emotions in Children S Drawings, (2019)","Mantas, J.; Gallos, P.; Zoulias, E.; Hasman, A.; Househ, M.S.; Diomidous, M.; Liaskos, J.; Charalampidou, M.","IOS Press BV","","","","","","18798365; 09269630","9781643685939; 9781643685946; 9781643683409; 1586031430; 9781607508052; 9781586037512; 1586036475; 1586032038; 9051993625; 9781614996521","","35773821","English","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85133298772"
"Abdel Hameed, M.; Hassaballah, M.; Hosney, M.E.; Alqahtani, A.","Abdel Hameed, Mohamed Abdel (57213688739); Hassaballah, M. (36602545600); Hosney, Mosa E. (57211993301); Alqahtani, Abdullah Saad (57396077400)","57213688739; 36602545600; 57211993301; 57396077400","An AI-Enabled Internet of Things Based Autism Care System for Improving Cognitive Ability of Children with Autism Spectrum Disorders","2022","Computational Intelligence and Neuroscience","2022","","2247675","","","0","45","10.1155/2022/2247675","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131337474&doi=10.1155%2F2022%2F2247675&partnerID=40&md5=90d247b8e6b4cb5000c2faf6e33b5a41","Department of Computer Science, Luxor University, Luxor, Egypt; Department of Computer Science, Faculty of Computer and Information, Qena, Egypt; Department of Computer Science, College of Information Technology, 6th October, Egypt; Department of Information System, Luxor University, Luxor, Egypt; College of Computer Engineering and Sciences, Prince Sattam Bin Abdulaziz University, Al Kharj, Saudi Arabia","Abdel Hameed, Mohamed Abdel, Department of Computer Science, Luxor University, Luxor, Egypt; Hassaballah, M., Department of Computer Science, Faculty of Computer and Information, Qena, Egypt, Department of Computer Science, College of Information Technology, 6th October, Egypt; Hosney, Mosa E., Department of Information System, Luxor University, Luxor, Egypt; Alqahtani, Abdullah Saad, College of Computer Engineering and Sciences, Prince Sattam Bin Abdulaziz University, Al Kharj, Saudi Arabia","Smart monitoring and assisted living systems for cognitive health assessment play a central role in assessment of individuals' health conditions. Autistic children suffer from some difficulties including social skills, repetitive behaviors, speech and nonverbal communication, and accommodating to the environment around them. Thus, dealing with autistic children is a serious public health problem as it is hard to determine what they feel with a lack of emotional cognitive ability. Currently, no medical treatments have been shown to cure autistic children, with most of the social assistive research to date focusing on Autism Spectrum Disorder (ASD) without suggesting a real treatment. In this paper, we focus on improving cognitive ability and daily living skills and maximizing the ability of the autistic child to function and participate positively in the community. Through utilizing intelligent systems based Artificial Intelligence (AI) and Internet of Things (IoT) technologies, we facilitate the process of adaptation to the world around the autistic children. To this end, we propose an AI-enabled IoT system embodied in a sensor for measuring the heart rate to predict the state of the child and then sending the state to the guardian with feeling and expected behavior of the child via a mobile application. Further, the system can provide a new virtual environment to help the child to be capable of improving eye contact with other people. This way is represented in pictures of these persons in 3D models that break this child's fear barrier. The system follows strategies that have focused on social communication skill development particularly at young ages to be more interactive with others. © 2022 Elsevier B.V., All rights reserved.","","Cognitive systems; Diseases; Intelligent systems; Public health; Speech communication; Virtual reality; Autism spectrum disorders; Autistic children; Children with autisms; Cognitive ability; Health assessments; Health condition; Living systems; Non-verbal communications; Smart monitoring; Social skills; Internet of things; artificial intelligence; autism; child; cognition; human; psychology; Artificial Intelligence; Autism Spectrum Disorder; Autistic Disorder; Child; Cognition; Humans; Internet of Things","","","","Javed, Abdul Rehman, Automated cognitive health assessment in smart homes using machine learning, Sustainable Cities and Society, 65, (2021); Mughal, Huma, Parkinson's Disease Management via Wearable Sensors: A Systematic Review, IEEE Access, 10, pp. 35219-35237, (2022); Journal of Computational and Theoretical Nanoscience, (2021); Rizwan, Muhammad, Brain Tumor and Glioma Grade Classification Using Gaussian Convolutional Neural Network, IEEE Access, 10, pp. 29731-29740, (2022); Houssein, Essam H., A Hybrid Heartbeats Classification Approach Based on Marine Predators Algorithm and Convolution Neural Networks, IEEE Access, 9, pp. 86194-86206, (2021); Bekhet, Saddam, An efficient method for covid-19 detection using light weight convolutional neural network, Computers, Materials and Continua, 69, 2, pp. 2475-2491, (2021); Saleem, Kiran, Situation-Aware BDI Reasoning to Detect Early Symptoms of Covid 19 Using Smartwatch, IEEE Sensors Journal, 23, 2, pp. 898-905, (2023); Mohiyuddin, Aqsa, Breast Tumor Detection and Classification in Mammogram Images Using Modified YOLOv5 Network, Computational and Mathematical Methods in Medicine, 2022, (2022); Akhtar, Salwa Muhammad, A Multi-Agent Formalism Based on Contextual Defeasible Logic for Healthcare Systems, Frontiers in Public Health, 10, (2022); Bhatnagar, Vaibhav, Internet of things: A conceptual visualisation, pp. 81-112, (2020)","","Hindawi Limited","","","","","","16875265; 16875273","","","35655510","English","Article","Final","All Open Access; Green Final Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85131337474"
"Wan, G.; Deng, F.; Jiang, Z.; Song, S.; Hu, D.; Chen, L.; Wang, H.; Li, M.; Chen, G.; Yan, T.; Su, J.; Zhang, J.","Wan, Guobin (56258026300); Deng, Fuhao (57202963132); Jiang, Zijian (57208391128); Song, Sifan (57207760100); Hu, Di (57681952700); Chen, Lifu (57217044554); Wang, Haibo (57219751817); Li, Miaochun (57682780600); Chen, Gong (57208398953); Yan, Ting (57211366183); Su, Jionglong (57045140900); Zhang, Jiaming (57202953338)","56258026300; 57202963132; 57208391128; 57207760100; 57681952700; 57217044554; 57219751817; 57682780600; 57208398953; 57211366183; 57045140900; 57202953338","FECTS: A Facial Emotion Cognition and Training System for Chinese Children with Autism Spectrum Disorder","2022","Computational Intelligence and Neuroscience","2022","","9213526","","","0","21","10.1155/2022/9213526","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129947060&doi=10.1155%2F2022%2F9213526&partnerID=40&md5=6c9832a6755f071d7bf66e8e4113dffb","Shenzhen Key Laboratory of Maternal and Child Health and Diseases, Shenzhen, China; Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen, China; Department of Mathematical Sciences, Xi'an Jiaotong-Liverpool University, Suzhou, China; Robert H. Smith School of Business, College Park, United States; Smart Children Education Center, Ltd, Shenzhen, China; China University of Mining and Technology, Xuzhou, China; Department of Information Management and Information System, Guangdong Pharmaceutical University, Guangzhou, China; Ltd., Shenzhen, China; Shenzhen Key Laboratory for Molecular Biology of Neural Development, Shenzhen Institutes of Advanced Technology, Shenzhen, China; School of AI and Advanced Computing, Xi'an Jiaotong-Liverpool University, Suzhou, China; Institute of Robotics and Intelligent Manufacturing, The Chinese University of Hong Kong, Shenzhen, Shenzhen, China","Wan, Guobin, Shenzhen Key Laboratory of Maternal and Child Health and Diseases, Shenzhen, China; Deng, Fuhao, Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen, China; Jiang, Zijian, Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen, China; Song, Sifan, Department of Mathematical Sciences, Xi'an Jiaotong-Liverpool University, Suzhou, China; Hu, Di, Robert H. Smith School of Business, College Park, United States; Chen, Lifu, Smart Children Education Center, Ltd, Shenzhen, China; Wang, Haibo, China University of Mining and Technology, Xuzhou, China; Li, Miaochun, Department of Information Management and Information System, Guangdong Pharmaceutical University, Guangzhou, China; Chen, Gong, Ltd., Shenzhen, China; Yan, Ting, Shenzhen Key Laboratory for Molecular Biology of Neural Development, Shenzhen Institutes of Advanced Technology, Shenzhen, China; Su, Jionglong, School of AI and Advanced Computing, Xi'an Jiaotong-Liverpool University, Suzhou, China; Zhang, Jiaming, Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen, China, Institute of Robotics and Intelligent Manufacturing, The Chinese University of Hong Kong, Shenzhen, Shenzhen, China","Traditional training methods such as card teaching, assistive technologies (e.g., augmented reality/virtual reality games and smartphone apps), DVDs, human-computer interactions, and human-robot interactions are widely applied in autistic rehabilitation training in recent years. In this article, we propose a novel framework for human-computer/robot interaction and introduce a preliminary intervention study for improving the emotion recognition of Chinese children with an autism spectrum disorder. The core of the framework is the Facial Emotion Cognition and Training System (FECTS, including six tasks to train children with ASD to match, infer, and imitate the facial expressions of happiness, sadness, fear, and anger) based on Simon Baron-Cohen's E-S (empathizing-systemizing) theory. Our system may be implemented on PCs, smartphones, mobile devices such as PADs, and robots. The training record (e.g., a tracked record of emotion imitation) of the Chinese autistic children interacting with the device implemented using our FECTS will be uploaded and stored in the database of a cloud-based evaluation system. Therapists and parents can access the analysis of the emotion learning progress of these autistic children using the cloud-based evaluation system. Deep-learning algorithms of facial expressions recognition and attention analysis will be deployed in the back end (e.g., devices such as a PC, a robotic system, or a cloud system) implementing our FECTS, which can perform real-time tracking of the imitation quality and attention of the autistic children during the expression imitation phase. In this preliminary clinical study, a total of 10 Chinese autistic children aged 3-8 are recruited, and each of them received a single 20-minute training session every day for four consecutive days. Our preliminary results validated the feasibility of the developed FECTS and the effectiveness of our algorithms based on Chinese children with an autism spectrum disorder. To verify that our FECTS can be further adapted to children from other countries, children with different cultural/sociological/linguistic contexts should be recruited in future studies. © 2022 Elsevier B.V., All rights reserved.","","Augmented reality; Computer games; Deep learning; Diseases; Human robot interaction; Learning algorithms; Microcomputers; Quality control; Smartphones; Spectrum analysis; Assistive technology; Autism spectrum disorders; Autistic children; Children with autisms; Cloud-based; Emotion cognition; Facial emotions; Smartphone apps; Training methods; Training Systems; Human computer interaction; autism; child; China; cognition; emotion; facial expression; human; Autism Spectrum Disorder; Child; Cognition; Emotions; Facial Expression; Humans","","","","Mindblindness an Essay on Autism and Theory of Mind, (1995); Golan, Ofer, Enhancing emotion recognition in children with autism spectrum conditions: An intervention using animated vehicles with real emotional faces, Journal of Autism and Developmental Disorders, 40, 3, pp. 269-279, (2010); Autism A Global Framework for Action, (2025); Report on the Industry Development of Autism Education and Rehabilitation in China II, (2017); Situation of the Life and the Learning of Two Million Autistic Children Became Severe, (2017); Baron-Cohen, Simon B., Does the autistic child have a ""theory of mind"" ?, Cognition, 21, 1, pp. 37-46, (1985); Autism Explaining the Enigma, (1989); Ozonoff, Sally J., Executive Function Deficits in High‐Functioning Autistic Individuals: Relationship to Theory of Mind, Journal of Child Psychology and Psychiatry and Allied Disciplines, 32, 7, pp. 1081-1105, (1991); Rumsey, Judith M., Neuropsychological findings in high-functioning men with infantile autism, residual state, Journal of Clinical and Experimental Neuropsychology, 10, 2, pp. 201-221, (1988); Autism as an Executive Disorder, (1997)","","Hindawi Limited","","","","","","16875265; 16875273","","","35528364","English","Article","Final","All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85129947060"
"Abbas, A.; Hansen, B.J.; Koesmahargyo, V.; Yadav, V.; Rosenfield, P.J.; Patil, O.; Dockendorf, M.F.; Moyer, M.; Shipley, L.A.; Pérez-Rodriguez, M.M.; Galatzer-Levy, I.R.","Abbas, Anzar (57192920513); Hansen, Bryan J. (26430675000); Koesmahargyo, Vidya (57220058297); Yadav, Vijay (57218511945); Rosenfield, Paul J. (6602140586); Patil, Omkar Sudhir (55612831500); Dockendorf, Marissa Fallon (36709368000); Moyer, Matthew (57203136142); Shipley, Lisa A. (7003727099); Pérez-Rodriguez, Mercedes Mercedez (57210324100); Galatzer-Levy, Isaac Robert (16303648200)","57192920513; 26430675000; 57220058297; 57218511945; 6602140586; 55612831500; 36709368000; 57203136142; 7003727099; 57210324100; 16303648200","Facial and Vocal Markers of Schizophrenia Measured Using Remote Smartphone Assessments: Observational Study","2022","JMIR Formative Research","6","1","e26276","","","0","27","10.2196/26276","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124153909&doi=10.2196%2F26276&partnerID=40&md5=b3a4c7977785f7074a219553adc46d68","AiCure, New York, United States; Merck & Co., Inc., Rahway, United States; Icahn School of Medicine at Mount Sinai, New York, United States; Department of Psychiatry, NYU Grossman School of Medicine, New York, United States","Abbas, Anzar, AiCure, New York, United States; Hansen, Bryan J., Merck & Co., Inc., Rahway, United States; Koesmahargyo, Vidya, AiCure, New York, United States; Yadav, Vijay, AiCure, New York, United States; Rosenfield, Paul J., Icahn School of Medicine at Mount Sinai, New York, United States; Patil, Omkar Sudhir, Merck & Co., Inc., Rahway, United States; Dockendorf, Marissa Fallon, Merck & Co., Inc., Rahway, United States; Moyer, Matthew, Merck & Co., Inc., Rahway, United States; Shipley, Lisa A., Merck & Co., Inc., Rahway, United States; Pérez-Rodriguez, Mercedes Mercedez, Icahn School of Medicine at Mount Sinai, New York, United States; Galatzer-Levy, Isaac Robert, AiCure, New York, United States, Department of Psychiatry, NYU Grossman School of Medicine, New York, United States","Background: Machine learning–based facial and vocal measurements have demonstrated relationships with schizophrenia diagnosis and severity. Demonstrating utility and validity of remote and automated assessments conducted outside of controlled experimental or clinical settings can facilitate scaling such measurement tools to aid in risk assessment and tracking of treatment response in populations that are difficult to engage. Objective: This study aimed to determine the accuracy of machine learning–based facial and vocal measurements acquired through automated assessments conducted remotely through smartphones. Methods: Measurements of facial and vocal characteristics including facial expressivity, vocal acoustics, and speech prevalence were assessed in 20 patients with schizophrenia over the course of 2 weeks in response to two classes of prompts previously utilized in experimental laboratory assessments: evoked prompts, where subjects are guided to produce specific facial expressions and speech; and spontaneous prompts, where subjects are presented stimuli in the form of emotionally evocative imagery and asked to freely respond. Facial and vocal measurements were assessed in relation to schizophrenia symptom severity using the Positive and Negative Syndrome Scale. Results: Vocal markers including speech prevalence, vocal jitter, fundamental frequency, and vocal intensity demonstrated specificity as markers of negative symptom severity, while measurement of facial expressivity demonstrated itself as a robust marker of overall schizophrenia symptom severity. Conclusions: Established facial and vocal measurements, collected remotely in schizophrenia patients via smartphones in response to automated task prompts, demonstrated accuracy as markers of schizophrenia symptom severity. Clinical implications are discussed. © 2022 Elsevier B.V., All rights reserved.","Computer vision; Digital biomarkers; Facial expressivity; Negative symptoms; Phenotyping; Vocal acoustics","","","","Funding text 1: The authors appreciate the involvement of the clinical, research, and operations staff at both Mount Sinai and AiCure for the development, deployment, and implementation of the technology presented here and the participants who volunteered to be involved in the research.; Funding text 2: IGL, AA, VY, and VK were employed and own shares at AiCure, LLC, at the time of the study. Authors OP, MD, MM, LS, and BH are employees of Merck Sharp & Dohme Corp, a subsidiary of Merck & Co, Inc, and may own stocks/stock options at Merck & Co, Inc. MMPR has received research grant funding from Neurocrine Biosciences Inc, Millennium Pharmaceuticals, Takeda, Merck, and AiCure. She is an advisory boardmember for Neurocrine Biosciences Inc.","Insel, Thomas M., Digital phenotyping: a global tool for psychiatry, World Psychiatry, 17, 3, pp. 276-277, (2018); Figueroa, Caroline Astrid, The Need for a Mental Health Technology Revolution in the COVID-19 Pandemic, Frontiers in Psychiatry, 11, (2020); Kopec, Kristin, Rapid Transition to Telehealth in a Community Mental Health Service Provider During the COVID-19 Pandemic, Primary Care Companion for CNS Disorders, 22, 5, (2020); Brunette, Mary F., Use of Smartphones, Computers and Social Media Among People with SMI: Opportunity for Intervention, Community Mental Health Journal, 55, 6, pp. 973-978, (2019); Insel, Thomas M., Digital phenotyping: Technology for a new science of behavior, JAMA, 318, 13, pp. 1215-1216, (2017); Insel, Thomas M., Bending the curve for mental health: Technology for a public health approach, American Journal of Public Health, 109, pp. S168-S170, (2019); Lecomte, Tania, Mobile apps for mental health issues: Meta-review of meta-analyses, JMIR mHealth and uHealth, 8, 5, (2020); Torous, John Blake, Characterizing the clinical relevance of digital phenotyping data quality with applications to a cohort with schizophrenia, npj Digital Medicine, 1, 1, (2018); Marsch, Lisa A., Opportunities and needs in digital phenotyping, Neuropsychopharmacology, 43, 8, pp. 1637-1638, (2018); Torous, John Blake, A new window into psychosis: The rise digital phenotyping, smartphone assessment, and mobile monitoring, Schizophrenia Research, 197, pp. 67-68, (2018)","","JMIR Publications Inc.","","","","","","2561326X","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85124153909"
"Santamaria-Granados, L.; Mendoza-Moreno, J.F.; Chantre-Astaiza, A.; Munoz-Organero, M.; Ramírez-González, G.","Santamaria-Granados, Luz (57203714727); Mendoza-Moreno, Juan Francisco (57203716892); Chantre-Astaiza, Angela (56278499700); Munoz-Organero, Mario (26030525600); Ramírez-González, Gustavo Adolfo (36603157500)","57203714727; 57203716892; 56278499700; 26030525600; 36603157500","Tourist experiences recommender system based on emotion recognition with wearable data","2021","Sensors","21","23","7854","","","0","14","10.3390/s21237854","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119666475&doi=10.3390%2Fs21237854&partnerID=40&md5=67e8c6c66111cb53bb2ac908d1136366","Faculty of Systems Engineering, Universidad Santo Tomás, Bogota, Colombia; Department of Tourism Sciences, Universidad del Cauca, Popayan, Colombia; Department of Telematic Engineering, Universidad Carlos III de Madrid, Getafe, Spain; Telematics Department, Universidad del Cauca, Popayan, Colombia","Santamaria-Granados, Luz, Faculty of Systems Engineering, Universidad Santo Tomás, Bogota, Colombia; Mendoza-Moreno, Juan Francisco, Faculty of Systems Engineering, Universidad Santo Tomás, Bogota, Colombia; Chantre-Astaiza, Angela, Department of Tourism Sciences, Universidad del Cauca, Popayan, Colombia; Munoz-Organero, Mario, Department of Telematic Engineering, Universidad Carlos III de Madrid, Getafe, Spain; Ramírez-González, Gustavo Adolfo, Telematics Department, Universidad del Cauca, Popayan, Colombia","The collection of physiological data from people has been facilitated due to the mass use of cheap wearable devices. Although the accuracy is low compared to specialized healthcare devices, these can be widely applied in other contexts. This study proposes the architecture for a tourist experiences recommender system (TERS) based on the user’s emotional states who wear these devices. The issue lies in detecting emotion from Heart Rate (HR) measurements obtained from these wearables. Unlike most state-of-the-art studies, which have elicited emotions in controlled experiments and with high-accuracy sensors, this research’s challenge consisted of emotion recognition (ER) in the daily life context of users based on the gathering of HR data. Furthermore, an objective was to generate the tourist recommendation considering the emotional state of the device wearer. The method used comprises three main phases: The first was the collection of HR measurements and labeling emotions through mobile applications. The second was emotional detection using deep learning algorithms. The final phase was the design and validation of the TERS-ER. In this way, a dataset of HR measurements labeled with emotions was obtained as results. Among the different algorithms tested for ER, the hybrid model of Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks had promising results. Moreover, concerning TERS, Collaborative Filtering (CF) using CNN showed better performance. © 2021 Elsevier B.V., All rights reserved.","CNN; Emotion detection; Heart rate; IoT; LSTM; Recommender system; Tourist experience; Wearable; Xiaomi mi band","Heart; Internet of things; Recommender systems; Speech recognition; Wearable technology; Convolutional neural network; Emotion detection; Emotion recognition; Emotional state; Heart-rate; Physiological data; Rate measurements; Tourist experience; Wearable devices; Xiaomi mi band; Long short-term memory; algorithm; electronic device; emotion; human; long term memory; Algorithms; Emotions; Humans; Memory, Long-Term; Neural Networks, Computer; Wearable Electronic Devices","","","Funding text 1: Acknowledgments: This research was financially supported by the Ministry of Science, Technology, and Innovation of Colombia (733-2015) and by the Universidad Santo Tomás Seccional Tunja. We thank the members of the GICAC group (Research Group in Administrative and Accounting Sciences); Funding text 2: Funding: This paper was funded by the Universidad Carlos III de Madrid.","John Dian, F. John, Wearables and the Internet of Things (IoT), Applications, Opportunities, and Challenges: A Survey, IEEE Access, 8, pp. 69200-69211, (2020); Nawara, Dina, IoT-based recommendation systems - An overview, (2020); Makikawa, Masaaki, Fundamentals of Wearable Sensors for the Monitoring of Physical and Physiological Changes in Daily Life, pp. 517-541, (2014); Choe, Yeongbae, The Quantified Traveler: Implications for Smart Tourism Development, Tourism on the Verge, Part F1056, pp. 65-77, (2017); Kim, Jeongmi (Jamie), Tourism Experience and Tourism Design, Tourism on the Verge, Part F1054, pp. 17-29, (2017); Information and Communication Technologies in Tourism 2015, (2015); (Jamie) Kim, Jeongmi, Measuring Human Senses and the Touristic Experience: Methods and Applications, Tourism on the Verge, Part F1056, pp. 47-63, (2017); A Practical Guide to Tourism Destination Management, (2007); Santamaria-Granados, Luz, Tourist recommender systems based on emotion recognition—a scientometric review, Future Internet, 13, 1, pp. 1-38, (2021); Kutt, Krzysztof, BandReader-A mobile application for data acquisition from wearable devices in affective computing experiments, pp. 42-48, (2018)","","MDPI","","","","","","14248220","","","34883853","English","Article","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85119666475"
"Hossain, M.D.; Kabir, M.A.; Anwar, A.; Islam, M.Z.","Hossain, Md Delowar (57212814863); Kabir, Muhammad Ashad (36630601000); Anwar, Adnan (54892893300); Islam, Md Zahidul (57198634079)","57212814863; 36630601000; 54892893300; 57198634079","Detecting autism spectrum disorder using machine learning techniques: An experimental analysis on toddler, child, adolescent and adult datasets","2021","Health Information Science and Systems","9","1","17","","","0","93","10.1007/s13755-021-00145-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115856706&doi=10.1007%2Fs13755-021-00145-9&partnerID=40&md5=a5d930ffd42f6a4fa387d5333a01701e","School of Computing and Mathematics, Charles Sturt University, Bathurst, Australia; School of Information Technology, Deakin University, Geelong, Australia","Hossain, Md Delowar, School of Computing and Mathematics, Charles Sturt University, Bathurst, Australia; Kabir, Muhammad Ashad, School of Computing and Mathematics, Charles Sturt University, Bathurst, Australia; Anwar, Adnan, School of Information Technology, Deakin University, Geelong, Australia; Islam, Md Zahidul, School of Computing and Mathematics, Charles Sturt University, Bathurst, Australia","Autism Spectrum Disorder (ASD), which is a neuro development disorder, is often accompanied by sensory issues such an over sensitivity or under sensitivity to sounds and smells or touch. Although its main cause is genetics in nature, early detection and treatment can help to improve the conditions. In recent years, machine learning based intelligent diagnosis has been evolved to complement the traditional clinical methods which can be time consuming and expensive. The focus of this paper is to find out the most significant traits and automate the diagnosis process using available classification techniques for improved diagnosis purpose. We have analyzed ASD datasets of toddler, child, adolescent and adult. We have evaluated state-of-the-art classification and feature selection techniques to determine the best performing classifier and feature set, respectively, for these four ASD datasets. Our experimental results show that multilayer perceptron (MLP) classifier outperforms among all other benchmark classification techniques and achieves 100% accuracy with minimal number of attributes for toddler, child, adolescent and adult datasets. We also identify that ‘relief F’ feature selection technique works best for all four ASD datasets to rank the most significant attributes. © 2022 Elsevier B.V., All rights reserved.","ASD detection; Autism spectrum disorder; classification; feature selection; machine learning","accuracy; adolescent; adult; Article; autism; autism-spectrum quotient; behavior; child; classification; clinical article; cross validation; data analysis; data processing; decision tree; diagnostic accuracy; discriminant analysis; feature ranking; feature selection; female; human; jaundice; language; machine learning; male; mobile application; multilayer perceptron; questionnaire; random forest; support vector machine; toddler; videorecording","","","","Thabtah, F. Abdeljaber, A machine learning autism classification based on logistic regression analysis, Health Information Science and Systems, 7, 1, (2019); Wiggins, Lisa D., Using Standardized Diagnostic Instruments to Classify Children with Autism in the Study to Explore Early Development, Journal of Autism and Developmental Disorders, 45, 5, pp. 1271-1280, (2015); Lord, Catherine E., Autism Diagnostic Interview-Revised: A revised version of a diagnostic interview for caregivers of individuals with possible pervasive developmental disorders, Journal of Autism and Developmental Disorders, 24, 5, pp. 659-685, (1994); Le Couteur, Ann Simone, Autism as a strongly genetic disorder evidence from a british twin Study, Psychological Medicine, 25, 1, pp. 63-77, (1995); Data Mining and Knowledge Discovery Handbook, (2010); Am J, (2001); Ijsce, (2014); Fischbach, Gerald D., The simons simplex collection: A resource for identification of autism genetic risk factors, Neuron, 68, 2, pp. 192-195, (2010); Islam, Md Rafiqul, Depression detection from social network data using machine learning techniques, Health Information Science and Systems, 6, 1, (2018); Tech Rep, (2012)","","Springer Science and Business Media Deutschland GmbH","","","","","","20472501","","","","English","Article","Final","All Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85115856706"
"Fuentes, S.; Gonzalez Viejo, C.G.; Torrico, D.D.; Dunshea, F.R.","Fuentes, S. (23982386600); Gonzalez Viejo, Claudia (57192378497); Torrico, Damir Dennis (37050187300); Dunshea, Frank Rowland (7005947650)","23982386600; 57192378497; 37050187300; 7005947650","Digital integration and automated assessment of eye-tracking and emotional response data using the biosensory app to maximize packaging label analysis","2021","Sensors","21","22","7641","","","0","9","10.3390/s21227641","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119067798&doi=10.3390%2Fs21227641&partnerID=40&md5=30d1b303e9ee45903ac138817d7bfc4e","School of Agriculture and Food, Faculty of Veterinary and Agricultural Sciences, Melbourne, Australia; Department of Wine, Lincoln University, Lincoln, New Zealand; Faculty of Biological Sciences, University of Leeds, Leeds, United Kingdom","Fuentes, S., School of Agriculture and Food, Faculty of Veterinary and Agricultural Sciences, Melbourne, Australia; Gonzalez Viejo, Claudia, School of Agriculture and Food, Faculty of Veterinary and Agricultural Sciences, Melbourne, Australia; Torrico, Damir Dennis, Department of Wine, Lincoln University, Lincoln, New Zealand; Dunshea, Frank Rowland, School of Agriculture and Food, Faculty of Veterinary and Agricultural Sciences, Melbourne, Australia, Faculty of Biological Sciences, University of Leeds, Leeds, United Kingdom","New and emerging non-invasive digital tools, such as eye-tracking, facial expression and physiological biometrics, have been implemented to extract more objective sensory responses by panelists from packaging and, specifically, labels. However, integrating these technologies from different company providers and software for data acquisition and analysis makes their practical application difficult for research and the industry. This study proposed a prototype integration between eye tracking and emotional biometrics using the BioSensory computer application for three sample labels: Stevia, Potato chips, and Spaghetti. Multivariate data analyses are presented, showing the integrative analysis approach of the proposed prototype system. Further studies can be conducted with this system and integrating other biometrics available, such as physiological response with heart rate, blood, pressure, and temperature changes analyzed while focusing on different label components or packaging features. By maximizing data extraction from various components of packaging and labels, smart predictive systems can also be implemented, such as machine learning to assess liking and other parameters of interest from the whole package and specific components. © 2021 Elsevier B.V., All rights reserved.","Areas of interest; Computer application; Computer vision; Eye fixations; Sensory analysis","Application programs; Computer vision; Digital devices; Eye tracking; Learning systems; Machine components; Multivariant analysis; Packaging; Physiological models; Physiology; Sensory analysis; Area of interest; Automated assessment; Digital tools; Emotional response; Eye fixations; Eye-tracking; Facial Expressions; Physiological biometrics; Response data; Tracking response; Data acquisition; emotion; facial expression; machine learning; mobile application; Emotions; Eye-Tracking Technology; Facial Expression; Machine Learning; Mobile Applications","","","Funding: This research was funded by the Australian Government through the Australian Research Council [Grant number IH120100053] ‘Unlocking the Food Value Chain: Australian industry transformation for ASEAN markets’.","Buss, Dale, Food companies get smart about artificial intelligence, Food Technology, 72, 7, pp. 26-41, (2018); He, Wei, Dynamics of autonomic nervous system responses and facial expressions to odors, Frontiers in Psychology, 5, FEB, (2014); Modica, Enrica, Neurophysiological responses to different product experiences, Computational Intelligence and Neuroscience, 2018, (2018); Schulte-Holierhoek, Aurelia, Sensory expectation, perception, and autonomic nervous system responses to package colours and product popularity, Food Quality and Preference, 62, pp. 60-70, (2017); Vila-López, Natalia, Consumers' physiological and verbal responses towards product packages: Could these responses anticipate product choices?, Physiology and Behavior, 200, pp. 166-173, (2019); Gonzalez Viejo, Claudia, Emerging technologies based on artificial intelligence to assess the quality and consumer preference of beverages, Beverages, 5, 4, (2019); Kreibig, Sylvia D., Autonomic nervous system activity in emotion: A review, Biological Psychology, 84, 3, pp. 394-421, (2010); Liao, Lewis Xinwei, Emotional responses towards food packaging: A joint application of self-report and physiological measures of emotion, Food Quality and Preference, 42, pp. 48-55, (2015); Vila-López, Natalia, Designing a low-fat food packaging: Comparing consumers’ responses in virtual and physical shopping environments, Foods, 10, 2, (2021); European Conference on Media Communication Film, (2018)","","MDPI","","","","","","14248220","","","34833713","English","Article","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85119067798"
"Kim, G.; Choi, I.; Li, Q.; Kim, J.","Kim, Gihwi (57304551900); Choi, Ilyoung (55095035900); Li, Qinglong (57224548864); Kim, Jaekyeong (7601382361)","57304551900; 55095035900; 57224548864; 7601382361","Article a cnn-based advertisement recommendation through real-time user face recognition","2021","Applied Sciences (Switzerland)","11","20","9705","","","0","23","10.3390/app11209705","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117566266&doi=10.3390%2Fapp11209705&partnerID=40&md5=ec10ea6374e6a4d57ef1606b690ece94","Department of Big Data Analytics, Kyung Hee University, Seoul, South Korea; School of Management, Kyung Hee University, Seoul, South Korea","Kim, Gihwi, Department of Big Data Analytics, Kyung Hee University, Seoul, South Korea; Choi, Ilyoung, Department of Big Data Analytics, Kyung Hee University, Seoul, South Korea; Li, Qinglong, Department of Big Data Analytics, Kyung Hee University, Seoul, South Korea; Kim, Jaekyeong, Department of Big Data Analytics, Kyung Hee University, Seoul, South Korea, School of Management, Kyung Hee University, Seoul, South Korea","The advertising market’s use of smartphones and kiosks for non-face-to-face ordering is growing. An advertising video recommender system is needed that continuously shows advertising videos that match a user’s taste and displays other advertising videos quickly for unwanted advertise-ments. However, it is difficult to make a recommender system to identify users’ dynamic preferences in real time. In this study, we propose an advertising video recommendation procedure based on computer vision and deep learning, which uses changes in users’ facial expressions captured at every moment. Facial expressions represent a user’s emotions toward advertisements. We can utilize facial expressions to find a user’s dynamic preferences. For such a purpose, a CNN-based prediction model was developed to predict ratings, and a SIFT algorithm-based similarity model was developed to search for users with similar preferences in real time. To evaluate the proposed recommendation procedure, we experimented with food advertising videos. The experimental results show that the proposed procedure is superior to benchmark systems such as a random recommendation, an average rating approach, and a typical collaborative filtering approach in recommending advertising videos to both existing users and new users. From these results, we conclude that facial expressions are a critical factor for advertising video recommendations and are helpful in properly addressing the new user problem in existing recommender systems. © 2021 Elsevier B.V., All rights reserved.","Computer vision; Convolutional neural network; Deep learning; Recommender system; Scale-invariant feature transform","","","","Funding: This research was supported by the BK21 FOUR Program (5199990913932) funded by the Ministry of Education (MOE, Korea) and the National Research Foundation of Korea (NRF), and the Industrial Strategic Technology Development Program (20009050) funded by the Ministry of Trade, Industry and Energy (MOTIE, Korea).","Krishnan, S. Shunmuga, Understanding the effectiveness of video ads: A measurement study, pp. 149-162, (2013); Choi, Ilyoung, Collaborative filtering with facial expressions for online video recommendation, International Journal of Information Management, 36, 3, pp. 397-402, (2016); Annual Review of Psychology, (1979); Somerville, Leah H., Behavioral and neural representation of emotional facial expressions across the lifespan, Developmental Neuropsychology, 36, 4, pp. 408-428, (2011); Kim, Hyea-kyeong, Customer-Driven Content Recommendation Over a Network of Customers, IEEE Transactions on Systems, Man, and Cybernetics Part A:Systems and Humans, 42, 1, pp. 48-56, (2012); Park, Deuk-hee, A literature review and classification of recommender systems research, Expert Systems with Applications, 39, 11, pp. 10059-10072, (2012); Kim, Jaekyeong, Customer satisfaction of recommender system: Examining accuracy and diversity in several types of recommendation approaches, Sustainability (Switzerland), 13, 11, (2021); Li, Qinglong, A hybrid cnn‐based review helpfulness filtering model for improving e‐commerce recommendation service, Applied Sciences (Switzerland), 11, 18, (2021); Lu, Jie, Recommender system application developments: A survey, Decision Support Systems, 74, pp. 12-32, (2015); Bobadilla, Jesús Bobadilla@upm Es, Recommender systems survey, Knowledge-Based Systems, 46, pp. 109-132, (2013)","","MDPI","","","","","","20763417","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85117566266"
"Hoti, K.; Chivers, P.T.; Hughes, J.D.","Hoti, Kreshnik (36627182800); Chivers, Paola Teresa (35145393400); Hughes, Jeff David (55241561700)","36627182800; 35145393400; 55241561700","Assessing procedural pain in infants: a feasibility study evaluating a point-of-care mobile solution based on automated facial analysis","2021","The Lancet Digital Health","3","10","","e623","e634","0","26","10.1016/S2589-7500(21)00129-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117434466&doi=10.1016%2FS2589-7500%2821%2900129-1&partnerID=40&md5=eac57b2b843d23e56f26ae588786df76","Faculty of Medicine, Universiteti i Prishtinës, Pristina, Serbia; Institute for Health Research, The University of Notre Dame Australia, Fremantle, Australia; School of Medical and Health Sciences, Edith Cowan University, Perth, Australia; Curtin Medical School, Perth, Australia","Hoti, Kreshnik, Faculty of Medicine, Universiteti i Prishtinës, Pristina, Serbia; Chivers, Paola Teresa, Institute for Health Research, The University of Notre Dame Australia, Fremantle, Australia, School of Medical and Health Sciences, Edith Cowan University, Perth, Australia; Hughes, Jeff David, Curtin Medical School, Perth, Australia","Background: The management of procedural pain in infants is suboptimal, in part, compounded by the scarcity of a simple, accurate, and reliable method of assessing such pain. In this study, we aimed to evaluate the psychometric properties of the PainChek Infant, a point-of-care mobile application that uses automated facial evaluation and analysis in the assessment of procedural pain in infants. Methods: Video recordings of 40 infants were randomly chosen from a purposely assembled digital library of 410 children undergoing immunisation as part of their standard care in Prishtina, Kosovo, between April 4, 2017, and July 11, 2018. For each infant recording, four 10 s video segments were extracted, corresponding to baseline, vaccine preparation, during vaccination, and recovery. Four trained assessors did pain assessments on the video segments of 30 infants, using PainChek Infant standard, PainChek Infant adaptive, the Neonatal Facial Coding System-Revised (NFCS-R) single, the NFCS-R multiple, and the Observer administered Visual Analogue Scale (ObsVAS), on two separate occasions. PainChek Infant's performance was compared to NFCS-R and ObsVAS using correlation in changes in pain scores, intra-rater and inter-rater reliability, and internal consistency. Findings: 4303 pain assessments were completed in two separate testing sessions, on Aug 31, and Oct 19, 2020. The study involved videos of 40 infants aged 2·2–6·9 months (median age 3·4 months [IQR 2·3–4·5]). All pain assessment tools showed significant changes in the recorded pain scores across the four video segments (p≤0·0006). All tools were found to be responsive to procedure-induced pain, with the degree of change in pain scores not influenced by pre-vaccination pain levels. PainChek Infant pain scores showed good correlation with NFCS-R and ObsVAS scores (r=0·82–0·88; p<0·0001). PainChek Infant also showed good to excellent inter-rater reliability (ICC=0·81–0·97, p<0·001) and high levels of internal consistency (α=0·82–0·97). Interpretation: PainChek Infant's use of automated facial expression analysis could offer a valid and reliable means of assessing and monitoring procedural pain in infants. Its clinical utility in clinical practice requires further research. Funding: PainChek. © 2022 Elsevier B.V., All rights reserved.","","algorithm; Article; artificial intelligence; behavior change; cancer chemotherapy; child; construct validity; data analysis; distress syndrome; facial expression; feasibility study; female; head movement; health care quality; human; human experiment; immunization; infant; internal consistency; interrater reliability; major clinical study; male; pain assessment; population; procedural pain; psychometry; questionnaire; sample size; vaccination; videorecording; visual analog scale; pain measurement; phenotype; photography; point of care system; procedures; reproducibility; Facial Expression; Feasibility Studies; Female; Humans; Infant; Male; Pain Measurement; Pain, Procedural; Phenotype; Photography; Point-of-Care Systems; Psychometrics; Reproducibility of Results","","","","Czarnecki, Michelle L., Procedural Pain Management: A Position Statement with Clinical Practice Recommendations, Pain Management Nursing, 12, 2, pp. 95-111, (2011); Rev Pain, (2011); Shave, Kassi, Procedural pain in children: A qualitative study of caregiver experiences and information needs, BMC Pediatrics, 18, 1, (2018); Pancekauskaitė, Gabija, Paediatric pain medicine: Pain differences, recognition and coping acute procedural pain in paediatric emergency room, Medicina (Lithuania), 54, 6, (2018); Eccleston, Christopher, Delivering transformative action in paediatric pain: a Lancet Child & Adolescent Health Commission, The Lancet Child and Adolescent Health, 5, 1, pp. 47-87, (2021); Bradford, Judith Young, Clinical Practice Guideline: Needle-Related or Minor Procedural Pain in Pediatric Patients, Journal of Emergency Nursing, 45, 4, pp. 437-437.e32, (2019); Herr, Keela Ann, Pain Assessment in the Patient Unable to Self-Report: Clinical Practice Recommendations in Support of the ASPMN 2019 Position Statement, Pain Management Nursing, 20, 5, pp. 404-417, (2019); Automated Pain Detection from Facial Expressions Using Facs A Review, (2018); Zamzmi, Ghada, A Review of Automated Pain Assessment in Infants: Features, Classification Tasks, and Databases, IEEE Reviews in Biomedical Engineering, 11, pp. 77-96, (2018); Grunau, Ruth E., Pain expression in neonates: facial action and cry, Pain, 28, 3, pp. 395-410, (1987)","","Elsevier Ltd","","","","","","25897500","","","34481769","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85117434466"
"Porras, A.R.; Rosenbaum, K.; Tor-Díez, C.; Summar, M.; Linguraru, M.G.","Porras, Antonio Reyes (39863441100); Rosenbaum, Kenneth N. (7005888620); Tor-Díez, Carlos (57204032876); Summar, Marshall L. (56827820700); Linguraru, Marius George (35588324900)","39863441100; 7005888620; 57204032876; 56827820700; 35588324900","Development and evaluation of a machine learning-based point-of-care screening tool for genetic syndromes in children: a multinational retrospective study","2021","The Lancet Digital Health","3","10","","e635","e643","0","56","10.1016/S2589-7500(21)00137-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116766619&doi=10.1016%2FS2589-7500%2821%2900137-0&partnerID=40&md5=594858602230587fb156cdbbb3547375","Sheikh Zayed Institute for Pediatric Surgical Innovation, Childrens National Health System, Washington, D.C., United States; Department of Biostatistics & Informatics, University of Colorado Anschutz Medical Campus, Aurora, United States; Division of Genetics and Metabolism, Childrens National Health System, Washington, D.C., United States; Departments of Radiology and Pediatrics, The George Washington University School of Medicine and Health Sciences, Washington, D.C., United States","Porras, Antonio Reyes, Sheikh Zayed Institute for Pediatric Surgical Innovation, Childrens National Health System, Washington, D.C., United States, Department of Biostatistics & Informatics, University of Colorado Anschutz Medical Campus, Aurora, United States; Rosenbaum, Kenneth N., Division of Genetics and Metabolism, Childrens National Health System, Washington, D.C., United States; Tor-Díez, Carlos, Sheikh Zayed Institute for Pediatric Surgical Innovation, Childrens National Health System, Washington, D.C., United States; Summar, Marshall L., Division of Genetics and Metabolism, Childrens National Health System, Washington, D.C., United States; Linguraru, Marius George, Sheikh Zayed Institute for Pediatric Surgical Innovation, Childrens National Health System, Washington, D.C., United States, Departments of Radiology and Pediatrics, The George Washington University School of Medicine and Health Sciences, Washington, D.C., United States","Background: Delays in the diagnosis of genetic syndromes are common, particularly in low and middle-income countries with limited access to genetic screening services. We, therefore, aimed to develop and evaluate a machine learning-based screening technology using facial photographs to evaluate a child's risk of presenting with a genetic syndrome for use at the point of care. Methods: In this retrospective study, we developed a facial deep phenotyping technology based on deep neural networks and facial statistical shape models to screen children for genetic syndromes. We trained the machine learning models on facial photographs from children (aged <21 years) with a clinical or molecular diagnosis of a genetic syndrome and controls without a genetic syndrome matched for age, sex, and race or ethnicity. Images were obtained from three publicly available databases (the Atlas of Human Malformations in Diverse Populations of the National Human Genome Research Institute, Face2Gene, and the dataset available from Ferry and colleagues) and the archives of the Children's National Hospital (Washington, DC, USA), in addition to photographs taken on a standard smartphone at the Children's National Hospital. We designed a deep learning architecture structured into three neural networks, which performed image standardisation (Network A), facial morphology detection (Network B), and genetic syndrome risk estimation, accounting for phenotypic variations due to age, sex, and race or ethnicity (Network C). Data were divided randomly into 40 groups for cross validation, and the performance of the model was evaluated in terms of accuracy, sensitivity, and specificity in both the total population and stratified by race or ethnicity, age, and sex. Findings: Our dataset included 2800 facial photographs of children (1318 [47%] female and 1482 [53%] male; 1576 [56%] White, 432 [15%] African, 430 [15%] Hispanic, and 362 [13%] Asian). 1400 children with 128 genetic conditions were included (the most prevalent being Williams-Beuren syndrome [19%], Cornelia de Lange syndrome [17%], Down syndrome [16%], 22q11.2 deletion [13%], and Noonan syndrome [12%] syndrome) in addition to 1400 photographs of matched controls. In the total population, our deep learning-based model had an accuracy of 88% (95% CI 87–89) for the detection of a genetic syndrome, with 90% sensitivity (95% CI 88–92) and 86% specificity (95% CI 84–88). Accuracy was greater in White (90%, 89–91) and Hispanic populations (91%, 88–94) than in African (84%, 81–87) and Asian populations (82%, 78–86). Accuracy was also similar in male (89%, 87–91) and female children (87%, 85–89), and similar in children younger than 2 years (86%, 84–88) and children aged 2 years or older (eg, 89% [87–91] for those aged 2 years to <5 years). Interpretation: This genetic screening technology could support early risk stratification at the point of care in global populations, which has the potential accelerate diagnosis and reduce mortality and morbidity through preventive care. Funding: Children's National Hospital and Government of Abu Dhabi. © 2022 Elsevier B.V., All rights reserved.","","adolescent; African; age; Article; artificial neural network; Asian; Caucasian; child; controlled study; data base; de Lange syndrome; deep neural network; diagnostic accuracy; diagnostic test accuracy study; disease risk assessment; Down syndrome; ethnicity; face; face profile; female; genetic disorder; genetic screening; groups by age; Hispanic; human; image analysis; intermethod comparison; machine learning; male; medical photography; molecular diagnosis; Noonan syndrome; pediatric patient; point of care testing; race; race difference; retrospective study; sensitivity and specificity; sex; Williams Beuren syndrome; Africa; Asia; facial expression; infant; international cooperation; phenotype; photography; point of care system; reproducibility; risk assessment; European Continental Ancestry Group; Face; Facial Expression; Female; Genetic Diseases, Inborn; Hispanic Americans; Humans; Infant; Internationality; Machine Learning; Male; Phenotype; Photography; Point-of-Care Systems; Reproducibility of Results; Retrospective Studies; Risk Assessment; Sensitivity and Specificity","","","We thank Elijah Biggs, Alec Boyle, Daniel Perez, and Sofia Gonzalez for their help with data curation at the Children's National Hospital. Support for this work was partially provided by the Rare Disease Institute at the Children's National Hospital. Partial support for this work was also provided by a philanthropic gift from the Government of Abu Dhabi to the Children's National Hospital. Editorial note: the Lancet Group takes a neutral position with respect to territorial claims in published maps and institutional affiliations.","Baird, Patricia A., Genetic disorders in children and young adults: A population study, American Journal of Human Genetics, 42, 5, pp. 677-693, (1988); Gonzaludo, Nina P., Estimating the burden and economic impact of pediatric genetic disease, Genetics in Medicine, 21, 8, pp. 1781-1789, (2019); Colquitt, John L., Cardiac Findings in Noonan Syndrome on Long-term Follow-up, Congenital Heart Disease, 9, 2, pp. 144-150, (2014); Tekola-Ayele, Fasil, Translational Genomics in Low- and Middle-Income Countries: Opportunities and Challenges, Public Health Genomics, 18, 4, pp. 242-247, (2015); Presson, Angela P., Current estimate of down syndrome population prevalence in the United States, Journal of Pediatrics, 163, 4, pp. 1163-1168, (2013); Christianson, Arnold L., Medical genetics in developing countries, Annual Review of Genomics and Human Genetics, 5, pp. 219-265, (2004); Trop J Obstet Gynaecol, (2019); Kruszka, Paul S., Down syndrome in diverse populations, American Journal of Medical Genetics, Part A, 173, 1, pp. 42-53, (2017); Kruszka, Paul S., 22q11.2 deletion syndrome in diverse populations, American Journal of Medical Genetics, Part A, 173, 4, pp. 879-888, (2017); Kruszka, Paul S., Noonan syndrome in diverse populations, American Journal of Medical Genetics, Part A, 173, 9, pp. 2323-2334, (2017)","","Elsevier Ltd","","","","","","25897500","","","34481768","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85116766619"
"Mazziotti, R.; Carrara, F.; Viglione, A.; Lupori, L.; Lo Verde, L.L.; Benedetto, A.; Ricci, G.; Sagona, G.; Amato, G.; Pizzorusso, T.","Mazziotti, Raffaele (57110486800); Carrara, Fabio (57021334000); Viglione, Aurelia (57193907749); Lupori, Leonardo (57189745304); Lo Verde, Luca (57193194902); Benedetto, Alessandro (57189371872); Ricci, Giulia (57289000800); Sagona, Giulia (57196020708); Amato, Giuseppe (35963890600); Pizzorusso, Tommaso (7004199973)","57110486800; 57021334000; 57193907749; 57189745304; 57193194902; 57189371872; 57289000800; 57196020708; 35963890600; 7004199973","Meye: Web-app for translational and real-time pupillometry","2021","eNeuro","8","5","ENEURO.0122-21.2021","","","0","18","10.1523/ENEURO.0122-21.2021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116699399&doi=10.1523%2FENEURO.0122-21.2021&partnerID=40&md5=1b8fb74eee6c3958db0084817f06892a","Department of Neuroscience Psychology, Università degli Studi di Firenze, Florence, Italy; Institute of Neuroscience, Pisa, Pisa, Italy; Department of Developmental Neuroscience, IRCCS Fondazione Stella Maris, San Miniato, Italy; Bio@SNS Laboratory, Scuola Normale Superiore di Pisa, Pisa, Italy; Istituto di Scienza e Tecnologie dell'Informazione A. Faedo, Pisa, Italy","Mazziotti, Raffaele, Department of Neuroscience Psychology, Università degli Studi di Firenze, Florence, Italy, Institute of Neuroscience, Pisa, Pisa, Italy; Carrara, Fabio, Istituto di Scienza e Tecnologie dell'Informazione A. Faedo, Pisa, Italy; Viglione, Aurelia, Institute of Neuroscience, Pisa, Pisa, Italy, Bio@SNS Laboratory, Scuola Normale Superiore di Pisa, Pisa, Italy; Lupori, Leonardo, Institute of Neuroscience, Pisa, Pisa, Italy, Bio@SNS Laboratory, Scuola Normale Superiore di Pisa, Pisa, Italy; Lo Verde, Luca, Institute of Neuroscience, Pisa, Pisa, Italy; Benedetto, Alessandro, Institute of Neuroscience, Pisa, Pisa, Italy; Ricci, Giulia, Department of Neuroscience Psychology, Università degli Studi di Firenze, Florence, Italy; Sagona, Giulia, Institute of Neuroscience, Pisa, Pisa, Italy, Department of Developmental Neuroscience, IRCCS Fondazione Stella Maris, San Miniato, Italy; Amato, Giuseppe, Istituto di Scienza e Tecnologie dell'Informazione A. Faedo, Pisa, Italy; Pizzorusso, Tommaso, Department of Neuroscience Psychology, Università degli Studi di Firenze, Florence, Italy, Institute of Neuroscience, Pisa, Pisa, Italy, Bio@SNS Laboratory, Scuola Normale Superiore di Pisa, Pisa, Italy","Pupil dynamics alterations have been found in patients affected by a variety of neuropsychiatric conditions, including autism. Studies in mouse models have used pupillometry for phenotypic assessment and as a proxy for arousal. Both in mice and humans, pupillometry is noninvasive and allows for longitudinal experiments supporting temporal specificity; however, its measure requires dedicated setups. Here, we introduce a convolutional neural network that performs online pupillometry in both mice and humans in a web app format. This solution dramatically simplifies the usage of the tool for the nonspecialist and nontechnical operators. Because a modern web browser is the only software requirement, this choice is of great interest given its easy deployment and setup time reduction. The tested model performances indicate that the tool is sensitive enough to detect both locomotor-induced and stimulus-evoked pupillary changes, and its output is comparable to state-of-the-art commercial devices. © 2022 Elsevier B.V., All rights reserved.","Arousal; Neural network; Oddball; Pupillometry; Virtual reality; Web app","animal experiment; arousal; article; controlled study; convolutional neural network; human; male; mouse; nonhuman; pupillometry; virtual reality; web browser; animal; mobile application; pupil; Animals; Arousal; Humans; Mice; Mobile Applications; Pupil","","","We gratefully acknowledge NVIDIA Corporation's support with the Jetson AGX Xavier Developer Kit's donation for this research. The authors would also like to thank Dr. Viviana Marchi, Dr. Grazia Rutigliano, and Dr. Carlo Campagnoli for the manuscript's critical reading.","Aggius-Vella, Elena, Non-spatial skills differ in the front and rear peri-personal space, Neuropsychologia, 147, (2020); Alemán, Tomás S., Impairment of the transient pupillary light reflex in Rpe65-/- mice and humans with leber congenital amaurosis, Investigative Ophthalmology and Visual Science, 45, 4, pp. 1259-1271, (2004); Angulo-Chavira, Armando Quetzalcóatl, Pupil response and attention skills in Down syndrome, Research in Developmental Disabilities, 70, pp. 40-49, (2017); Artoni, Pietro, Deep learning of spontaneous arousal fluctuations detects early cholinergic defects across neurodevelopmental mouse models and patients, Proceedings of the National Academy of Sciences of the United States of America, 117, 38, pp. 23298-23303, (2020); Azevedo-Santos, Isabela Freire, Pain measurement techniques: Spotlight on mechanically ventilated patients, Journal of Pain Research, 11, pp. 2969-2980, (2018); Beggiato, Matthias, Using Smartbands, Pupillometry and Body Motion to Detect Discomfort in Automated Driving, Frontiers in Human Neuroscience, 12, (2018); Binda, Paola, Attention to bright surfaces enhances the pupillary light reflex, Journal of Neuroscience, 33, 5, pp. 2199-2204, (2013); Binda, Paola, Pupil constrictions to photographs of the sun, Journal of Vision, 13, 6, (2013); Blaser, Erik A., Pupillometry reveals a mechanism for the Autism Spectrum Disorder (ASD) advantage in visual tasks, Scientific Reports, 4, (2014); Brainard, David H., The Psychophysics Toolbox, Spatial Vision, 10, 4, pp. 433-436, (1997)","","Society for Neuroscience","","","","","","23732822","","","34518364","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85116699399"
"Lin, R.F.; Cheng, S.-H.; Liu, Y.-P.; Chen, C.-P.; Wang, Y.-J.; Chang, S.-Y.","Lin, Ray F. (55681906400); Cheng, Shuhsing (55626088300); Liu, Yungping (12801340300); Chen, Chengpin (57204287063); Wang, Yijyun (57253274700); Chang, Shuying (57253399000)","55681906400; 55626088300; 12801340300; 57204287063; 57253274700; 57253399000","Predicting emotional valence of people living with the human immunodeficiency virus using daily voice clips: A preliminary study","2021","Healthcare (Switzerland)","9","9","1148","","","0","1","10.3390/healthcare9091148","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114651519&doi=10.3390%2Fhealthcare9091148&partnerID=40&md5=28fef636b60824f61b83313b0b8e73fc","Department of Industrial Engineering and Management, Yuan Ze University, Taoyuan, Taiwan; Department of Infectious Diseases, Tao-Yuan General Hospital, Taoyuan, Taiwan; School of Public Health, Taipei Medical University, Taipei, Taiwan; Department of Industrial Engineering and Management, Chaoyang University of Technology, Taichung, Taiwan; Institute of Clinical Medicine, National Yang Ming Chiao Tung University, Hsinchu, Taiwan","Lin, Ray F., Department of Industrial Engineering and Management, Yuan Ze University, Taoyuan, Taiwan; Cheng, Shuhsing, Department of Infectious Diseases, Tao-Yuan General Hospital, Taoyuan, Taiwan, School of Public Health, Taipei Medical University, Taipei, Taiwan; Liu, Yungping, Department of Industrial Engineering and Management, Chaoyang University of Technology, Taichung, Taiwan; Chen, Chengpin, Department of Infectious Diseases, Tao-Yuan General Hospital, Taoyuan, Taiwan, Institute of Clinical Medicine, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Wang, Yijyun, Department of Industrial Engineering and Management, Yuan Ze University, Taoyuan, Taiwan; Chang, Shuying, Department of Infectious Diseases, Tao-Yuan General Hospital, Taoyuan, Taiwan","To detect depression in people living with the human immunodeficiency virus (PLHIV), this preliminary study developed an artificial intelligence (AI) model aimed at discriminating the emotional valence of PLHIV. Sixteen PLHIV recruited from the Taoyuan General Hospital, Ministry of Health and Welfare, participated in this study from 2019 to 2020. A self‐developed mobile application (app) was installed on sixteen participants’ mobile phones and recorded their daily voice clips and emotional valence values. After data preprocessing of the collected voice clips was conducted, an open‐source software, openSMILE, was applied to extract 384 voice features. These features were then tested with statistical methods to screen critical modeling features. Several decision‐tree models were built based on various data combinations to test the effectiveness of feature selection methods. The developed model performed very well for individuals who reported an adequate amount of data with widely distributed valence values. The effectiveness of feature selection methods, limitations of collected data, and future research were discussed. © 2021 Elsevier B.V., All rights reserved.","Artificial intelligence; Clinical diagnosis; Feature selection; HIV; Speech emotion recognition","","","","Funding: This research was funded by Hospital and Social Welfare Organizations Administration Commission, Ministry of Health and Welfare, grant number 10807, 10908, and 11010 and Ministry of Science and Technology, MOST107‐2221‐E‐155 ‐033 ‐MY3.","Ruffieux, Yann, Mortality from suicide among people living with HIV and the general Swiss population: 1988-2017, Journal of the International AIDS Society, 22, 8, (2019); Ciesla, Jeffrey A., Meta-analysis of the relationship between HIV infection and risk for depressive disorders, American Journal of Psychiatry, 158, 5, pp. 725-730, (2001); Cook, Judith A., Prevalence, Comorbidity, and Correlates of Psychiatric and Substance Use Disorders and Associations with HIV Risk Behaviors in a Multisite Cohort of Women Living with HIV, AIDS and Behavior, 22, 10, pp. 3141-3154, (2018); Grov, Christian, Loneliness and HIV-related stigma explain depression among older HIV-positive adults, AIDS Care - Psychological and Socio-Medical Aspects of AIDS/HIV, 22, 5, pp. 630-639, (2010); Zeng, Chengbo, A structural equation model of perceived and internalized stigma, depression, and suicidal status among people living with HIV/AIDS, BMC Public Health, 18, 1, (2018); Rubin, Leah H., HIV, Depression, and Cognitive Impairment in the Era of Effective Antiretroviral Therapy, Current HIV/AIDS Reports, 16, 1, pp. 82-95, (2019); Arseniou, Stylianos, HIV infection and depression., Psychiatry and Clinical Neurosciences, 68, 2, pp. 96-109, (2014); Wang, Tingting, Prevalence of depression or depressive symptoms among people living with HIV/AIDS in China: A systematic review and meta-analysis, BMC Psychiatry, 18, 1, (2018); Padilla, Mabel, Mental health, substance use and HIV risk behaviors among HIV-positive adults who experienced homelessness in the United States–Medical Monitoring Project, 2009–2015, AIDS Care - Psychological and Socio-Medical Aspects of AIDS/HIV, 32, 5, pp. 594-599, (2020); Miller, Theodore R., A Biopsychosocial Approach to Managing HIV-Related Pain and Associated Substance Abuse in Older Adults: a Review, Ageing International, 44, 1, pp. 74-116, (2019)","","MDPI","","","","","","22279032","","","","English","Article","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85114651519"
"Chang, Z.; Di Martino, J.M.; Aiello, R.; Baker, J.; Carpenter, K.; Compton, S.; Davis, N.; Eichner, B.; Espinosa, S.; Flowers, J.; Franz, L.; Harris, A.; Howard, J.; Perochon, S.; Perrin, E.M.; Krishnappa Babu, P.R.; Spanos, M.; Sullivan, C.; Walter, B.K.; Kollins, S.H.; Dawson, G.; Sapiro, G.","Chang, Zhuoqing (57190190445); Di Martino, J. Matías (36664485600); Aiello, Rachel E. (57147175300); Baker, Jeffrey P. (7404126939); Carpenter, Kimberly L.H. (56303131100); Compton, Scott N. (55409539800); Davis, Naomi Ornstein (14827849000); Eichner, Brian H. (35798146300); Espinosa, Steven (14017589600); Flowers, Jacqueline (57215024980); Franz, Lauren (26221735600); Harris, Adrianne A. (57143352800); Howard, Jill (57216863537); Perochon, Sam (57222180368); Perrin, Eliana Miller (7102308294); Krishnappa Babu, Pradeep Raj (57204900977); Spanos, Marina (24477904200); Sullivan, Connor P. (55931041700); Walter, Barbara Keith (56389296200); Kollins, Scott Haden (6603784387); Dawson, Geraldine (59028708100); Sapiro, Guillermo R. (7005450011)","57190190445; 36664485600; 57147175300; 7404126939; 56303131100; 55409539800; 14827849000; 35798146300; 14017589600; 57215024980; 26221735600; 57143352800; 57216863537; 57222180368; 7102308294; 57204900977; 24477904200; 55931041700; 56389296200; 6603784387; 59028708100; 7005450011","Computational Methods to Measure Patterns of Gaze in Toddlers with Autism Spectrum Disorder","2021","JAMA Pediatrics","175","8","","827","836","0","63","10.1001/jamapediatrics.2021.0530","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105129011&doi=10.1001%2Fjamapediatrics.2021.0530&partnerID=40&md5=e6f0ff798640d9a6ad659fa6cd3d0bb6","Pratt School of Engineering, Durham, United States; Duke University School of Medicine, Durham, United States; Duke University, Durham, United States; Duke University School of Medicine, Durham, United States; Office of Information Technology, Duke University, Durham, United States; Duke University School of Medicine, Durham, United States; Department of Psychology and Neuroscience, Duke University, Durham, United States; École Normale Supérieure Paris-Saclay, Gif-sur-Yvette, France; Duke University School of Medicine, Durham, United States; Duke University School of Medicine, Durham, United States; Mathematics, Duke University, Durham, United States","Chang, Zhuoqing, Pratt School of Engineering, Durham, United States; Di Martino, J. Matías, Pratt School of Engineering, Durham, United States; Aiello, Rachel E., Duke University School of Medicine, Durham, United States, Duke University, Durham, United States; Baker, Jeffrey P., Duke University School of Medicine, Durham, United States; Carpenter, Kimberly L.H., Duke University School of Medicine, Durham, United States, Duke University, Durham, United States; Compton, Scott N., Duke University School of Medicine, Durham, United States, Duke University, Durham, United States; Davis, Naomi Ornstein, Duke University School of Medicine, Durham, United States, Duke University, Durham, United States; Eichner, Brian H., Duke University School of Medicine, Durham, United States; Espinosa, Steven, Office of Information Technology, Duke University, Durham, United States; Flowers, Jacqueline, Duke University School of Medicine, Durham, United States, Duke University, Durham, United States; Franz, Lauren, Duke University School of Medicine, Durham, United States, Duke University, Durham, United States, Duke University School of Medicine, Durham, United States; Harris, Adrianne A., Duke University, Durham, United States, Department of Psychology and Neuroscience, Duke University, Durham, United States; Howard, Jill, Duke University School of Medicine, Durham, United States, Duke University, Durham, United States; Perochon, Sam, Pratt School of Engineering, Durham, United States, École Normale Supérieure Paris-Saclay, Gif-sur-Yvette, France; Perrin, Eliana Miller, Duke University School of Medicine, Durham, United States, Duke University School of Medicine, Durham, United States; Krishnappa Babu, Pradeep Raj, Pratt School of Engineering, Durham, United States; Spanos, Marina, Duke University School of Medicine, Durham, United States, Duke University, Durham, United States; Sullivan, Connor P., Duke University School of Medicine, Durham, United States, Duke University, Durham, United States; Walter, Barbara Keith, Duke University School of Medicine, Durham, United States; Kollins, Scott Haden, Duke University School of Medicine, Durham, United States, Duke University, Durham, United States; Dawson, Geraldine, Duke University School of Medicine, Durham, United States, Duke University, Durham, United States, Duke University School of Medicine, Durham, United States, Duke University School of Medicine, Durham, United States; Sapiro, Guillermo R., Pratt School of Engineering, Durham, United States, Duke University, Durham, United States, Mathematics, Duke University, Durham, United States","Importance: Atypical eye gaze is an early-emerging symptom of autism spectrum disorder (ASD) and holds promise for autism screening. Current eye-tracking methods are expensive and require special equipment and calibration. There is a need for scalable, feasible methods for measuring eye gaze. Objective: Using computational methods based on computer vision analysis, we evaluated whether an app deployed on an iPhone or iPad that displayed strategically designed brief movies could elicit and quantify differences in eye-gaze patterns of toddlers with ASD vs typical development. Design, Setting, and Participants: A prospective study in pediatric primary care clinics was conducted from December 2018 to March 2020, comparing toddlers with and without ASD. Caregivers of 1564 toddlers were invited to participate during a well-child visit. A total of 993 toddlers (63%) completed study measures. Enrollment criteria were aged 16 to 38 months, healthy, English- or Spanish-speaking caregiver, and toddler able to sit and view the app. Participants were screened with the Modified Checklist for Autism in Toddlers-Revised With Follow-up during routine care. Children were referred by their pediatrician for diagnostic evaluation based on results of the checklist or if the caregiver or pediatrician was concerned. Forty toddlers subsequently were diagnosed with ASD. Exposures: A mobile app displayed on a smartphone or tablet. Main Outcomes and Measures: Computer vision analysis quantified eye-gaze patterns elicited by the app, which were compared between toddlers with ASD vs typical development. Results: Mean age of the sample was 21.1 months (range, 17.1-36.9 months), and 50.6% were boys, 59.8% White individuals, 16.5% Black individuals, 23.7% other race, and 16.9% Hispanic/Latino individuals. Distinctive eye-gaze patterns were detected in toddlers with ASD, characterized by reduced gaze to social stimuli and to salient social moments during the movies, and previously unknown deficits in coordination of gaze with speech sounds. The area under the receiver operating characteristic curve discriminating ASD vs non-ASD using multiple gaze features was 0.90 (95% CI, 0.82-0.97). Conclusions and Relevance: The app reliably measured both known and new gaze biomarkers that distinguished toddlers with ASD vs typical development. These novel results may have potential for developing scalable autism screening tools, exportable to natural settings, and enabling data sets amenable to machine learning. © 2021 Elsevier B.V., All rights reserved.","","African American; anatomic landmark; Article; autism; caregiver; Caucasian; child; computer vision; controlled study; conversation; coordination; developmental delay; face; follow up; gaze; head movement; Hispanic; human; language delay; male; mobile application; pediatrician; primary medical care; prospective study; race; social preference; toddler; eye fixation; female; infant; personal digital assistant; preschool child; primary health care; Autism Spectrum Disorder; Child, Preschool; Computers, Handheld; Female; Fixation, Ocular; Humans; Infant; Male; Mobile Applications; Primary Health Care; Prospective Studies","","","Funding text 1: supported by NIH Autism Centers of Excellence Award NICHD P50HD093074 (Dr Dawson, director; Drs Dawson and Kollins, Co-PIs), NIMH R01MH121329 (Drs Dawson and Sapiro, Co-PIs), and NIMH R01MH120093 (Drs Sapiro and Dawson, Co-PIs). Additional support was provided by The Marcus Foundation; the Simons Foundation; NSF-1712867; ONR N00014-18-1-2143 and N00014-20-1-233; NGA HM04761912010; Apple, Inc; Microsoft, Inc; Amazon Web Services; and Google, Inc.; Funding text 2: receiving royalties from Apple Inc from licensing during the conduct of the study. Dr Aiello reports receiving grants from the National Institutes of Health (NIH) during the conduct of the study and personal fees from Private Diagnostic Clinic outside the submitted work. Dr Baker reports having a patent pending (15141391) and royalties from Apple Inc. Dr Carpenter reports receiving grants from NIH during the conduct of the study, having a patent pending (15141391) for developing technology that has been licensed to Apple, Inc, and receiving financial compensation for it, along with Duke University. Dr Compton reports receiving grants from the National Institute of Mental Health (NIMH) and personal fees from Mursion, Inc during the conduct of the study. Dr Davis reports receiving grants from NIH during the conduct of the study and research support from Akili Interactive outside the submitted work. Mr Espinosa reports receiving grants from the National Institute of Child Health and Human Development (NICHD), grants from NIMH, and fees from Apple Storage during the conduct of the study, and had a patent pending (15141391) and royalties for technology licensed to Apple Inc. Mr Harris reports receiving grants from NIH during the conduct of the study, having a patent pending (15141391) for developing technology that has been licensed to Apple, Inc, and receiving financial compensation for it, along with Duke University. Dr Howard reports receiving personal fees from Roche outside the submitted work. Dr Kollins reports receiving grants from NIH during the conduct of the study. Dr Dawson reports receiving grants from NICHD (P50HD093074), NIMH (R01MH121329 and R01MH120093), The Marcus Foundation, and Simons Foundation; receiving support from Apple Inc for data storage; receiving software during the conduct of the study; receiving personal fees from Apple; being a consultant for Apple; developing technology and data related to the app that has been licensed to Apple Inc, from which both she and Duke University have benefited financially; receiving other from Janssen Scientific Advisory Board; receiving other from Akili Scientific Advisory Board; receiving personal fees from LabCorp Scientific Advisory Board, Roche Scientific Advisory Board, Tris Pharma Scientific Advisory Board; receiving consultant fees from the Gerson Lehrman Group, Guidepoint, Axial Ventures, and Teva Pharmaceutical; receiving other from DASIO CEO; and receiving book royalties from Guilford Press, Oxford University Press, and Springer Nature Press outside the submitted work. Dr Dawson also reports patents pending (1802952, 15141391, 1802942, and 16493754). Dr Sapiro reports receiving consultant fees from Apple at submission of the manuscript, speaker fees from Johnson & Johnson, nonfinancial support from DASIO (nonactive cofounder) during the conduct of the study, consultant fees from Restore3D on activities unrelated to this work, nonfinancial support from SIS as a board member (unrelated work), and consultant fees from Volvo on activities outside the submitted work. Dr Sapiro has a patent pending (15141391) for technology on behavioral coding and corresponding data licensed to Apple (Duke University has licensed technology to Apple), a patent for technology for video deblurring licensed to Adobe (licensed by Duke University) not related to this work, and a patent for technology for face recognition licensed to Coral (licensed by Duke University) not related to this work. After the manuscript was submitted, Dr Sapiro started a sabbatical and is currently working at Apple part-time. No other disclosures were reported.","Kennedy, Daniel Patrick, The social brain in psychiatric and neurological disorders, Trends in Cognitive Sciences, 16, 11, pp. 559-572, (2012); Haxby, James V., The distributed human neural system for face perception, Trends in Cognitive Sciences, 4, 6, pp. 223-233, (2000); Adolphs, Ralph, Cognitive neuroscience: Cognitive neuroscience of human social behaviour, Nature Reviews Neuroscience, 4, 3, pp. 165-178, (2003); Reynolds, Greg Durelle, The development of attentional biases for faces in infancy: A developmental systems perspective, Frontiers in Psychology, 9, FEB, (2018); Chawarska, Katarzyna, Decreased spontaneous attention to social scenes in 6-month-old infants later diagnosed with autism spectrum disorders, Biological Psychiatry, 74, 3, pp. 195-203, (2013); Webb, Sara Jane, The motivation for very early intervention for infants at high risk for autism spectrum disorders, International Journal of Speech-Language Pathology, 16, 1, pp. 36-42, (2014); Dawson, Geraldine, Early behavioral intervention is associated with normalized brain activity in young children with autism, Journal of the American Academy of Child and Adolescent Psychiatry, 51, 11, pp. 1150-1159, (2012); Jones, Emily J., Parent-delivered early intervention in infants at risk for ASD: Effects on electrophysiological and habituation measures of social attention, Autism Research, 10, 5, pp. 961-972, (2017); Chita-Tegmark, Meia, Social attention in ASD: A review and meta-analysis of eye-tracking studies, Research in Developmental Disabilities, 48, pp. 79-93, (2016); Klin, Ami J., Defining and quantifying the social phenotype in autism, American Journal of Psychiatry, 159, 6, pp. 895-908, (2002)","","American Medical Association","","","","","","21686211; 21686203","","","33900383","English","Article","Final","All Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85105129011"
"Liakopoulos, L.; Stagakis, N.; Zacharaki, E.I.; Moustakas, K.","Liakopoulos, Leonidas (57302763300); Stagakis, Nikolaos (57212527515); Zacharaki, Evangelia I. (14526218700); Moustakas, Konstantinos (6507331953)","57302763300; 57212527515; 14526218700; 6507331953","CNN-based stress and emotion recognition in ambulatory settings","2021","","","","9555508","","","0","14","10.1109/IISA52424.2021.9555508","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117468520&doi=10.1109%2FIISA52424.2021.9555508&partnerID=40&md5=9e6e62813f04e0bf219c375159c97753","Department of Electrical & Computer Engineering, University of Patras, Rio, Greece","Liakopoulos, Leonidas, Department of Electrical & Computer Engineering, University of Patras, Rio, Greece; Stagakis, Nikolaos, Department of Electrical & Computer Engineering, University of Patras, Rio, Greece; Zacharaki, Evangelia I., Department of Electrical & Computer Engineering, University of Patras, Rio, Greece; Moustakas, Konstantinos, Department of Electrical & Computer Engineering, University of Patras, Rio, Greece","Stress has been recognized as a major contributor in a number of mental, psychological or physical conditions which reduce the quality of human life. The monitoring of affective states through readily available wearables and unobtrusive sensors can allow to recognize early signs of stress and burn-out, thereby develop prevention policies to combat psychosocial risks. This study analyzes data from diverse sensing modalities with signal processing techniques and advanced machine learning approaches in order to unobtrusively recognize stress and negative emotions. We investigate the performance of - easy to obtain in ambulatory settings - heart rate signal and juxtapose it against multi-modal information from electrophysiological signals, facial expression features and body posture. For the former, we introduce 2D spectrograms into a convolutional neural network (CNN) and use the obtained activation maps as frequency patterns differentiating stressful conditions. For the rest of the sensors, we compare different classifiers (SVM, KNN, Random Forest, Neural Networks) and data fusion schemes. In addition, a second phase assessment is conceptualized for emotion recognition reflected in facial expressions using images from a smartphone's camera. Our CNN implementation in Android platform enables near real-time estimation of the instantaneous emotional expressions, which, when combined with stress-indicators, can help elucidating the relationship between stress and negative affective states. © 2021 Elsevier B.V., All rights reserved.","convolutional neural network; emotion recognition; facial expression; mHealth; spectrogram; stress detection","Convolution; Convolutional neural networks; Data fusion; Decision trees; Electrophysiology; Face recognition; Speech recognition; Affective state; Convolutional neural network; Emotion recognition; Facial Expressions; Human lives; Network-based; Physical conditions; Spectrograms; Stress detection; Stress recognition; Spectrographs","","","This work has been supported by the EU Horizon2020 funded project “Smart, Personalized and Adaptive ICT Solutions for Active, Healthy and Productive Ageing with enhanced Workability (Ageing@Work)” under Grant Agreement No. 826299.","DeLongis, Anita, The Impact of Daily Stress on Health and Mood: Psychological and Social Resources as Mediators, Journal of Personality and Social Psychology, 54, 3, pp. 486-495, (1988); J A Statistical Portrait European Union Eurostat, (2010); Stress in the Workplace Past Present and Future, (2001); Smets, Elena, Into the Wild: The Challenges of Physiological Stress Detection in Laboratory and Ambulatory Settings, IEEE Journal of Biomedical and Health Informatics, 23, 2, pp. 463-473, (2019); Panicker, Suja Sreejith, A survey of machine learning techniques in physiology based mental stress detection systems, Biocybernetics and Biomedical Engineering, 39, 2, pp. 444-469, (2019); Zhang, Jianhua, Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review, Information Fusion, 59, pp. 103-126, (2020); Costin, Hariton Nicolae, Mental stress detection using heart rate variability and morphologic variability of EeG signals, pp. 591-596, (2012); De-Santos-Sierra, Alberto, Two stress detection schemes based on physiological signals for real-time applications, pp. 364-367, (2010); Zubair, Muhammad, Smart wearable band for stress detection, (2015); Iccbr Workshops, (2005)","","Institute of Electrical and Electronics Engineers Inc.","","12th International Conference on Information, Intelligence, Systems and Applications, IISA 2021","","Virtual, Chania Crete","172453","","9781665400329","","","English","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85117468520"
"De Campos Souza, P.V.C.; Guimarães, A.J.; Araújo, V.S.; Lughofer, E.","De Campos Souza, Paulo Vitor (57218164676); Guimarães, Augusto Junio (57204780479); Araújo, Vanessa Souza (57206288108); Lughofer, Edwin David (6506767029)","57218164676; 57204780479; 57206288108; 6506767029","An intelligent Bayesian hybrid approach to help autism diagnosis","2021","Soft Computing","25","14","","9163","9183","0","9","10.1007/s00500-021-05877-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106441025&doi=10.1007%2Fs00500-021-05877-0&partnerID=40&md5=78834a21516bc4c047935190ea5a87d9","Department of Knowledge-Based Mathematical Systems, Johannes Kepler University Linz, Linz, Austria; Faculty Una of Betim, Betim, Brazil","De Campos Souza, Paulo Vitor, Department of Knowledge-Based Mathematical Systems, Johannes Kepler University Linz, Linz, Austria; Guimarães, Augusto Junio, Faculty Una of Betim, Betim, Brazil; Araújo, Vanessa Souza, Faculty Una of Betim, Betim, Brazil; Lughofer, Edwin David, Department of Knowledge-Based Mathematical Systems, Johannes Kepler University Linz, Linz, Austria","This paper proposes a Bayesian hybrid approach based on neural networks and fuzzy systems to construct fuzzy rules to assist experts in detecting features and relations regarding the presence of autism in human beings. The model proposed in this paper works with a database generated through mobile devices that deals with diagnoses of autistic characteristics in human beings who answer a series of questions in a mobile application. The Bayesian model works with the construction of Gaussian fuzzy neurons in the first and logical neurons in the second layer of the model to form a fuzzy inference system connected to an artificial neural network that activates a robust output neuron. The new fuzzy neural network model was compared with traditional state-of-the-art machine learning models based on high-dimensional based on real-world data sets comprising the autism occurrence in children, adults, and adolescents. The results (97.73- Children/94.32-Adolescent/97.28-Adult) demonstrate the efficiency of our new method in determining children, adolescents, and adults with autistic traits (being among the top performers among all ML models tested), can generate knowledge about the dataset through fuzzy rules. © 2021 Elsevier B.V., All rights reserved.","Autism prediction; Autistic spectrum disorder; Bayesian fuzzy clustering; Bayesian fuzzy neural network","Bayesian networks; Diseases; Fuzzy logic; Fuzzy neural networks; Fuzzy rules; Multilayer neural networks; Neurons; Fuzzy inference systems; Fuzzy neural network model; High-dimensional; Hybrid approach; Machine learning models; Mobile applications; Output neurons; State of the art; Fuzzy inference","","","The authors acknowledge the Austrian Science Fund (FWF) support: contract number P32272-N38, acronym IL-EFS.","Akay, Yasemin M., Using wavelet-based fuzzy neural networks, IEEE Engineering in Medicine and Biology Magazine, 13, 5, pp. 761-764, (1994); Ann Probab, (1993); Allen, William Hand, Fuzzy Neural Network-Based Health Monitoring for HVAC System Variable-Air-Volume Unit, IEEE Transactions on Industry Applications, 52, 3, pp. 2513-2524, (2016); Ando, Tatsuya, Fuzzy neural network applied to gene expression profiling for predicting the prognosis of diffuse large B-cell Lymphoma, Japanese Journal of Cancer Research, 93, 11, pp. 1207-1212, (2002); Bach, Francis R., Bolasso: Model consistent Lasso estimation through the bootstrap, pp. 33-40, (2008); Bahado-Singh, Ray Oliver, Artificial intelligence analysis of newborn leucocyte epigenomic markers for the prediction of autism, Brain Research, 1724, (2019); Baron-Cohen, Simon B., The Autism-Spectrum Quotient (AQ) - Adolescent version, Journal of Autism and Developmental Disorders, 36, 3, pp. 343-350, (2006); Bauer, Frank, Comparingparameter choice methods for regularization of ill-posed problems, Mathematics and Computers in Simulation, 81, 9, pp. 1795-1841, (2011); Bertoncelli, Carlo Mario, Using Artificial Intelligence to Identify Factors Associated with Autism Spectrum Disorder in Adolescents with Cerebral Palsy, Neuropediatrics, 50, 3, pp. 178-187, (2019); Pattern Recognition with Fuzzy Objective Function Algorithms, (1981)","","Springer Science and Business Media Deutschland GmbH","","","","","","14327643; 14337479","","","","English","Article","Final","All Open Access; Green Final Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85106441025"
"Yamamoto, T.; Mizuta, H.; Ueji, K.","Yamamoto, Toshiyuki (55703202300); Mizuta, Haruno (57203166210); Ueji, Kayoko (54917191700)","55703202300; 57203166210; 54917191700","Analysis of facial expressions in response to basic taste stimuli using artificial intelligence to predict perceived hedonic ratings","2021","PLOS ONE","16","5 May","e0250928","","","0","10","10.1371/journal.pone.0250928","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105318783&doi=10.1371%2Fjournal.pone.0250928&partnerID=40&md5=30634c9432aed1174bc2f5df5261a262","Kio University, Kitakatsuragi, Japan; Department of Nutrition, Kio University, Kitakatsuragi, Japan","Yamamoto, Toshiyuki, Kio University, Kitakatsuragi, Japan, Department of Nutrition, Kio University, Kitakatsuragi, Japan; Mizuta, Haruno, Kio University, Kitakatsuragi, Japan; Ueji, Kayoko, Department of Nutrition, Kio University, Kitakatsuragi, Japan","Taste stimuli can induce a variety of physiological reactions depending on the quality and/or hedonics (overall pleasure) of tastants, for which objective methods have long been desired. In this study, we used artificial intelligence (AI) technology to analyze facial expressions with the aim of assessing its utility as an objective method for the evaluation of food and beverage hedonics compared with conventional subjective (perceived) evaluation methods. The face of each participant (10 females; age range, 21-22 years) was photographed using a smartphone camera a few seconds after drinking 10 different solutions containing five basic tastes with different hedonic tones. Each image was then uploaded to an AI application to achieve outcomes for eight emotions (surprise, happiness, fear, neutral, disgust, sadness, anger, and embarrassment), with scores ranging from 0 to 100. For perceived evaluations, each participant also rated the hedonics of each solution from -10 (extremely unpleasant) to +10 (extremely pleasant). Based on these, we then conducted a multiple linear regression analysis to obtain a formula to predict perceived hedonic ratings. The applicability of the formula was examined by combining the emotion scores with another 11 taste solutions obtained from another 12 participants of both genders (age range, 22-59 years). The predicted hedonic ratings showed good correlation and concordance with the perceived ratings. To our knowledge, this is the first study to demonstrate a model that enables the prediction of hedonic ratings based on emotional facial expressions to food and beverage stimuli. © 2021 Elsevier B.V., All rights reserved.","","adult; Article; artificial intelligence; cohort analysis; controlled study; correlation analysis; emotion assessment; facial expression; facial recognition; female; human; human experiment; male; normal human; taste; anger; disgust; emotion; fear; happiness; middle aged; physiology; pleasure; sadness; young adult; Adult; Anger; Artificial Intelligence; Disgust; Emotions; Facial Expression; Fear; Female; Happiness; Humans; Male; Middle Aged; Pleasure; Sadness; Taste; Young Adult","","","","Kikut-Ligaj, Dariusz, How taste works: Cells, receptors and gustatory perception, Cellular and Molecular Biology Letters, 20, 5, pp. 699-716, (2015); Running, Cordelia A., Oleogustus: The unique taste of fat, Chemical Senses, 40, 7, pp. 507-516, (2015); Torrico, Damir Dennis, Cross-cultural effects of food product familiarity on sensory acceptability and non-invasive physiological responses of consumers, Food Research International, 115, pp. 439-450, (2019); Bartkienė, Elena, Factors Affecting Consumer Food Preferences: Food Taste and Depression-Based Evoked Emotional Expressions with the Use of Face Reading Technology, BioMed Research International, 2019, (2019); Ericson, Sune, The variability of the human parotid flow rate on stimulation with citric acid, with special reference to taste, Archives of Oral Biology, 16, 1, pp. 9-IN1, (1971); Kawamura, Yojiro, Studies on neural mechanisms of the gustatory‐salivary reflex in rabbits., Journal of Physiology, 285, 1, pp. 35-47, (1978); de Wijk, R. A., ANS responses and facial expressions differentiate between the taste of commercial breakfast drinks, PLOS ONE, 9, 4, (2014); Verastegui-Tena, Luz, Beyond expectations: The responses of the autonomic nervous system to visual food cues, Physiology and Behavior, 179, pp. 478-486, (2017); Kashima, Hideaki, Basic taste stimuli elicit unique responses in facial skin blood flow, PLOS ONE, 6, 12, (2011); Kashima, Hideaki, Palatability of tastes is associated with facial circulatory responses, Chemical Senses, 39, 3, pp. 243-248, (2014)","","Public Library of Science","","","","","","19326203","","POLNC","33945568","English","Article","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85105318783"
"Marquez Herbuela, V.R.D.M.; Karita, T.; Furukawa, Y.; Wada, Y.; Yagi, Y.; Senba, S.; Onishi, E.; Saeki, T.","Marquez Herbuela, Von Ralph Dane (57211965949); Karita, Tomonori (57193791513); Furukawa, Yoshiya (57052121600); Wada, Yoshinori (57221319642); Yagi, Yoshihiro (57224548219); Senba, Shuichiro (57221322755); Onishi, Eiko (57221317154); Saeki, Tatsuo (57221318517)","57211965949; 57193791513; 57052121600; 57221319642; 57224548219; 57221322755; 57221317154; 57221318517","Integrating behavior of children with profound intellectual, multiple, or severe motor disabilities with location and environment data sensors for independent communication and mobility: App development and pilot testing","2021","JMIR Rehabilitation and Assistive Technologies","8","2","e28020","","","0","3","10.2196/28020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107841455&doi=10.2196%2F28020&partnerID=40&md5=5c5fa782343716cafe7e60797a5f8448","Department of Special Education, Ehime University, Matsuyama, Japan; Graduate School of Humanities and Social Sciences, Hiroshima University, Higashihiroshima, Japan; Department of Contemporary Liberal Arts, Showa Women's University, Tokyo, Japan; Ltd., Matsuyama, Japan","Marquez Herbuela, Von Ralph Dane, Department of Special Education, Ehime University, Matsuyama, Japan; Karita, Tomonori, Department of Special Education, Ehime University, Matsuyama, Japan; Furukawa, Yoshiya, Department of Special Education, Ehime University, Matsuyama, Japan, Graduate School of Humanities and Social Sciences, Hiroshima University, Higashihiroshima, Japan; Wada, Yoshinori, Department of Special Education, Ehime University, Matsuyama, Japan; Yagi, Yoshihiro, Department of Special Education, Ehime University, Matsuyama, Japan, Department of Contemporary Liberal Arts, Showa Women's University, Tokyo, Japan; Senba, Shuichiro, Ltd., Matsuyama, Japan; Onishi, Eiko, Ltd., Matsuyama, Japan; Saeki, Tatsuo, Ltd., Matsuyama, Japan","Background: Children with profound intellectual and multiple disabilities (PIMD) or severe motor and intellectual disabilities (SMID) only communicate through movements, vocalizations, body postures, muscle tensions, or facial expressions on a pre- or protosymbolic level. Yet, to the best of our knowledge, there are few systems developed to specifically aid in categorizing and interpreting behaviors of children with PIMD or SMID to facilitate independent communication and mobility. Further, environmental data such as weather variables were found to have associations with human affects and behaviors among typically developing children; however, studies involving children with neurological functioning impairments that affect communication or those who have physical and/or motor disabilities are unexpectedly scarce. Objective: This paper describes the design and development of the ChildSIDE app, which collects and transmits data associated with children's behaviors, and linked location and environment information collected from data sources (GPS, iBeacon device, ALPS Sensor, and OpenWeatherMap application programming interface [API]) to the database. The aims of this study were to measure and compare the server/API performance of the app in detecting and transmitting environment data from the data sources to the database, and to categorize the movements associated with each behavior data as the basis for future development and analyses. Methods: This study utilized a cross-sectional observational design by performing multiple single-subject face-to-face and video-recorded sessions among purposively sampled child-caregiver dyads (children diagnosed with PIMD/SMID, or severe or profound intellectual disability and their primary caregivers) from September 2019 to February 2020. To measure the server/API performance of the app in detecting and transmitting data from data sources to the database, frequency distribution and percentages of 31 location and environment data parameters were computed and compared. To categorize which body parts or movements were involved in each behavior, the interrater agreement κ statistic was used. Results: The study comprised 150 sessions involving 20 child-caregiver dyads. The app collected 371 individual behavior data, 327 of which had associated location and environment data from data collection sources. The analyses revealed that ChildSIDE had a server/API performance >93% in detecting and transmitting outdoor location (GPS) and environment data (ALPS sensors, OpenWeatherMap API), whereas the performance with iBeacon data was lower (82.3%). Behaviors were manifested mainly through hand (22.8%) and body movements (27.7%), and vocalizations (21.6%). Conclusions: The ChildSIDE app is an effective tool in collecting the behavior data of children with PIMD/SMID. The app showed high server/API performance in detecting outdoor location and environment data from sensors and an online API to the database with a performance rate above 93%. The results of the analysis and categorization of behaviors suggest a need for a system that uses motion capture and trajectory analyses for developing machine- or deep-learning algorithms to predict the needs of children with PIMD/SMID in the future. © 2021 Elsevier B.V., All rights reserved.","AAC; App; Augmentative and alternative communication; Behavior; Child; Communication; Development; Mobile app development; Mobility; Pilot; Profound intellectual and multiple disabilities; Sensor; Severe motor and intellectual disabilities; Smartphone-based data collection","","","","Funding text 1: This study was supported by the Ministry of Internal Affairs and Communications' Strategic Information and Communications R&D Promotion Program (SCOPE) (MIC contract number: 181609003; research contract number 7; 2019507002; 2020507001) and the Japan Society for the Promotion of Science Grants-in-Aid for Scientific Research (B) (19H04228), which had no role in the design, data collection and analysis, and writing of this manuscript. YY and YF were affiliated with the Special Needs Education Department at Ehime University at the time of the study and are currently affiliated with Showa Women's University and Hiroshima University, respectively. The authors would like to thank all of the children and their caregivers who participated in this study.; Funding text 2: This study was supported by the Ministry of Internal Affairs and Communications’ Strategic Information and Communications R&D Promotion Program (SCOPE) (MIC contract number: 181609003; research contract number 7; 2019507002; 2020507001) and the Japan Society for the Promotion of Science Grants-in-Aid for Scientific Research (B) (19H04228), which had no role in the design, data collection and analysis, and writing of this manuscript. YY and YF were affiliated with the Special Needs Education Department at Ehime University at the time of the study and are currently affiliated with Showa Women's University and Hiroshima University, respectively. The authors would like to thank all of the children and their caregivers who participated in this study.","Journal of Policy and Practice in Intellectual Disabilities, (2007); Ozawa, Hiroshi, The present situation of children with psycho-motor disabilities and their parents, No To Hattatsu, 39, 4, pp. 279-282, (2007); Educating Children with Multiple Disabilities A Transdisciplinary Approach, (1996); Palisano, Robert J., Validation of a model of gross motor function for children with cerebral palsy, Physical Therapy, 80, 10, pp. 974-985, (2000); Oeseburg, Barth, Prevalence of chronic health conditions in children with intellectual disability: A systematic literature review, Intellectual and Developmental Disabilities, 49, 2, pp. 59-85, (2011); van Timmeren, E. A., Prevalence of reported physical health problems in people with severe or profound intellectual and motor disabilities: a cross-sectional study of medical records and care plans, Journal of Intellectual Disability Research, 60, 11, pp. 1109-1118, (2016); Karita, Tomonori, Development of a communication aid app with ios devices to support children/persons with speech disabilities : Improvement in obtaining positioning information with iBeacon as near field radio communication technology, Journal of Advanced Computational Intelligence and Intelligent Informatics, 21, 2, pp. 371-377, (2017); Scripts Plans Goals and Understanding, (1977); Ciucci, Enrica, Seasonal variation, weather and behavior in day-care children: A multilevel approach, International Journal of Biometeorology, 57, 6, pp. 845-856, (2013); Harrison, Flo C.D., Weather and children's physical activity; How and why do relationships vary between countries?, International Journal of Behavioral Nutrition and Physical Activity, 14, 1, (2017)","","JMIR Publications Inc.","","","","","","23692529","","","","English","Article","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85107841455"
"Wang, H.; Tobón, D.P.V.; Hossain, M.S.; El Saddik, A.","Wang, Haopeng (59074646700); Tobón, Diana P. (55863782600); Hossain, M. Shamim (24066717900); El Saddik, Abdulmotaleb Ei (35431360000)","59074646700; 55863782600; 24066717900; 35431360000","Deep Learning (DL)-Enabled System for Emotional Big Data","2021","IEEE Access","9","","","116073","116082","0","10","10.1109/ACCESS.2021.3103501","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153353257&doi=10.1109%2FACCESS.2021.3103501&partnerID=40&md5=befd263164ced164f1051befcb5eb98f","University of Ottawa, Ottawa, Canada; Department of Software Engineering, King Saud University, Riyadh, Saudi Arabia","Wang, Haopeng, University of Ottawa, Ottawa, Canada; Tobón, Diana P., University of Ottawa, Ottawa, Canada; Hossain, M. Shamim, Department of Software Engineering, King Saud University, Riyadh, Saudi Arabia; El Saddik, Abdulmotaleb Ei, University of Ottawa, Ottawa, Canada","Emotion care for human well-being is important for all ages. In this paper, we propose an emotion care system based on big data analysis for autism disorder patient training, where emotion is detected in terms of facial expression. The expression can be captured through a camera as well as Internet of Things (IoT)-enabled devices. The system works with deep learning techniques on emotional big data to extract emotional features and recognize six kinds of facial expressions in real-time and offline. A convolutional neural network (CNN) model based on MobileNet V1 structure is trained with two emotional datasets, FER-2013 dataset and a new proposed dataset named MCFER. The experiments on three strategies showed that the proposed system with deep learning model obtained an accuracy of 95.89%. The system can also detect and track multiple faces as well as recognize facial expressions with high performance on mobile devices with a speed of up to 12 frames per second. © 2023 Elsevier B.V., All rights reserved.","Convolutional neural network; emotion; facial expression recognition; mobile application","Convolution; Convolutional neural networks; Deep learning; Emotion Recognition; Face recognition; Internet of things; Learning systems; Neural network models; Autism disorders; Convolutional neural network; Emotion; Facial expression recognition; Facial Expressions; Learning techniques; Mobile applications; Patient training; Real- time; Well being; Big data","","","This work was supported by the Researchers Supporting Project number (RSP-2021/32), King Saud University, Riyadh, Saudi Arabia.","Ahmed, Ejaz, The role of big data analytics in Internet of Things, Computer Networks, 129, pp. 459-471, (2017); Lin, Kai, System Design for Big Data Application in Emotion-Aware Healthcare, IEEE Access, 4, pp. 6901-6909, (2016); Jiang, Yingying, A snapshot research and implementation of multimodal information fusion for data-driven emotion recognition, Information Fusion, 53, pp. 209-221, (2020); Chatterjee, Ankush, Understanding Emotions in Text Using Deep Learning and Big Data, Computers in Human Behavior, 93, pp. 309-317, (2019); Ma, Yaxiong, Audio-visual emotion fusion (AVEF): A deep efficient weighted approach, Information Fusion, 46, pp. 184-192, (2019); Hossain, M. Shamim, Emotion recognition using deep learning approach from audio–visual emotional big data, Information Fusion, 49, pp. 69-78, (2019); Miao, Yu, A deep learning system for recognizing facial expression in real-time, ACM Transactions on Multimedia Computing, Communications and Applications, 15, 2, (2019); Muhammad, Ghulam, EEG-Based Pathology Detection for Home Health Monitoring, IEEE Journal on Selected Areas in Communications, 39, 2, pp. 603-610, (2021); El Saddik, Abdulmotaleb Ei, Digital Twins: The Convergence of Multimedia Technologies, IEEE Multimedia, 25, 2, pp. 87-92, (2018); Chuah, Mooichoo, Smartphone based autism social alert system, pp. 6-13, (2012)","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21693536","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85153353257"
"Özçelik, N.; Selimoğlu, İ.","Özçelik, Neslihan (56033911200); Selimoğlu, İnci (57291364400)","56033911200; 57291364400","Artificial intelligence applications in pulmonology and its advantages during the pandemic period; Göğüs hastalıklarında yapay zeka uygulamaları ve pandemi döneminde sağ-ladığı avantajlar","2021","Tuberkuloz ve Toraks","69","3","","380","386","0","0","10.5578/tt.20219710","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116884635&doi=10.5578%2Ftt.20219710&partnerID=40&md5=2225082676c3ac8d952e7977bd811535","Department of Pulmonary Medicine, Recep Tayyip Erdogan University, Rize, Turkey","Özçelik, Neslihan, Department of Pulmonary Medicine, Recep Tayyip Erdogan University, Rize, Turkey; Selimoğlu, İnci, Department of Pulmonary Medicine, Recep Tayyip Erdogan University, Rize, Turkey","Artificial intelligence, with its increasing data volume, developing technolo-gies, more information processing power and new algorithms, has a wide usage area in all sectors. In the field of health, these technologies is gaining an increasing place every day. Artificial intelligence methods can act as a simulation of the human mind and intelligence, resulting in the analysis and classification of complex data in a short time. Thus, by separating the small differenc-es in the images examined, it can help diagnosis, detect preliminary signs of the disease and predict how the disease will develop. Computer based pro-grams; diagnostic algorithms, surgical support and robotic systems developed on the basis of patient data are increasingly used in the drug development industry. In this study, artificial intelligence applications in the field of health and its use in pulmonology, the place of wearable technologies in our department and the advantages they provide us during the pandemic period were discussed in the light of the literature. © 2021 Elsevier B.V., All rights reserved.","Artificial intelligence; Pandemic; Pulmonology; Wearable devices","algorithm; artificial intelligence; autism; chronic obstructive lung disease; computer assisted tomography; deep learning; drug development; echography; emphysema; endobronchial ultrasonography; gene editing; human; image quality; lung angiography; lung cancer; lung embolism; machine learning; mesothelioma; mobile application; pandemic; pneumothorax; polymerase chain reaction; pulmonology; Review; sleep disorder; telemedicine; thorax radiography; tuberculosis; Artificial Intelligence; Humans; Pandemics; Pulmonary Medicine","","","","Topol, Eric J., High-performance medicine: the convergence of human and artificial intelligence, Nature Medicine, 25, 1, pp. 44-56, (2019); Wang, Xiaosong, ChestX-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2017-January, pp. 3462-3471, (2017); Li, Zhe, Thoracic Disease Identification and Localization with Limited Supervision, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 8290-8299, (2018); Titano, Joseph J., Automated deep-neural-network surveillance of cranial images for acute neurologic events, Nature Medicine, 24, 9, pp. 1337-1341, (2018); Willems, Jos Louis H., The diagnostic performance of computer programs for the interpretation of electrocardiograms, New England Journal of Medicine, 325, 25, pp. 1767-1773, (1991); Esteva, Andre, Dermatologist-level classification of skin cancer with deep neural networks, Nature, 542, 7639, pp. 115-118, (2017); Haenssle, Holger Andreas, Man against Machine: Diagnostic performance of a deep learning convolutional neural network for dermoscopic melanoma recognition in comparison to 58 dermatologists, Annals of Oncology, 29, 8, pp. 1836-1842, (2018); Wong, Tienyin, Artificial intelligence with deep learning technology looks into diabetic retinopathy screening, JAMA, 316, 22, pp. 2366-2367, (2016); Cao, Bokai, DeepMood: Modeling mobile phone typing dynamics for mood detection, Part F129685, pp. 747-755, (2017); Fitzpatrick, Kathleen Kara, Delivering cognitive behavior therapy to young adults with symptoms of depression and anxiety using a fully automated conversational agent (Woebot): A randomized controlled trial, JMIR Mental Health, 4, 2, (2017)","","Ankara University","","","","","","04941373","","","34581159","English","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85116884635"
"Plasencia-Salgueiro, A.D.J.; Shichkina, Y.; García, A.G.; Rodríguez, L.G.","Plasencia-Salgueiro, Armando De Jesús (56741551300); Shichkina, Yulia A. (57144627300); García, Arlety (57226797141); Rodríguez, Lynnette González (57226792433)","56741551300; 57144627300; 57226797141; 57226792433","Parkinson's Disease Classification and Medication Adherence Monitoring Using Smartphone-based Gait Assessment and Deep Reinforcement Learning Algorithm","2021","Procedia Computer Science","186","","","546","554","0","11","10.1016/j.procs.2021.04.175","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112694680&doi=10.1016%2Fj.procs.2021.04.175&partnerID=40&md5=2cd09303122ed21446ce9d053aec0720","Instituto de Cibernética, Matemática y Física, Havana, Cuba; Sankt-Peterburgskij Gosudarstvennyj Elektrotehniceskij Universitet, Saint Petersburg, Russian Federation; Universidad de la Isla de la Juventud Jesús Montané Oropesa, Nueva Gerona, Cuba","Plasencia-Salgueiro, Armando De Jesús, Instituto de Cibernética, Matemática y Física, Havana, Cuba; Shichkina, Yulia A., Sankt-Peterburgskij Gosudarstvennyj Elektrotehniceskij Universitet, Saint Petersburg, Russian Federation; García, Arlety, Universidad de la Isla de la Juventud Jesús Montané Oropesa, Nueva Gerona, Cuba; Rodríguez, Lynnette González, Instituto de Cibernética, Matemática y Física, Havana, Cuba","For the diagnosis and classification of Parkinson's patients, the unified scale of Parkinson's patients (Unified Parkinson's Disease Rating Scale - UPDRS) is used, which requires the patient to perform a series of tests among which the biomarkers of speech, the facial expression, the hand movement and walking analysis are considered, after which the doctor diagnoses the patient whether or not he has Parkinson's disease according to the score obtained. The work proposes a system for monitoring patients with the use of cell phones and their automatic classification according to the data collected by them. The system starts from the budget that Parkinson's patients have different abnormalities when walking if they do not follow the required medication. The cell phone collects the data passively while the patient has his cell phone in his pocket. After that, the data preprocessor helps to extract the walking cycles that this Parkinson-related biomarker contains. The algorithm proposed for classification and Medication Adherence Monitoring is the Deep Reinforcement Learning. With this work we demonstrate the feasibility of using cell phones to monitor the biomarker walking in Parkinson's patients and the possibility of Passive Medication Adherence Monitoring and Dynamic Treatment Regimes. © 2021 Elsevier B.V., All rights reserved.","biomarkers monitoring; deep reinforcement learning; medication adherence; Parkinson's disease; smatphone","Biomarkers; Budget control; Classification (of information); Computer aided diagnosis; Decision trees; Learning algorithms; mHealth; Neurodegenerative diseases; Patient treatment; Reinforcement learning; Smartphones; Biomarker monitoring; Cell phone; Disease classification; Gait assessments; Medication adherence; Parkinson's disease; Reinforcement learning algorithms; Reinforcement learnings; Smart phones; Smatphone; Deep learning","","","The Project has the subventio n of the National Program “Automation, Ro botics and Artificial Intelligence” of the Ministry of Science and Technology of Cuba. The re ported study was funded by FBR and CITMR A according to the research project №18-57-34001.","Ix Taller Internacional De Cibernetica Aplicada, (2019); Int J Eng Sci Invent Ijesi, (2018); Proceedings of the ACM on Interactive Mobile Wearable and Ubiquitous Technologies, (2019); Thomas, Peter J., Control theory in biology and medicine: Introduction to the special issue, Biological Cybernetics, 113, 1-2, (2019); Haeri, Mohammad, Modeling the Parkinson's tremor and its treatments, Journal of Theoretical Biology, 236, 3, pp. 311-322, (2005); Reinforcement Learning in Healthcare A Survey, (2019); Salgueiro, Armando Plasencia, Open source robotic simulators platforms for teaching deep reinforcement learning algorithms, Procedia Computer Science, 150, pp. 162-170, (2019); Zhang, Xiang, Multi-modality sensor data classification with selective attention, IJCAI International Joint Conference on Artificial Intelligence, 2018-July, pp. 3111-3117, (2018); Shichkina, Yulia A., The Architecture of the System for Monitoring the Status in Patients with Parkinson’s Disease Using Mobile Technologies, Studies in Computational Intelligence, 868, pp. 531-540, (2020); Dima, Alexandra L., Computation of adherence to medication and visualization of medication histories in R with AdhereR: Towards transparent and reproducible use of electronic healthcare data, PLOS ONE, 12, 4, (2017)","Zelinka, I.; Pereira, F.M.FL.; Das, S.; Ilin, A.; Diveev, A.; Nikulchev, E.","Elsevier B.V.","","14th International Symposium on Intelligent Systems, INTELS 2020","","Moscow","169475","18770509","9781510849914","","","English","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85112694680"
"Bum, J.; Choo, H.; Whang, J.J.","Bum, Junghyun (57190274322); Choo, Hyunseung (7006228656); Whang, Joyce Jiyoung (58306607700)","57190274322; 7006228656; 58306607700","Image-based lifelogging: User emotion perspective","2021","Computers, Materials and Continua","67","2","","1963","1977","0","4","10.32604/cmc.2021.014931","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102563494&doi=10.32604%2Fcmc.2021.014931&partnerID=40&md5=fe21a07c8f06dea445b743089a26fafd","College of Computing, Sungkyunkwan University, Seoul, South Korea; School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","Bum, Junghyun, College of Computing, Sungkyunkwan University, Seoul, South Korea; Choo, Hyunseung, College of Computing, Sungkyunkwan University, Seoul, South Korea; Whang, Joyce Jiyoung, School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","Lifelog is a digital record of an individual s daily life. It collects, records, and archives a large amount of unstructured data; therefore, techniques are required to organize and summarize those data for easy retrieval. Lifelogging has been utilized for diverse applications including healthcare, self-Tracking, and entertainment, among others. With regard to the imagebased lifelogging, even though most users prefer to present photos with facial expressions that allow us to infer their emotions, there have been few studies on lifelogging techniques that focus upon users emotions. In this paper, we develop a system that extracts users own photos from their smartphones and configures their lifelogs with a focus on their emotions. We design an emotion classifier based on convolutional neural networks (CNN) to predict the users emotions. To train the model, we create a new dataset by collecting facial images from the CelebFaces Attributes (CelebA) dataset and labeling their facial emotion expressions, and by integrating parts of the Radboud Faces Database (RaFD). Our dataset consists of 4,715 high-resolution images. We propose Representative Emotional Data Extraction Scheme (REDES) to select representative photos based on inferring users emotions from their facial expressions. In addition, we develop a system that allows users to easily configure diaries for a special day and summaize their lifelogs. Our experimental results show that our method is able to effectively incorporate emotions into lifelog, allowing an enriched experience. © 2021 Elsevier B.V., All rights reserved.","Emotion; Emotion classifier; Facial expression; Lifelog; Transfer learning","Convolutional neural networks; mHealth; Data extraction; Digital records; Diverse applications; Facial emotions; Facial Expressions; High resolution image; Large amounts; Unstructured data; Image processing","","","Funding Statement: This research was supported by the MSIT(Ministry of Science and ICT), Korea, under the Grand Information Technology Research Center Support Program (IITP-2020-2015-0-00742), Artificial Intelligence Graduate School Program (Sungkyunkwan University, 2019-0-00421), and the ICT Creative Consilience Program (IITP-2020-2051-001) supervised by the IITP. This work was also supported by NRF of Korea (2019R1C1C1008956, 2018R1A5A1059921) to J. J. Whang.","Pigeau, Antoine, Life Gallery: event detection in a personal media collection: An application, a basic algorithm and a benchmark, Multimedia Tools and Applications, 76, 7, pp. 9713-9734, (2017); Guo, Ao, A smartphone-based system for personal data management and personality analysis, pp. 2114-2122, (2015); Khan, Inayat, Smartphone-Based Lifelogging: An Investigation of Data Volume Generation Strength of Smartphone Sensors, Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST, 295 LNICST, pp. 63-73, (2019); Lonn, Stefan, Smartphone picture organization: A hierarchical approach, Computer Vision and Image Understanding, 187, (2019); Gemmell, Jim, MyLifeBits: A personal database for everything, Communications of the ACM, 49, 1, pp. 88-95, (2006); Doherty, Aiden Roger, Automatically segmenting lifelog data into events, pp. 20-23, (2008); Gurrin, Cathal, LifeLogging: Personal big data, Foundations and Trends in Information Retrieval, 8, 1, pp. 1-125, (2014); Vonikakis, Vassilios, A Probabilistic Approach to People-Centric Photo Selection and Sequencing, IEEE Transactions on Multimedia, 19, 11, pp. 2609-2624, (2017); Chen, Li, User evaluations on sentiment-based recommendation explanations, ACM Transactions on Interactive Intelligent Systems, 9, 4, (2019); Mu, Ruihui, A review of deep learning research, KSII Transactions on Internet and Information Systems, 13, 4, pp. 1738-1764, (2019)","","Tech Science Press","","","","","","15462218; 15462226","","","","English","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85102563494"
"Leblanc, E.; Washington, P.; Varma, M.; Dunlap, K.; Penev, Y.; Kline, A.; Wall, D.P.","Leblanc, Emilie (57218776057); Washington, Peter Yigitcan (57191498961); Varma, Maya (57194055814); Dunlap, Kaitlyn L. (57208624014); Penev, Yordan (57218775787); Kline, Aaron (57191505004); Wall, Dennis Paul (7202196193)","57218776057; 57191498961; 57194055814; 57208624014; 57218775787; 57191505004; 7202196193","Feature replacement methods enable reliable home video analysis for machine learning detection of autism","2020","Scientific Reports","10","1","21245","","","0","28","10.1038/s41598-020-76874-w","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097058730&doi=10.1038%2Fs41598-020-76874-w&partnerID=40&md5=943d4d89d4c0a0b766be71bf3c2cad1c","Department of Pediatrics, Stanford University, Stanford, United States; Stanford University, Stanford, United States; Stanford Engineering, Stanford, United States; Department of Biomedical Data Science, Stanford University, Stanford, United States; Department of Pediatrics and Psychiatry (by courtesy), Stanford University, Stanford, United States","Leblanc, Emilie, Department of Pediatrics, Stanford University, Stanford, United States; Washington, Peter Yigitcan, Stanford University, Stanford, United States; Varma, Maya, Stanford Engineering, Stanford, United States; Dunlap, Kaitlyn L., Department of Pediatrics, Stanford University, Stanford, United States, Department of Biomedical Data Science, Stanford University, Stanford, United States; Penev, Yordan, Department of Pediatrics, Stanford University, Stanford, United States, Department of Biomedical Data Science, Stanford University, Stanford, United States; Kline, Aaron, Department of Pediatrics, Stanford University, Stanford, United States, Department of Biomedical Data Science, Stanford University, Stanford, United States; Wall, Dennis Paul, Department of Pediatrics, Stanford University, Stanford, United States, Department of Biomedical Data Science, Stanford University, Stanford, United States, Department of Pediatrics and Psychiatry (by courtesy), Stanford University, Stanford, United States","Autism Spectrum Disorder is a neuropsychiatric condition affecting 53 million children worldwide and for which early diagnosis is critical to the outcome of behavior therapies. Machine learning applied to features manually extracted from readily accessible videos (e.g., from smartphones) has the potential to scale this diagnostic process. However, nearly unavoidable variability in video quality can lead to missing features that degrade algorithm performance. To manage this uncertainty, we evaluated the impact of missing values and feature imputation methods on two previously published autism detection classifiers, trained on standard-of-care instrument scoresheets and tested on ratings of 140 children videos from YouTube. We compare the baseline method of listwise deletion to classic univariate and multivariate techniques. We also introduce a feature replacement method that, based on a score, selects a feature from an expanded dataset to fill-in the missing value. The replacement feature selected can be identical for all records (general) or automatically adjusted to the record considered (dynamic). Our results show that general and dynamic feature replacement methods achieve a higher performance than classic univariate and multivariate methods, supporting the hypothesis that algorithmic management can maintain the fidelity of video-based diagnostics in the face of missing values and variable video quality. © 2020 Elsevier B.V., All rights reserved.","","algorithm; autism; early diagnosis; female; human; machine learning; male; multivariate analysis; Algorithms; Autism Spectrum Disorder; Autistic Disorder; Early Diagnosis; Female; Humans; Machine Learning; Male; Multivariate Analysis","","","We thank all raters that viewed and scored the 140 YouTube videos and enabled this study. The work was supported in part by funds to DPW from the National Institutes of Health (1R01EB025025-01, 1R21HD091500-01, 1R01LM013083), the National Science Foundation (Award 2014232), The Hartwell Foundation, Bill and Melinda Gates Foundation, Coulter Foundation, Lucile Packard Foundation, the Weston Havens Foundation, and program grants from Stanford’s Human Centered Artificial Intelligence Program, Precision Health and Integrated Diagnostics Center (PHIND), Beckman Center, Bio-X Center, Predictives and Diagnostics Accelerator (SPADA) Spectrum, Spark Program in Translational Research, and from the Wu Tsai Neurosciences Institute’s Neuroscience:Translate Program. We also acknowledge generous support from David Orr, Imma Calvo, Bobby Dekesyer and Peter Sullivan. P.W. would like to acknowledge support from the Stanford Interdisciplinary Graduate Fellowship (SIGF).","Baio, Jon, Prevalence of autism spectrum disorder among children aged 8 Years - Autism and developmental disabilities monitoring network, 11 Sites, United States, 2014, MMWR Surveillance Summaries, 67, 6, pp. 1-23, (2018); Baxter, Amanda J., The epidemiology and global burden of autism spectrum disorders, Psychological Medicine, 45, 3, pp. 601-613, (2015); Olusanya, Bolajoko O., Developmental disabilities among children younger than 5 years in 195 countries and territories, 1990–2016: a systematic analysis for the Global Burden of Disease Study 2016, The Lancet Global Health, 6, 10, pp. e1100-e1121, (2018); Boyle, Coleen A., Trends in the prevalence of developmental disabilities in US children, 1997-2008, Pediatrics, 127, 6, pp. 1034-1042, (2011); Kogan, Michael D., The prevalence of parent-reported autism spectrum disorder among US children, Pediatrics, 142, 6, (2018); Xu, Guifeng, Prevalence and Treatment Patterns of Autism Spectrum Disorder in the United States, 2016, JAMA Pediatrics, 173, 2, pp. 153-159, (2019); Encyclopedia of Autism Spectrum Disorders, (2013); Duda, Marlena, Use of machine learning for behavioral distinction of autism and ADHD, Translational Psychiatry, 6, 2, (2016); Duda, Marlena, Crowdsourced validation of a machine-learning classification system for autism and ADHD, Translational Psychiatry, 7, 5, (2017); Washington, Peter Yigitcan, Feature selection and dimension reduction of social autism data, Pacific Symposium on Biocomputing, 25, 2020, pp. 707-718, (2020)","","Nature Research","","","","","","20452322","","","33277527","English","Article","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85097058730"
"Wingfield, B.; Miller, S.; Pratheepan, P.; Kerr, D.; Gardiner, B.; Seneviratne, S.; Samarasinghe, P.; Coleman, S.","Wingfield, Benjamin (57192667090); Miller, Shane (59859753000); Pratheepan, Yogarajah (36844934000); Kerr, Dermot (24376886200); Gardiner, Bryan (24477235800); Seneviratne, Sudarshi (55224193300); Samarasinghe, Pradeepa D. (57212511361); Coleman, Sonya A. (7201402808)","57192667090; 59859753000; 36844934000; 24376886200; 24477235800; 55224193300; 57212511361; 7201402808","A predictive model for paediatric autism screening","2020","Health Informatics Journal","26","4","","2538","2553","0","35","10.1177/1460458219887823","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082186993&doi=10.1177%2F1460458219887823&partnerID=40&md5=12e6a7ed62e91b45608d151d3dbcd268","Ulster University, Coleraine, United Kingdom; University of Colombo, Colombo, Sri Lanka; Sri Lanka Institute of Information Technology, Colombo, Sri Lanka","Wingfield, Benjamin, Ulster University, Coleraine, United Kingdom; Miller, Shane, Ulster University, Coleraine, United Kingdom; Pratheepan, Yogarajah, Ulster University, Coleraine, United Kingdom; Kerr, Dermot, Ulster University, Coleraine, United Kingdom; Gardiner, Bryan, Ulster University, Coleraine, United Kingdom; Seneviratne, Sudarshi, University of Colombo, Colombo, Sri Lanka; Samarasinghe, Pradeepa D., Sri Lanka Institute of Information Technology, Colombo, Sri Lanka; Coleman, Sonya A., Ulster University, Coleraine, United Kingdom","Autism spectrum disorder is an umbrella term for a group of neurodevelopmental disorders that is associated with impairments to social interaction, communication, and behaviour. Typically, autism spectrum disorder is first detected with a screening tool (e.g. modified checklist for autism in toddlers). However, the interpretation of autism spectrum disorder behavioural symptoms varies across cultures: the sensitivity of modified checklist for autism in toddlers is as low as 25 per cent in Sri Lanka. A culturally sensitive screening tool called pictorial autism assessment schedule has overcome this problem. Low- and middle-income countries have a shortage of mental health specialists, which is a key barrier for obtaining an early autism spectrum disorder diagnosis. Early identification of autism spectrum disorder enables intervention before atypical patterns of behaviour and brain function become established. This article proposes a culturally sensitive autism spectrum disorder screening mobile application. The proposed application embeds an intelligent machine learning model and uses a clinically validated symptom checklist to monitor and detect autism spectrum disorder in low- and middle-income countries for the first time. Machine learning models were trained on clinical pictorial autism assessment schedule data and their predictive performance was evaluated, which demonstrated that the random forest was the optimal classifier (area under the receiver operating characteristic (0.98)) for embedding into the mobile screening tool. In addition, feature selection demonstrated that many pictorial autism assessment schedule questions are redundant and can be removed to optimise the screening process. © 2020 Elsevier B.V., All rights reserved.","autism spectrum disorder; decision support system; machine learning","autism; child; early diagnosis; human; mass screening; Sri Lanka; Autism Spectrum Disorder; Autistic Disorder; Child; Early Diagnosis; Humans; Mass Screening","","","Funding text 1: The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: The research outlined here was supported by Department of Economy under the Global Challenge Research Fund grants. The funding sources had no role in the design, analysis, or interpretation of data or in the preparation of the report or decision to publish.; Funding text 2: The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: The research outlined here was supported by Department of Economy under the Global Challenge Research Fund grants. The funding sources had no role in the design, analysis, or interpretation of data or in the preparation of the report or decision to publish.","Taylor, Brent W., Prevalence and incidence rates of autism in the UK: Time trend from 2004-2010 in children aged 8 years, BMJ Open, 3, 10, (2013); Elsabbagh, M., Global Prevalence of Autism and Other Pervasive Developmental Disorders, Autism Research, 5, 3, pp. 160-179, (2012); Elsabbagh, M., Perspectives from the Common Ground, Autism Research, 5, 3, pp. 153-155, (2012); Zwaigenbaum, Lonnie, Early identification of autism spectrum disorder: Recommendations for practice and research, Pediatrics, 136, pp. S10-S40, (2015); Rahman, Atif, Effectiveness of the parent-mediated intervention for children with autism spectrum disorder in south Asia in India and Pakistan (PASS): A randomised controlled trial, The Lancet Psychiatry, 3, 2, pp. 128-136, (2016); Robins, Diana L., Validation of the modified checklist for autism in toddlers, revised with follow-up (M-CHAT-R/F), Pediatrics, 133, 1, pp. 37-45, (2014); Perera, Hemamali J.M., Screening of 18-24-month-old children for autism in a semi-urban community in Sri Lanka, Journal of Tropical Pediatrics, 55, 6, pp. 402-405, (2009); World Journal of Clinical Pediatrics, (2017); Hyde, Kayleigh K., Applications of Supervised Machine Learning in Autism Spectrum Disorder Research: a Review, Review Journal of Autism and Developmental Disorders, 6, 2, pp. 128-146, (2019); Lord, Catherine E., Autism Diagnostic Interview-Revised: A revised version of a diagnostic interview for caregivers of individuals with possible pervasive developmental disorders, Journal of Autism and Developmental Disorders, 24, 5, pp. 659-685, (1994)","","SAGE Publications Ltd","","","","","","14604582; 17412811","","HIJEA","32191164","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85082186993"
"Dzieżyc, M.; Gjoreski, M.; Kazienko, P.; Saganowski, S.; Gams, M.","Dzieżyc, Maciej (57219266034); Gjoreski, Martin (56470741800); Kazienko, Przemysław (35615668400); Saganowski, Stanisław (50262928000); Gams, Matjaž Ž. (35617835400)","57219266034; 56470741800; 35615668400; 50262928000; 35617835400","Can we ditch feature engineering? End-to-end deep learning for affect recognition from physiological sensor data","2020","Sensors","20","22","6535","1","21","0","52","10.3390/s20226535","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096110881&doi=10.3390%2Fs20226535&partnerID=40&md5=c46791ec15dc565bfdce3337b7f0e926","Department of Computational Intelligence, Politechnika Wrocławska, Wroclaw, Poland; Faculty of Computer Science and Management, Politechnika Wrocławska, Wroclaw, Poland; Institut ""Jožef Stefan"", Ljubljana, Slovenia; Institut ""Jožef Stefan"", Ljubljana, Slovenia","Dzieżyc, Maciej, Department of Computational Intelligence, Politechnika Wrocławska, Wroclaw, Poland, Faculty of Computer Science and Management, Politechnika Wrocławska, Wroclaw, Poland; Gjoreski, Martin, Institut ""Jožef Stefan"", Ljubljana, Slovenia, Institut ""Jožef Stefan"", Ljubljana, Slovenia; Kazienko, Przemysław, Department of Computational Intelligence, Politechnika Wrocławska, Wroclaw, Poland, Faculty of Computer Science and Management, Politechnika Wrocławska, Wroclaw, Poland; Saganowski, Stanisław, Department of Computational Intelligence, Politechnika Wrocławska, Wroclaw, Poland, Faculty of Computer Science and Management, Politechnika Wrocławska, Wroclaw, Poland; Gams, Matjaž Ž., Institut ""Jožef Stefan"", Ljubljana, Slovenia, Institut ""Jožef Stefan"", Ljubljana, Slovenia","To further extend the applicability of wearable sensors in various domains such as mobile health systems and the automotive industry, new methods for accurately extracting subtle physiological information from these wearable sensors are required. However, the extraction of valuable information from physiological signals is still challenging—smartphones can count steps and compute heart rate, but they cannot recognize emotions and related affective states. This study analyzes the possibility of using end-to-end multimodal deep learning (DL) methods for affect recognition. Ten end-to-end DL architectures are compared on four different datasets with diverse raw physiological signals used for affect recognition, including emotional and stress states. The DL architectures specialized for time-series classification were enhanced to simultaneously facilitate learning from multiple sensors, each having their own sampling frequency. To enable fair comparison among the different DL architectures, Bayesian optimization was used for hyperparameter tuning. The experimental results showed that the performance of the models depends on the intensity of the physiological response induced by the affective stimuli, i.e., the DL models recognize stress induced by the Trier Social Stress Test more successfully than they recognize emotional changes induced by watching affective content, e.g., funny videos. Additionally, the results showed that the CNN-based architectures might be more suitable than LSTM-based architectures for affect recognition from physiological sensors. © 2020 Elsevier B.V., All rights reserved.","Affect recognition; Deep learning; Emotion recognition; End-to-end machine learning; Multimodal deep learning; Personal sensors; Physiological signals; Stress detection; Wearables","Architecture; Biomedical signal processing; Deep learning; Long short-term memory; Mobile telecommunication systems; Physiological models; Physiology; Bayesian optimization; CNN-based architecture; Mobile health systems; Physiological informations; Physiological response; Physiological sensors; Physiological signals; Time series classifications; Wearable sensors; affect; automated pattern recognition; Bayes theorem; emotion; heart rate; human; physiologic monitoring; Affect; Bayes Theorem; Deep Learning; Emotions; Heart Rate; Humans; Monitoring, Physiologic; Pattern Recognition, Automated","","","Funding: This work was partially supported by the following: the statutory funds of the Department of Computational Intelligence, Wroclaw University of Science and Technology; European Union’s Horizon 2020 research and innovation program under the Marie Skłodowska-Curie grant agreement No. 691152 (RENOIR); the Polish Ministry of Science and Higher Education fund for supporting internationally co-financed projects in 2016–2019 no. 3628/H2020/2016/2; the Polish Ministry of Education and Science—the CLARIN-PL Project; Slovenian Research Agency under grant U2-AG-16/0672–0287.","Psychology, (2004); Kreibig, Sylvia D., Autonomic nervous system activity in emotion: A review, Biological Psychology, 84, 3, pp. 394-421, (2010); Consumer Wearables and Affective Computing for Wellbeing Support, (2020); Nalepa, Grzegorz J., Analysis and use of the emotional context with wearable devices for games and intelligent assistants, Sensors, 19, 11, (2019); Kanjo, Eiman, Deep learning analysis of mobile physiological, environmental and location sensor data for emotion detection, Information Fusion, 49, pp. 46-56, (2019); Gjoreski, Martin, Classical and deep learning methods for recognizing human activities and modes of transportation with smartphone sensors, Information Fusion, 62, pp. 47-62, (2020); Gjoreski, Martin, Datasets for cognitive load inference using wearable sensors and psychological traits, Applied Sciences (Switzerland), 10, 11, (2020); Saganowski, Stanisław, Emotion Recognition Using Wearables: A Systematic Literature Review-Work-in-progress, (2020); Advances in Neural Information Processing Systems, (2012); Szegedy, Christian, Inception-v4, inception-ResNet and the impact of residual connections on learning, pp. 4278-4284, (2017)","","MDPI AG","","","","","","14248220","","","33207564","English","Article","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85096110881"
"Kolakowska, A.; Szwoch, W.; Szwoch, M.","Kolakowska, Agata (9250282300); Szwoch, Wioleta (22433908200); Szwoch, Mariusz (22433906700)","9250282300; 22433908200; 22433906700","A review of emotion recognition methods based on data acquired via smartphone sensors","2020","Sensors","20","21","6367","1","43","0","50","10.3390/s20216367","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096030937&doi=10.3390%2Fs20216367&partnerID=40&md5=212be60596dbdab9dce8d390a64b1bc9","Telecommunications and Informatics, Gdańsk University of Technology, Gdansk, Poland","Kolakowska, Agata, Telecommunications and Informatics, Gdańsk University of Technology, Gdansk, Poland; Szwoch, Wioleta, Telecommunications and Informatics, Gdańsk University of Technology, Gdansk, Poland; Szwoch, Mariusz, Telecommunications and Informatics, Gdańsk University of Technology, Gdansk, Poland","In recent years, emotion recognition algorithms have achieved high efficiency, allowing the development of various affective and affect-aware applications. This advancement has taken place mainly in the environment of personal computers offering the appropriate hardware and sufficient power to process complex data from video, audio, and other channels. However, the increase in computing and communication capabilities of smartphones, the variety of their built-in sensors, as well as the availability of cloud computing services have made them an environment in which the task of recognising emotions can be performed at least as effectively. This is possible and particularly important due to the fact that smartphones and other mobile devices have become the main computer devices used by most people. This article provides a systematic overview of publications from the last 10 years related to emotion recognition methods using smartphone sensors. The characteristics of the most important sensors in this respect are presented, and the methods applied to extract informative features on the basis of data read from these input channels. Then, various machine learning approaches implemented to recognise emotional states are described. © 2020 Elsevier B.V., All rights reserved.","Affective computing; Emotion recognition; Human–computer interaction; Sensors; Sensory data; Smartphones","Smartphones; Speech recognition; Affect aware applications; Built-in sensors; Cloud computing services; Communication capabilities; Emotion recognition; Emotional state; High-efficiency; Machine learning approaches; Personal computers; algorithm; Bayes theorem; emotion; human; machine learning; smartphone; Algorithms; Bayes Theorem; Emotions; Humans; Machine Learning; Smartphone","","","Funding: This research was supported by DS Funds of the ETI Faculty, Gdansk University of Technology.","Pakistan Journal of Science, (2014); Khan, Wazir Zada, Mobile phone sensing systems: A survey, IEEE Communications Surveys and Tutorials, 15, 1, pp. 402-427, (2013); 6th International Workshop on Ubiquitous Health and Wellness Ubihealth 2012, (2012); Rana, Rajib Kumar, Opportunistic and Context-Aware Affect Sensing on Smartphones, IEEE Pervasive Computing, 15, 2, pp. 60-69, (2016); Politou, Eugenia A., A survey on mobile affective computing, Computer Science Review, 25, pp. 79-100, (2017); Szwoch, Mariusz, Evaluation of affective intervention process in development of affect-Aware educational video games, pp. 1675-1679, (2016); Image Processing and Communications Challenges, (2019); Landowska, Agnieszka, Methodology of affective intervention design for intelligent systems, Interacting with Computers, 28, 6, pp. 737-759, (2016); Kolakowska, Agata, Emotion recognition and its application in software engineering, pp. 532-539, (2013); Kolakowska, Agata, Towards detecting programmers' stress on the basis of keystroke dynamics, pp. 1621-1626, (2016)","","MDPI AG","","","","","","14248220","","","33171646","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85096030937"
"Narain, J.; Johnson, K.T.; Ferguson, C.; O’Brien, A.; Talkar, T.; Zhang, Y.Z.; Wofford, P.; Quatieri, T.; Picard, R.; Maes, P.","Narain, Jaya (57038499000); Johnson, Kristina T. (57195218312); Ferguson, Craig (57201498054); O’Brien, Amanda M. (57190948483); Talkar, Tanya (57218455198); Zhang, Yue (56413223700); Wofford, Peter (57208618496); Quatieri, Thomas F. (7005856167); Picard, Rosalind W. (7005583409); Maes, Pattie (56268463300)","57038499000; 57195218312; 57201498054; 57190948483; 57218455198; 56413223700; 57208618496; 7005856167; 7005583409; 56268463300","Personalized Modeling of Real-World Vocalizations from Nonverbal Individuals","2020","","","","","665","669","0","10","10.1145/3382507.3418854","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096656269&doi=10.1145%2F3382507.3418854&partnerID=40&md5=e0f79b953bc0dbfd789ebaa0bff08fe3","Massachusetts Institute of Technology, Cambridge, United States; Lincoln Laboratory, Lexington, United States","Narain, Jaya, Massachusetts Institute of Technology, Cambridge, United States; Johnson, Kristina T., Massachusetts Institute of Technology, Cambridge, United States; Ferguson, Craig, Massachusetts Institute of Technology, Cambridge, United States; O’Brien, Amanda M., Massachusetts Institute of Technology, Cambridge, United States; Talkar, Tanya, Lincoln Laboratory, Lexington, United States; Zhang, Yue, Massachusetts Institute of Technology, Cambridge, United States; Wofford, Peter, Massachusetts Institute of Technology, Cambridge, United States; Quatieri, Thomas F., Lincoln Laboratory, Lexington, United States; Picard, Rosalind W., Massachusetts Institute of Technology, Cambridge, United States; Maes, Pattie, Massachusetts Institute of Technology, Cambridge, United States","Nonverbal vocalizations contain important affective and communicative information, especially for those who do not use traditional speech, including individuals who have autism and are non- or minimally verbal (nv/mv). Although these vocalizations are often understood by those who know them well, they can be challenging to understand for the community-at-large. This work presents (1) a methodology for collecting spontaneous vocalizations from nv/mv individuals in natural environments, with no researcher present, and personalized in-the-moment labels from a family member; (2) speaker-dependent classification of these real-world sounds for three nv/mv individuals; and (3) an interactive application to translate the nonverbal vocalizations in real time. Using support-vector machine and random forest models, we achieved speaker-dependent unweighted average recalls (UARs) of 0.75, 0.53, and 0.79 for the three individuals, respectively, with each model discriminating between 5 nonverbal vocalization classes. We also present first results for real-time binary classification of positive- and negative-affect nonverbal vocalizations, trained using a commercial wearable microphone and tested in real time using a smartphone. This work informs personalized machine learning methods for non-traditional communicators and advances real-world interactive augmentative technology for an underserved population. © 2020 Elsevier B.V., All rights reserved.","affect detection; autism; human-computer interaction; machine learning; nonverbal communication; paralinguistics","Decision trees; Interactive computer systems; Support vector machines; Binary classification; Interactive applications; Machine learning methods; Natural environments; Negative affects; Nonverbal vocalizations; Personalized model; Speaker dependents; Learning systems","","","","Abdelwahab, Mohammed, Supervised domain adaptation for emotion recognition from speech, Proceedings - ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing, 2015-August, pp. 5058-5062, (2015); Q J Exp Psychol, (2017); Anikin, Andrey, Nonlinguistic vocalizations from online amateur videos for emotion research: A validated corpus, Behavior Research Methods, 49, 2, pp. 758-771, (2017); Bacon, Elizabeth C., Naturalistic language sampling to characterize the language abilities of 3-year-olds with autism spectrum disorder, Autism, 23, 3, pp. 699-712, (2019); Augmentative and Alternative Communication Management of Severe Communication Disorders in Children and Adults, (1998); Brady, Nancy C., Development of the Communication Complexity Scale, American Journal of Speech-Language Pathology, 21, 1, pp. 16-28, (2012); Data Mining Practical Machine Learning Tools and Techniques, (2016); Eyben, Florian, The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research and Affective Computing, IEEE Transactions on Affective Computing, 7, 2, pp. 190-202, (2016); Eyben, Florian, Recent developments in openSMILE, the munich open-source multimedia feature extractor, pp. 835-838, (2013); International Journal of Health Professions, (2015)","","Association for Computing Machinery, Inc","ACM SIGCHI","22nd ACM International Conference on Multimodal Interaction, ICMI 2020","","Virtual, Online","164395","","9781450375818","","","English","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85096656269"
"Yadav, D.; Garg, R.K.; Chhabra, D.; Yadav, R.; Kumar, A.; Shukla, P.","Yadav, Dinesh (58584891900); Garg, Ramesh Kumar (57196542349); Chhabra, Deepak (56198514600); Yadav, Rajkumar (57208815822); Kumar, Ashwani Naveen (58736686800); Shukla, Pratyoosh (7202961636)","58584891900; 57196542349; 56198514600; 57208815822; 58736686800; 7202961636","Smart diagnostics devices through artificial intelligence and mechanobiological approaches","2020","3 Biotech","10","8","351","","","0","11","10.1007/s13205-020-02342-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088397843&doi=10.1007%2Fs13205-020-02342-x&partnerID=40&md5=5c0fc030366e00257e04b88693535cd6","Department of Mechanical Engineering, Deenbandhu Chhotu Ram University of Science and Technology, Sonipat, India; Department of Mechanical Engineering, Maharshi Dayanand University, Rohtak, India; Department of Computer Science and Engineering, Indira Gandhi University, Rewari, India; Department of Microbiology, Maharshi Dayanand University, Rohtak, India","Yadav, Dinesh, Department of Mechanical Engineering, Deenbandhu Chhotu Ram University of Science and Technology, Sonipat, India; Garg, Ramesh Kumar, Department of Mechanical Engineering, Deenbandhu Chhotu Ram University of Science and Technology, Sonipat, India; Chhabra, Deepak, Department of Mechanical Engineering, Maharshi Dayanand University, Rohtak, India; Yadav, Rajkumar, Department of Computer Science and Engineering, Indira Gandhi University, Rewari, India; Kumar, Ashwani Naveen, Department of Mechanical Engineering, Maharshi Dayanand University, Rohtak, India; Shukla, Pratyoosh, Department of Microbiology, Maharshi Dayanand University, Rohtak, India","The present work illustrates the promising intervention of smart diagnostics devices through artificial intelligence (AI) and mechanobiological approaches in health care practices. The artificial intelligence and mechanobiological approaches in diagnostics widen the scope for point of care techniques for the timely revealing of diseases by understanding the biomechanical properties of the tissue of interest. Smart diagnostic device senses the physical parameters due to change in mechanical, biological, and luidic properties of the cells and to control these changes, supply the necessary drugs immediately using AI techniques. The latest techniques like sweat diagnostics to measure the overall health, Photoplethysmography (PPG) for real-time monitoring of pulse waveform by capturing the reflected signal due to blood pulsation), Micro-electromechanical systems (MEMS) and Nano-electromechanical systems (NEMS) smart devices to detect disease at its early stage, lab-on-chip and organ-on-chip technologies, Ambulatory Circadian Monitoring device (ACM), a wrist-worn device for Parkinson’s disease have been discussed. The recent and futuristic smart diagnostics tool/techniques like emotion recognition by applying machine learning algorithms, atomic force microscopy that measures the fibrinogen and erythrocytes binding force, smartphone-based retinal image analyser system, image-based computational modeling for various neurological disorders, cardiovascular diseases, tuberculosis, predicting and preventing of Zika virus, optimal drugs and doses for HIV using AI, etc. have been reviewed. The objective of this review is to examine smart diagnostics devices based on artificial intelligence and mechanobiological approaches, with their medical applications in healthcare. This review determines that smart diagnostics devices have potential applications in healthcare, but more research work will be essential for prospective accomplishments of this technology. © 2024 Elsevier B.V., All rights reserved.","Artificial intelligence; Biological; Diagnostics; Fluidic; Mechanical; Mechanobiology","biological marker; focal adhesion kinase; algorithm; artificial intelligence; cardiovascular disease; disease exacerbation; flow rate; fluorescence imaging; fluorescence resonance energy transfer; health care; human; image analysis; machine learning; mechanosensing; mechanotransduction; microscopy; neuroimaging; neurologic disease; photoelectric plethysmography; pulse wave; Review; tuberculosis; Zika virus","","","Funding text 1: PS acknowledges Department of Science and Technology, New Delhi, Govt. of India, FIST grant (Grant No. 1196 SR/FST/LS-I/2017/4).; Funding text 2: PS acknowledges Department of Science and Technology, New Delhi, Govt. of India, FIST grant (Grant No. 1196 SR/FST/LS-I/2017/4).","Agnifili, Luca, Circadian intraocular pressure patterns in healthy subjects, primary open angle and normal tension glaucoma patients with a contact lens sensor, Acta Ophthalmologica, 93, 1, pp. e14-e21, (2015); Ahmed, Asif, Biosensors for whole-cell bacterial detection, Clinical Microbiology Reviews, 27, 3, pp. 631-646, (2014); Alcantara, Marlon Fernandes, Improving tuberculosis diagnostics using deep learning and mobile health technologies among resource-poor communities in Perú, Smart Health, 1-2, pp. 66-76, (2017); Aoe, Jo, Automatic diagnosis of neurological diseases using MEG signals with a deep neural network, Scientific Reports, 9, 1, (2019); Bao, Fangjun, Consideration of corneal biomechanics in the diagnosis and management of keratoconus: is it important?, Eye and Vision, 3, 1, (2016); Bauer, Maria, Fabrication of a lab-on-chip device using material extrusion (3D printing) and demonstration via malaria-Ab ELISA, Micromachines, 9, 1, (2018); Blanchoin, Laurent, Actin dynamics, architecture, and mechanics in cell motility, Physiological Reviews, 94, 1, pp. 235-263, (2014); Cai, Xiaojun, A layer-by-layer assembled and carbon nanotubes/gold nanoparticles-based bienzyme biosensor for cholesterol detection, Sensors and Actuators B: Chemical, 181, pp. 575-583, (2013); Chakradhar, Shraddha, Predictable response: Finding optimal drugs and doses using artificial intelligence, Nature Medicine, 23, 11, pp. 1244-1247, (2017); Biophys J, (2017)","","Springer Science and Business Media Deutschland GmbH","","","","","","2190572X; 21905738","","","","English","Review","Final","All Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85088397843"
"Kalantarian, H.; Jedoui, K.; Dunlap, K.; Schwartz, J.; Washington, P.; Husic, A.; Tariq, Q.; Ning, M.; Kline, A.; Wall, D.P.","Kalantarian, Haik (55820889400); Jedoui, Khaled (57210217052); Dunlap, Kaitlyn L. (57208624014); Schwartz, Jessey Nicole (57196152141); Washington, Peter Yigitcan (57191498961); Husic, Arman (57220907587); Tariq, Qandeel (57204803280); Ning, Michael (57209214321); Kline, Aaron (57191505004); Wall, Dennis Paul (7202196193)","55820889400; 57210217052; 57208624014; 57196152141; 57191498961; 57220907587; 57204803280; 57209214321; 57191505004; 7202196193","The performance of emotion classifiers for children with parent-reported autism: Quantitative feasibility study","2020","JMIR Mental Health","7","4","e13174","","","0","34","10.2196/13174","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090258329&doi=10.2196%2F13174&partnerID=40&md5=c28323bbe7826748e931fde0942c6757","Department of Pediatrics, Stanford University, Stanford, United States; Department of Biomedical Data Science, Stanford University, Stanford, United States; Department of Mathematics, Stanford University, Stanford, United States; Department of Psychiatry and Behavioral Sciences, Stanford University, Stanford, United States","Kalantarian, Haik, Department of Pediatrics, Stanford University, Stanford, United States, Department of Biomedical Data Science, Stanford University, Stanford, United States; Jedoui, Khaled, Department of Mathematics, Stanford University, Stanford, United States; Dunlap, Kaitlyn L., Department of Pediatrics, Stanford University, Stanford, United States, Department of Biomedical Data Science, Stanford University, Stanford, United States; Schwartz, Jessey Nicole, Department of Pediatrics, Stanford University, Stanford, United States, Department of Biomedical Data Science, Stanford University, Stanford, United States; Washington, Peter Yigitcan, Department of Pediatrics, Stanford University, Stanford, United States, Department of Biomedical Data Science, Stanford University, Stanford, United States; Husic, Arman, Department of Pediatrics, Stanford University, Stanford, United States, Department of Biomedical Data Science, Stanford University, Stanford, United States; Tariq, Qandeel, Department of Pediatrics, Stanford University, Stanford, United States, Department of Biomedical Data Science, Stanford University, Stanford, United States; Ning, Michael, Department of Pediatrics, Stanford University, Stanford, United States, Department of Biomedical Data Science, Stanford University, Stanford, United States; Kline, Aaron, Department of Pediatrics, Stanford University, Stanford, United States, Department of Biomedical Data Science, Stanford University, Stanford, United States; Wall, Dennis Paul, Department of Pediatrics, Stanford University, Stanford, United States, Department of Biomedical Data Science, Stanford University, Stanford, United States, Department of Psychiatry and Behavioral Sciences, Stanford University, Stanford, United States","Background: Autism spectrum disorder (ASD) is a developmental disorder characterized by deficits in social communication and interaction, and restricted and repetitive behaviors and interests. The incidence of ASD has increased in recent years; it is now estimated that approximately 1 in 40 children in the United States are affected. Due in part to increasing prevalence, access to treatment has become constrained. Hope lies in mobile solutions that provide therapy through artificial intelligence (AI) approaches, including facial and emotion detection AI models developed by mainstream cloud providers, available directly to consumers. However, these solutions may not be sufficiently trained for use in pediatric populations. Objective: Emotion classifiers available off-the-shelf to the general public through Microsoft, Amazon, Google, and Sighthound are well-suited to the pediatric population, and could be used for developing mobile therapies targeting aspects of social communication and interaction, perhaps accelerating innovation in this space. This study aimed to test these classifiers directly with image data from children with parent-reported ASD recruited through crowdsourcing. Methods: We used a mobile game called Guess What? that challenges a child to act out a series of prompts displayed on the screen of the smartphone held on the forehead of his or her care provider. The game is intended to be a fun and engaging way for the child and parent to interact socially, for example, the parent attempting to guess what emotion the child is acting out (eg, surprised, scared, or disgusted). During a 90-second game session, as many as 50 prompts are shown while the child acts, and the video records the actions and expressions of the child. Due in part to the fun nature of the game, it is a viable way to remotely engage pediatric populations, including the autism population through crowdsourcing. We recruited 21 children with ASD to play the game and gathered 2602 emotive frames following their game sessions. These data were used to evaluate the accuracy and performance of four state-of-the-art facial emotion classifiers to develop an understanding of the feasibility of these platforms for pediatric research. Results: All classifiers performed poorly for every evaluated emotion except happy. None of the classifiers correctly labeled over 60.18% (1566/2602) of the evaluated frames. Moreover, none of the classifiers correctly identified more than 11% (6/51) of the angry frames and 14% (10/69) of the disgust frames. Conclusions: The findings suggest that commercial emotion classifiers may be insufficiently trained for use in digital approaches to autism treatment and treatment tracking. Secure, privacy-preserving methods to increase labeled training data are needed to boost the models' performance before they can be used in AI-enabled approaches to social therapy of the kind that is common in autism treatments. © 2021 Elsevier B.V., All rights reserved.","Affect; Artificial intelligence; Autism; Digital data; Digital health; Emotion; Machine learning; Mhealth; Mobile app; Mobile phone","Article; artificial intelligence; Asperger syndrome; autism; child; childhood disintegrative disorder; classifier; clinical article; crowdsourcing; data processing; disgust; emotion; feasibility study; happiness; human; incidence; interrater reliability; measurement accuracy; mobile application; parent; pervasive developmental disorder not otherwise specified; prevalence; privacy; psychosocial care; quantitative study; sadness; social interaction; United States; video game; videorecording","","","","Diagnostic and Statistical Manual of Mental Disorders, (2000); Lozier, Leah M., Impairments in facial affect recognition associated with autism spectrum disorders: A meta-analysis, Development and Psychopathology, 26, 4, pp. 933-945, (2014); Loth, Eva, Facial expression recognition as a candidate marker for autism spectrum disorder: How frequent and severe are deficits?, Molecular Autism, 9, 1, (2018); Fridenson-Hayo, Shimrit, Basic and complex emotion recognition in children with autism: Cross-cultural findings, Molecular Autism, 7, 1, (2016); Kogan, Michael D., The prevalence of parent-reported autism spectrum disorder among US children, Pediatrics, 142, 6, (2018); Rogers, Sally J., Brief report: Early intervention in autism, Journal of Autism and Developmental Disorders, 26, 2, pp. 243-246, (1996); Applied Behavior Analysis, (1987); Dawson, Geraldine, Randomized, controlled trial of an intervention for toddlers with autism: The early start Denver model, Pediatrics, 125, 1, pp. e17-e23, (2010); Dawson, Geraldine, Early behavioral intervention, brain plasticity, and the prevention of autism spectrum disorder, Development and Psychopathology, 20, 3, pp. 775-803, (2008); Dawson, Geraldine, Early behavioral intervention is associated with normalized brain activity in young children with autism, Journal of the American Academy of Child and Adolescent Psychiatry, 51, 11, pp. 1150-1159, (2012)","","JMIR Publications Inc.","","","","","","23687959","","","","English","Article","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85090258329"
"Wang, D.; Yang, X.; Liu, X.; Jing, J.; Fang, S.","Wang, Dingliang (56434684800); Yang, Xuezhi (7406505132); Liu, Xuenan (57202917714); Jing, Jin (57216177017); Fang, Shuai (7402422537)","56434684800; 7406505132; 57202917714; 57216177017; 7402422537","Detail-preserving pulse wave extraction from facial videos using consume-level camera","2020","Biomedical Optics Express","11","4","","1876","1891","0","32","10.1364/BOE.380646","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082739338&doi=10.1364%2FBOE.380646&partnerID=40&md5=a6f16c614cc48ed90f373753943d67e0","School of Computer and Information, Hefei University of Technology, Hefei, China; School of Software, Hefei University of Technology, Hefei, China; Anhui Province Key Laboratory of Industry Safety and Emergency Technology, Hefei, China","Wang, Dingliang, School of Computer and Information, Hefei University of Technology, Hefei, China; Yang, Xuezhi, School of Software, Hefei University of Technology, Hefei, China, Anhui Province Key Laboratory of Industry Safety and Emergency Technology, Hefei, China; Liu, Xuenan, School of Computer and Information, Hefei University of Technology, Hefei, China; Jing, Jin, School of Computer and Information, Hefei University of Technology, Hefei, China; Fang, Shuai, School of Computer and Information, Hefei University of Technology, Hefei, China","With the popularity of smart phones, non-contact video-based vital sign monitoring using a camera has gained increased attention over recent years. Especially, imaging photoplethysmography (IPPG), a technique for extracting pulse waves from videos, conduces to monitor physiological information on a daily basis, including heart rate, respiration rate, blood oxygen saturation, and so on. The main challenge for accurate pulse wave extraction from facial videos is that the facial color intensity change due to cardiovascular activities is subtle and is often badly disturbed by noise, such as illumination variation, facial expression changes, and head movements. Even a tiny interference could bring a big obstacle for pulse wave extraction and reduce the accuracy of the calculated vital signs. In recent years, many novel approaches have been proposed to eliminate noise such as filter banks, adaptive filters, Distance-PPG, and machine learning, but these methods mainly focus on heart rate detection and neglect the retention of useful details of pulse wave. For example, the pulse wave extracted by the filter bank method has no dicrotic wave and approaching sine wave, but dicrotic waves are essential for calculating vital signs like blood viscosity and blood pressure. Therefore, a new framework is proposed to achieve accurate pulse wave extraction that contains mainly two steps: 1) preprocessing procedure to remove baseline offset and high frequency random noise; and 2) a self-adaptive singular spectrum analysis algorithm to obtain cyclical components and remove aperiodic irregular noise. Experimental results show that the proposed method can extract detail-preserved pulse waves from facial videos under realistic situations and outperforms state-of-the-art methods in terms of detail-preserving and real time heart rate estimation. Furthermore, the pulse wave extracted by our approach enabled the non-contact estimation of atrial fibrillation, heart rate variability, blood pressure, as well as other physiological indices that require standard pulse wave. © 2020 Elsevier B.V., All rights reserved.","","Adaptive filtering; Adaptive filters; Blood; Blood pressure; Cameras; Extraction; Filter banks; Patient monitoring; Smartphones; Spectrum analysis; Viscosity; Blood oxygen saturation; Heart rate variability; Illumination variation; Imaging photoplethysmography (IPPG); Physiological indices; Physiological informations; Singular spectrum analysis; State-of-the-art methods; Heart; algorithm; Article; atrial fibrillation; blood pressure measurement; data compression; face; female; heart rate measurement; heart rate variability; human; human experiment; illumination; image processing; male; mathematical analysis; pulse wave; signal noise ratio; singular value decomposition; spectrum; spike; videorecording","","","This work is supported by 2018 Training Programme Foundation for Application of Scientific and Technological Achievements of Hefei University of Technology, and is also supported by 2019 Independent Innovation Project of Industrial Safety and Emergency Technology of Anhui Key Laboratory. We sincerely thank Professor Yang and other colleagues for providing advices for solving technological problems during project research.","Wu, Ting, Photoplethysmography imaging: A new noninvasive and non-contact method for mapping of the dermal perfusion changes, Proceedings of SPIE - The International Society for Optical Engineering, 4163, pp. 62-70, (2000); Allen, John, Photoplethysmography and its application in clinical physiological measurement, Physiological Measurement, 28, 3, pp. R1-R39, (2007); Poh, Ming Zher, Non-contact, automated cardiac pulse measurements using video imaging and blind source separation, Optics Express, 18, 10, pp. 10762-10774, (2010); Pavlidis, I. T., Interacting with human physiology, Computer Vision and Image Understanding, 108, 1-2, pp. 150-170, (2007); Liu, Weichao, Reliability analysis of an integrated device of ECG, PPG and pressure pulse wave for cardiovascular disease, Microelectronics Reliability, 87, pp. 183-187, (2018); Zhong, Wenjun, Pulse wave velocity and cognitive function in older adults, Alzheimer Disease and Associated Disorders, 28, 1, pp. 44-49, (2014); Stein, James H., Relationships between sleep apnea, cardiovascular disease risk factors, and aortic pulse wave velocity over 18 years: the Wisconsin Sleep Cohort, Sleep and Breathing, 20, 2, pp. 813-817, (2016); Lane, Helen A., Noninvasive assessment of preclinical atherosclerosis, Vascular Health and Risk Management, 2, 1, pp. 19-30, (2006); Balakrishnan, Guha, Detecting pulse from head motions in video, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 3430-3437, (2013); Siddiqui, Sarah Ali, A Pulse Rate Estimation Algorithm Using PPG and Smartphone Camera, Journal of Medical Systems, 40, 5, (2016)","","OSA - The Optical Society custserv@osa.org","","","","","","21567085","","","","English","Article","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85082739338"
"Generosi, A.; Ceccacci, S.; Faggiano, S.; Giraldi, L.; Mengoni, M.","Generosi, Andrea (57201216842); Ceccacci, Silvia (55546505300); Faggiano, Samuele (57220105724); Giraldi, Luca (57191833336); Mengoni, Maura (22634930400)","57201216842; 55546505300; 57220105724; 57191833336; 22634930400","A toolkit for the automatic analysis of human behavior in HCI applications in the wild","2020","Advances in Science, Technology and Engineering Systems","5","6","","185","192","0","16","10.25046/aj050622","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096909853&doi=10.25046%2Faj050622&partnerID=40&md5=ef802ca23ce7c08342b34a9be4b11cee","Department of Industrial Engineering and Mathematical Sciences, Università Politecnica delle Marche, Ancona, Italy; Emoj s.r.l., Ancona, Italy","Generosi, Andrea, Department of Industrial Engineering and Mathematical Sciences, Università Politecnica delle Marche, Ancona, Italy; Ceccacci, Silvia, Department of Industrial Engineering and Mathematical Sciences, Università Politecnica delle Marche, Ancona, Italy; Faggiano, Samuele, Emoj s.r.l., Ancona, Italy; Giraldi, Luca, Emoj s.r.l., Ancona, Italy; Mengoni, Maura, Department of Industrial Engineering and Mathematical Sciences, Università Politecnica delle Marche, Ancona, Italy","Nowadays, smartphones and laptops equipped with cameras have become an integral part of our daily lives. The pervasive use of cameras enables the collection of an enormous amount of data, which can be easily extracted through video images processing. This opens up the possibility of using technologies that until now had been restricted to laboratories, such as eye-tracking and emotion analysis systems, to analyze users' behavior in the wild, during the interaction with websites. In this context, this paper introduces a toolkit that takes advantage of deep learning algorithms to monitor user's behavior and emotions, through the acquisition of facial expression and eye gaze from the video captured by the webcam of the device used to navigate the web, in compliance with the EU General data protection regulation (GDPR). Collected data are potentially useful to support user experience assessment of web-based applications in the wild and to improve the effectiveness of e-commerce recommendation systems. © 2020 Elsevier B.V., All rights reserved.","Affective Computing; Convolutional Neural Networks; Deep Learning; Gaze detection; User Experience","","","","The toolkit is part of the NextPerception project that has received funding from the European Union Horizon 2020, ECSEL-2019-2-RIA Joint Undertaking (Grant Agreement Number 876487). The authors would also thank the research fellow Lorenzo Marchesini for its support in the design and development of the eye gaze detection system.","Talipu, Abudukaiyoumu, Evaluation of Deep Convolutional Neural Network architectures for Emotion Recognition in the Wild, pp. 25-27, (2019); Lallemand, Carine, User experience: A concept without consensus? Exploring practitioners' perspectives through an international survey, Computers in Human Behavior, 43, pp. 35-48, (2015); Jabbar, Jahanzeb, Real-time sentiment analysis on E-Commerce application, pp. 391-396, (2019); Fang, Xing, Sentiment analysis using product review data, Journal of Big Data, 2, 1, (2015); Hedegaard, Steffen, Extracting usability and user experience information from online user reviews, Conference on Human Factors in Computing Systems - Proceedings, pp. 2089-2098, (2013); 90 9 1 Rule for Participation Inequality in Social Media and Online Communities, (2006); Jaiswal, Saurabh, An intelligent recommendation system using gaze and emotion detection, Multimedia Tools and Applications, 78, 11, pp. 14231-14250, (2019); Portugal, Ivens Da Silva, The use of machine learning algorithms in recommender systems: A systematic review, Expert Systems with Applications, 97, pp. 205-227, (2018); Bielozorov, Artem, The Role of User Emotions for Content Personalization in e-Commerce: Literature Review, Lecture Notes in Computer Science, 11588 LNCS, pp. 177-193, (2019); Generosi, Andrea, A deep learning-based system to track and analyze customer behavior in retail store, IEEE International Conference on Consumer Electronics - Berlin, ICCE-Berlin, 2018-September, (2018)","","ASTES Publishers","","","","","","24156698","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85096909853"
"Masui, K.; Okada, G.; Tsumura, N.","Masui, Kenta (57202893222); Okada, Genki (57190068638); Tsumura, Norimichi (7005877111)","57202893222; 57190068638; 7005877111","Measurement of advertisement effect based on multimodal emotional responses considering personality","2020","ITE Transactions on Media Technology and Applications","8","1","","49","59","0","13","10.3169/mta.8.49","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078008809&doi=10.3169%2Fmta.8.49&partnerID=40&md5=b67f70a21542db0601e5bacabf24852f","Chiba University, Chiba, Japan; Graduate School of Advanced Integration Science, Chiba University, Chiba, Japan","Masui, Kenta, Chiba University, Chiba, Japan; Okada, Genki, Graduate School of Advanced Integration Science, Chiba University, Chiba, Japan; Tsumura, Norimichi, Chiba University, Chiba, Japan","The market size of online video advertising is expanding rapidly along with the spread of smartphones and social media. In this study, we estimate advertising effectiveness in the natural environment using online data collection and the remote measurement of webcam facial expressions and physiological responses. We collected 4, 108 videos of the faces of 411 Japanese people who were watching the video advertisement in their natural environment via the Internet. Facial expression and physiological responses such as heart rate and gaze were remotely measured by analyzing facial videos. We found that the accuracies of ad liking and purchase intent prediction are better when various acquired features are combined and machine learning is used than when only single-mode features are used. In addition, we aim to improve prediction accuracy by clustering the personality of the subjects and designing an estimation model for each personality. © 2020 Elsevier B.V., All rights reserved.","Emotion; Heart rate variability; Physiological signals; Remote measurement; RGB camera","","","","","Teixeira, Thales S., Emotion-induced engagement in Internet video advertisements, Journal of Marketing Research, 49, 2, pp. 144-159, (2012); Lang, Annie, Involuntary Attention and Physiological Arousal Evoked by Structural Features and Emotional Content in TV Commercials, Communication Research, 17, 3, pp. 275-299, (1990); Venkatraman, Vinod, Predicting advertising success beyond traditional measures: New insights from neurophysiological methods and market response modeling, Journal of Marketing Research, 52, 4, pp. 436-452, (2015); Pham, Phuong, Understanding emotional responses to mobile video advertisements via physiological signal sensing and facial expression analysis, pp. 67-78, (2017); Girard, Jeffrey M., Historical Heterogeneity Predicts Smiling: Evidence from Large-Scale Observational Analyses, pp. 719-726, (2017); Expression of the Emotions in Man and Animals, (1872); Tomkins, Silvan S., WHAT AND WHERE ARE THE PRIMARY AFFECTS?SOME EVIDENCE FOR A THEORY., Perceptual and Motor Skills, 18, pp. 119-158, (1964); Ekman, Paul, Constants across cultures in the face and emotion, Journal of Personality and Social Psychology, 17, 2, pp. 124-129, (1971); Ekman, Paul, Facial expression and emotion, American Psychologist, 48, 4, pp. 384-392, (1993); Well Being the Foundations of Hedonic Psychology, (1999)","","Institute of Image Information and Television Engineers mta@ite.or.jp","","","","","","21867364","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85078008809"
"Tabassum, K.","Tabassum, Kahkashan (36160501700)","36160501700","Using wireless and mobile technologies to enhance teaching and learning strategies","2020","Indonesian Journal of Electrical Engineering and Computer Science","17","3","","1555","1561","0","9","10.11591/ijeecs.v17.i3.pp1555-1561","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075483841&doi=10.11591%2Fijeecs.v17.i3.pp1555-1561&partnerID=40&md5=faade502cfda275504945309b8ee9174","Department of Computer Science, Princess Nourah Bint Abdulrahman University, Riyadh, Saudi Arabia","Tabassum, Kahkashan, Department of Computer Science, Princess Nourah Bint Abdulrahman University, Riyadh, Saudi Arabia","The tremendous development and sensational popularity of smart mobile devices that offer uninterrupted communication services could be utilized for inculcating creative skills in young children. Many tools and applications that target children education and entertainment are easily available these days. Both parents and educators find a huge responsibility in evaluating and selecting such devices and decide which of these offer potential educational benefits for their children, decision seems critical for them since they use restricted tools to evaluate the applications available in the market. Hence the development of an interactive and multisensory component based mobile application environment with integrated physical and virtual reality scenario could solve the issue related to the children education and entertainment. This is not just limited to education and the entertainment domain but will explore essential features of virtual reality environment and interactivity. Children with special education needs face difficulties in understanding new information and concept but by the use of virtual reality worlds, their level of understanding could be improved. This paper discusses the development of adaptable, customizable and virtual reality based compliant applications for children. These applications may be useful to enhance learning essentials and improve diverse sensory and cognitive impairments in children with special needs. This paper discusses development of application that could be made available on i-pads, tablets, etc and will be intelligent to perform switch between the different intellectual ranges of children accessing it. In other words it will present self-adaptability feature. This research study also focuses on the activities that are suitable for learning purposes with students with impairments. © 2019 Elsevier B.V., All rights reserved.","Artificial intelligence; Autism; Children literature; Communication skills; Intelligent application; Learning skills; Reading skills","","","","Funding text 1: This research is funded by Princess Nourah Bint Abdulrahman University (PNU), Children’s Literature Research Centre (CLRC) and Saudi British Centre (SBC) under the Grant No. CLS2018-08. I am thankful to the Research Unit for their constant support and encouragement to improve the research skills of the researchers at Princess Nourah University.; Funding text 2: This research is funded by Princess Nourah Bint Abdulrahman University (PNU), Children's Literature Research Centre (CLRC) and Saudi British Centre (SBC) under the Grant No. CLS2018-08. I am thankful to the Research Unit for their constant support and encouragement to improve the research skills of the researchers at Princess Nourah University","Young Children 0 8 and Digital Technology A Qualitative Exploratory Study Across Seven Countries, (2015); Programming and Software Technology Research Group Technical Report, (2008); Object Oriented Software Composition, (1995); Contemporary Issues in Technology and Teacher Education, (2009); A Curriculum Guide for Teaching Young Mentally Handicapped Children, (1996); Hannafin, Michael J., Emerging technologies, ISD, and learning environments: Critical perspectives, Educational Technology Research and Development, 40, 1, pp. 49-63, (1992); Costabile, M. Francesca, Software environments for end-user development and tailoring, PsychNology Journal, 2, 1, pp. 99-122, (2004); Hooper, Simon R., Psychological Perspectives on Emerging Instructional Technologies: A Critical Analysis, Educational Psychologist, 26, 1, pp. 69-95, (1991); Mandryk, Regan Lee, Supporting children's collaboration across handheld computers, Conference on Human Factors in Computing Systems - Proceedings, pp. 255-256, (2001); Sakamura, Ken, Ubiquitous computing technologies for ubiquitous learning, 2005, pp. 11-18, (2005)","","Institute of Advanced Engineering and Science info@iaesjournal.com","","","","","","25024760; 25024752","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85075483841"
"Westera, W.; Prada, R.; Mascarenhas, S.; Santos, P.A.; Dias, J.; Guimarães, M.; Georgiadis, K.; Nyamsuren, E.; Bahreini, K.; Yumak, Z.; Christyowidiasmoro, C.; Dascalu, M.; Gutu-Robu, G.; Rușeți, S.","Westera, Wim (6603477787); Prada, Rui (6603926477); Mascarenhas, Samuel F. (35105447700); Santos, Pedro A. (7102995678); Dias, João Miguel (55107049600); Guimarães, Manuel (57200166288); Georgiadis, Konstantinos (57201100211); Nyamsuren, Enkhbold (24833533100); Bahreini, Kiavash (24828994300); Yumak, Zerrin (14057125600); Christyowidiasmoro, C. (57189388108); Dascalu, Mihai (24831962300); Gutu-Robu, Gabriel (57203851523); Rușeți, Ștefan (57188672704)","6603477787; 6603926477; 35105447700; 7102995678; 55107049600; 57200166288; 57201100211; 24833533100; 24828994300; 14057125600; 57189388108; 24831962300; 57203851523; 57188672704","Artificial intelligence moving serious gaming: Presenting reusable game AI components","2020","Education and Information Technologies","25","1","","351","380","0","96","10.1007/s10639-019-09968-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069895570&doi=10.1007%2Fs10639-019-09968-2&partnerID=40&md5=675fd88fa286c6272ca35acb5583e1d9","Open Universiteit, Heerlen, Netherlands; Instituto de Engenharia de Sistemas e Computadores: Investigação e Desenvolvimento em Lisboa, Lisbon, Portugal; Instituto Superior Técnico, Lisbon, Portugal; Department of Information and Computing Sciences, Universiteit Utrecht, Utrecht, Netherlands; University Politehnica of Bucharest, Bucharest, Romania","Westera, Wim, Open Universiteit, Heerlen, Netherlands; Prada, Rui, Instituto de Engenharia de Sistemas e Computadores: Investigação e Desenvolvimento em Lisboa, Lisbon, Portugal, Instituto Superior Técnico, Lisbon, Portugal; Mascarenhas, Samuel F., Instituto de Engenharia de Sistemas e Computadores: Investigação e Desenvolvimento em Lisboa, Lisbon, Portugal; Santos, Pedro A., Instituto de Engenharia de Sistemas e Computadores: Investigação e Desenvolvimento em Lisboa, Lisbon, Portugal, Instituto Superior Técnico, Lisbon, Portugal; Dias, João Miguel, Instituto de Engenharia de Sistemas e Computadores: Investigação e Desenvolvimento em Lisboa, Lisbon, Portugal, Instituto Superior Técnico, Lisbon, Portugal; Guimarães, Manuel, Instituto de Engenharia de Sistemas e Computadores: Investigação e Desenvolvimento em Lisboa, Lisbon, Portugal, Instituto Superior Técnico, Lisbon, Portugal; Georgiadis, Konstantinos, Open Universiteit, Heerlen, Netherlands; Nyamsuren, Enkhbold, Open Universiteit, Heerlen, Netherlands; Bahreini, Kiavash, Open Universiteit, Heerlen, Netherlands; Yumak, Zerrin, Department of Information and Computing Sciences, Universiteit Utrecht, Utrecht, Netherlands; Christyowidiasmoro, null, Department of Information and Computing Sciences, Universiteit Utrecht, Utrecht, Netherlands; Dascalu, Mihai, University Politehnica of Bucharest, Bucharest, Romania; Gutu-Robu, Gabriel, University Politehnica of Bucharest, Bucharest, Romania; Rușeți, Ștefan, University Politehnica of Bucharest, Bucharest, Romania","This article provides a comprehensive overview of artificial intelligence (AI) for serious games. Reporting about the work of a European flagship project on serious game technologies, it presents a set of advanced game AI components that enable pedagogical affordances and that can be easily reused across a wide diversity of game engines and game platforms. Serious game AI functionalities include player modelling (real-time facial emotion recognition, automated difficulty adaptation, stealth assessment), natural language processing (sentiment analysis and essay scoring on free texts), and believable non-playing characters (emotional and socio-cultural, non-verbal bodily motion, and lip-synchronised speech), respectively. The reuse of these components enables game developers to develop high quality serious games at reduced costs and in shorter periods of time. All these components are open source software and can be freely downloaded from the newly launched portal at gamecomponents.eu. The components come with detailed installation manuals and tutorial videos. All components have been applied and validated in serious games that were tested with real end-users. © 2020 Elsevier B.V., All rights reserved.","Artificial intelligence; Component-based architecture; Game development; Intelligent tutoring systems; Serious games; Software reuse","","","","Funding text 1: This work has been partially funded by the EC H2020 project RAGE (Realising an Applied Gaming Eco-System); http://www.rageproject.eu/; Grant agreement No 644187 and by national funds through Fundação para a Ciência e a Tecnologia (FCT-UID/CEC/500 21/2013).; Funding text 2: This work has been partially funded by the EC H2020 project RAGE (Realising an Applied Gaming Eco-System); http://www.rageproject.eu/; Grant agreement No 644187 and by national funds through Funda??o para a Ci?ncia e a Tecnologia (FCT-UID/CEC/500 21/2013).","Technical Concepts of Component Based Software Engineering, (2000); Bahreini, Kiavash, Data Fusion for Real-time Multimodal Emotion Recognition through Webcams and Microphones in E-Learning, International Journal of Human-Computer Interaction, 32, 5, pp. 415-430, (2016); Bahreini, Kiavash, Communication skills training exploiting multimodal emotion recognition, Interactive Learning Environments, 25, 8, pp. 1065-1082, (2017); A Fuzzy Logic Approach to Reliable Real Time Recognition of Facial Emotions Multimedia Tools and Applications, (2018); D5 4 Pilots Quality Report Round 2 Rage Project, (2018); Beck, Aryel, Body movements generation for virtual characters and social robots, pp. 273-286, (2017); Bernardini, Sara, ECHOES: An intelligent serious game for fostering social communication in children with autism, Information Sciences, 264, pp. 41-60, (2014); Birman, Ken P., Exploiting virtual synchrony in distributed systems, pp. 123-138, (1987); Blei, David M., Latent Dirichlet allocation, Journal of Machine Learning Research, 3, 4-5, pp. 993-1022, (2003); Artificial Intelligence in Games A Survey of the State of the Art, (2012)","","Springer","","","","","","13602357","","","","English","Article","Final","All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85069895570"
"Thabtah, F.","Thabtah, F. Abdeljaber (8349253300)","8349253300","An accessible and efficient autism screening method for behavioural data and predictive analyses","2019","Health Informatics Journal","25","4","","1739","1755","0","112","10.1177/1460458218796636","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058976038&doi=10.1177%2F1460458218796636&partnerID=40&md5=989df41405ff5d99b40dd59296468cd4","Manukau Institute of Technology, Auckland, New Zealand","Thabtah, F. Abdeljaber, Manukau Institute of Technology, Auckland, New Zealand","Autism spectrum disorder is associated with significant healthcare costs, and early diagnosis can substantially reduce these. Unfortunately, waiting times for an autism spectrum disorder diagnosis are lengthy due to the fact that current diagnostic procedures are time-consuming and not cost-effective. Overall, the economic impact of autism and the increase in the number of autism spectrum disorder cases across the world reveal an urgent need for the development of easily implemented and effective screening methods. This article proposes a new mobile application to overcome the problem by offering users and the health community a friendly, time-efficient and accessible mobile-based autism spectrum disorder screening tool called ASDTests. The proposed ASDTests app can be used by health professionals to assist their practice or to inform individuals whether they should pursue formal clinical diagnosis. Unlike existing autism screening apps being tested, the proposed app covers a larger audience since it contains four different tests, one each for toddlers, children, adolescents and adults as well as being available in 11 different languages. More importantly, the proposed app is a vital tool for data collection related to autism spectrum disorder for toddlers, children, adolescent and adults since initially over 1400 instances of cases and controls have been collected. Feature and predictive analyses demonstrate small groups of autistic traits improving the efficiency and accuracy of screening processes. In addition, classifiers derived using machine learning algorithms report promising results with respect to sensitivity, specificity and accuracy rates. © 2021 Elsevier B.V., All rights reserved.","accessibility; autism spectrum disorder; autism spectrum disorder screening methods; behavioural science; classification; machine learning; mobile application","autism; early diagnosis; human; machine learning; mass screening; mobile application; procedures; psychology; Autistic Disorder; Early Diagnosis; Humans; Machine Learning; Mass Screening; Mobile Applications","","","The author(s) received no financial support for the research, authorship and/or publication of this article.","Diagnostic and Statistical Manual of Mental Disorders; Ruzich, Emily, Measuring autistic traits in the general population: A systematic review of the Autism-Spectrum Quotient (AQ) in a nonclinical population sample of 6,900 typical adult males and females, Molecular Autism, 6, 1, (2015); Prevalence of autism spectrum disorder among children aged 8 years - autism and developmental disabilities monitoring network, 11 sites, United States, 2010., MMWR Surveillance Summaries, 63, 2, pp. 1-21, (2014); Brugha, Traolach Sean, Epidemiology of autism spectrum disorders in adults in the community in England, Archives of General Psychiatry, 68, 5, pp. 459-466, (2011); Autism Paradigms Recent Research and Clinical Applications, (2017); Russell, Ailsa J., The mental health of individuals referred for assessment of autism spectrum disorder in adulthood: A clinic report, Autism, 20, 5, pp. 623-627, (2016); Buescher, Ariane V.S., Costs of autism spectrum disorders in the United Kingdom and the United States, JAMA Pediatrics, 168, 8, pp. 721-728, (2014); Leigh, John Paul, Brief Report: Forecasting the Economic Burden of Autism in 2015 and 2025 in the United States, Journal of Autism and Developmental Disorders, 45, 12, pp. 4135-4139, (2015); Informatics for Health and Social Care, (2018); Crane, Laura, Experiences of autism diagnosis: A survey of over 1000 parents in the United Kingdom, Autism, 20, 2, pp. 153-162, (2016)","","SAGE Publications Ltd","","","","","","14604582; 17412811","","HIJEA","30230414","English","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85058976038"
"Dapogny, A.; Grossard, C.; Hun, S.; Serret, S.; Grynszpan, O.; Dubuisson, S.; Cohen, D.; Bailly, K.","Dapogny, Arnaud (56916151100); Grossard, Charline (56646446100); Hun, Stéphanie (56272482600); Serret, Sylvie (7801641628); Grynszpan, Ouriel (22034481500); Dubuisson, Séverine (6507070738); Cohen, David (57198586747); Bailly, Kevin (25633701200)","56916151100; 56646446100; 56272482600; 7801641628; 22034481500; 6507070738; 57198586747; 25633701200","On Automatically Assessing Children's Facial Expressions Quality: A Study, Database, and Protocol","2019","Frontiers in Computer Science","1","","5","","","0","6","10.3389/fcomp.2019.00005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098396226&doi=10.3389%2Ffcomp.2019.00005&partnerID=40&md5=a32de33dff815f7857083e4ca8db0ca7","Sorbonne Université, Paris, France; Service de psychiatrie de l'enfant et de l'adolescent, AP-HP Assistance Publique - Hopitaux de Paris, Paris, France; Cognition Behavior Technology, Nice, France","Dapogny, Arnaud, Sorbonne Université, Paris, France; Grossard, Charline, Service de psychiatrie de l'enfant et de l'adolescent, AP-HP Assistance Publique - Hopitaux de Paris, Paris, France; Hun, Stéphanie, Cognition Behavior Technology, Nice, France; Serret, Sylvie, Cognition Behavior Technology, Nice, France; Grynszpan, Ouriel, Sorbonne Université, Paris, France; Dubuisson, Séverine, Sorbonne Université, Paris, France; Cohen, David, Cognition Behavior Technology, Nice, France; Bailly, Kevin, Sorbonne Université, Paris, France","While there exists a number of serious games geared toward helping children with ASD to produce facial expressions, most of them fail to provide a precise feedback to help children to adequately learn. In the scope of the JEMImE project, which aims at developing such serious game platform, we introduce throughout this paper a machine learning approach for discriminating between facial expressions and assessing the quality of the emotional display. In particular, we point out the limits in generalization capacities of models trained on adult subjects. To circumvent this issue in the design of our system, we gather a large database depicting children's facial expressions to train and validate the models. We describe our protocol to elicit facial expressions and obtain quality annotations, and empirically show that our models obtain high accuracies in both classification and quality assessment of children's facial expressions. Furthermore, we provide some insight on what the models learn and which features are the most useful to discriminate between the various facial expressions classes and qualities. This new model trained on the dedicated dataset has been integrated into a proof of concept of the serious game. © 2021 Elsevier B.V., All rights reserved.","children; dataset; emotion; expression quality; facial expression recognition; random forests","","","","This work has been supported by the French National Agency (ANR) in the frame of its Technological Research CONTINT program (JEMImE, project number ANR-13-CORD-0004) and JCJC program (FACIL, project ANR-17-CE33-0002).","Feldman Barrett, Lisa Feldman, Context in emotion perception, Current Directions in Psychological Science, 20, 5, pp. 286-290, (2011); Breiman, Leo, Random forests, Machine Learning, 45, 1, pp. 5-32, (2001); Bylander, Tom C., Estimating generalization error on two-class datasets using out-of-bag estimates, Machine Learning, 48, 1-3, pp. 287-297, (2002); Using Random Forest to Learn Imbalanced Data, (2004); Proceedings from the 8th IEEE International Conference on Automatic Face Gesture Recognition, (2008); Dalrymple, Kirsten A., The dartmouth database of children's faces: Acquisition and validation of a new face stimulus set, PLOS ONE, 8, 11, (2013); Dapogny, Arnaud, Pairwise conditional random forests for facial expression recognition, Proceedings of the IEEE International Conference on Computer Vision, 2015 International Conference on Computer Vision, ICCV 2015, pp. 3783-3791, (2015); Dapogny, Arnaud, Multi-Output Random Forests for Facial Action Unit Detection, pp. 135-140, (2017); Dapogny, Arnaud, JEMImE: A serious game to teach children with ASD how to adequately produce facial expressions, pp. 723-730, (2018); Dollár, Piotr, Integral channel features, (2009)","","Frontiers Media S.A.","","","","","","26249898","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85098396226"
"Ghosh, S.; Goenka, S.; Ganguly, N.; Mitra, B.; De, P.","Ghosh, Surjya (57188717803); Goenka, Shivam (57215129533); Ganguly, Niloy (56081670000); Mitra, Bivas (23035267800); De, Pradipta (7101660853)","57188717803; 57215129533; 56081670000; 23035267800; 7101660853","Representation Learning for Emotion Recognition from Smartphone Keyboard Interactions","2019","","","","8925518","704","710","0","13","10.1109/ACII.2019.8925518","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077789020&doi=10.1109%2FACII.2019.8925518&partnerID=40&md5=0259fe59b7fc2d382965da0e3a9d5dc8","Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science, Georgia Southern University, Statesboro, United States; Centrum Wiskunde & Informatica, Amsterdam, Netherlands","Ghosh, Surjya, Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India, Centrum Wiskunde & Informatica, Amsterdam, Netherlands; Goenka, Shivam, Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Ganguly, Niloy, Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Mitra, Bivas, Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; De, Pradipta, Department of Computer Science, Georgia Southern University, Statesboro, United States","Characteristics of typing on smartphone keyboards among different individuals can elicit emotion, similar to speech prosody or facial expressions. Existing works on typing based emotion recognition rely on feature engineering to build machine learning models, while recent speech and facial expression based techniques have shown the efficacy of learning the features automatically. Therefore, in this work, we explore the effectiveness of such learning models in keyboard interaction based emotion detection. In this paper, we propose an end-to-end framework, which first uses a sequence-based encoding method to automatically learn the representation from raw keyboard interaction pattern and subsequently uses this representation to train a multi-task learning based neural network (MTL-NN)to identify different emotions. We carry out a 3-week in-the-wild study involving 24 participants using a custom keyboard capable of tracing users' interaction pattern during text entry. We collect interaction details like touch speed, error rate, pressure and self-reported emotions (happy, sad, stressed, relaxed) during the study. Our analysis on the collected dataset reveals that the representation learnt from the interaction pattern has an average correlation of 0.901 within the same emotion and 0.811 between different emotions. As a result, the representation is effective in distinguishing different emotions with an average accuracy (AUCROC)of 84%. © 2020 Elsevier B.V., All rights reserved.","Emotion detection; Keyboard interaction; Representation learning; Smartphone interaction","Intelligent computing; Learning systems; Smartphones; Emotion detection; Emotion recognition; Facial Expressions; Feature engineerings; Interaction pattern; Keyboard interaction; Machine learning models; Representation learning; Speech recognition","","","ACKNOWLEDGEMENT This research was supported by TCS under the project titled “Behavior Modeling In Multi-sensor Environments - Integrating Environment Sensing, Human Sensing & Social Sensing for Rich Insights” (VN/BK/18-19/AUG/65).","Mottelson, Aske, An affect detection technique using mobile commodity sensors in the wild, pp. 781-792, (2016); Ghosh, Surjya, Evaluating effectiveness of smartphone typing as an indicator of user emotion, 2018-January, pp. 146-151, (2017); Lee, Hosub, Towards unobtrusive emotion recognition for affective social communication, pp. 260-264, (2012); Ghosh, Surjya, TapSense: Combining self-report patterns and typing characteristics for smartphone based emotion detection, (2017); IEEE Trans Affective Comput, (2016); Kim, Hyunjun, Exploring emotional preference for smartphone applications, pp. 245-249, (2012); Bixler, Robert E., Detecting boredom and engagement during writing with keystroke analysis, task appraisals, and stable traits, International Conference on Intelligent User Interfaces, Proceedings IUI, pp. 225-233, (2013); Proceedings of the Pervasivehealth, (2015); Ghosh, Surjya, Emotion detection from touch interactions during text entry on smartphones, International Journal of Human Computer Studies, 130, pp. 47-57, (2019); Li, Pengcheng, An attention pooling based representation learning method for speech emotion recognition, Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH, 2018-September, pp. 3087-3091, (2018)","","Institute of Electrical and Electronics Engineers Inc.","","8th International Conference on Affective Computing and Intelligent Interaction, ACII 2019","","Cambridge","155962","","9781728138886","","","English","Conference paper","Final","All Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85077789020"
"Kanjo, E.; Younis, E.M.G.; Ang, C.S.","Kanjo, Eiman (25224732300); Younis, Eman M. G. (55413342500); Ang, Chee Siang (15831174100)","25224732300; 55413342500; 15831174100","Deep learning analysis of mobile physiological, environmental and location sensor data for emotion detection","2019","Information Fusion","49","","","46","56","0","247","10.1016/j.inffus.2018.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054071151&doi=10.1016%2Fj.inffus.2018.09.001&partnerID=40&md5=59c2a479681e316784a54a3ca8fc8b88","Computing and Technology, Nottingham Trent University, Nottingham, United Kingdom; Faculty of Computers and Information, Minya, Egypt; School of Engineering and Digital Arts, University of Kent, Canterbury, United Kingdom","Kanjo, Eiman, Computing and Technology, Nottingham Trent University, Nottingham, United Kingdom; Younis, Eman M. G., Faculty of Computers and Information, Minya, Egypt; Ang, Chee Siang, School of Engineering and Digital Arts, University of Kent, Canterbury, United Kingdom","The detection and monitoring of emotions are important in various applications, e.g., to enable naturalistic and personalised human-robot interaction. Emotion detection often require modelling of various data inputs from multiple modalities, including physiological signals (e.g., EEG and GSR), environmental data (e.g., audio and weather), videos (e.g., for capturing facial expressions and gestures) and more recently motion and location data. Many traditional machine learning algorithms have been utilised to capture the diversity of multimodal data at the sensors and features levels for human emotion classification. While the feature engineering processes often embedded in these algorithms are beneficial for emotion modelling, they inherit some critical limitations which may hinder the development of reliable and accurate models. In this work, we adopt a deep learning approach for emotion classification through an iterative process by adding and removing large number of sensor signals from different modalities. Our dataset was collected in a real-world study from smart-phones and wearable devices. It merges local interaction of three sensor modalities: on-body, environmental and location into global model that represents signal dynamics along with the temporal relationships of each modality. Our approach employs a series of learning algorithms including a hybrid approach using Convolutional Neural Network and Long Short-term Memory Recurrent Neural Network (CNN-LSTM) on the raw sensor data, eliminating the needs for manual feature extraction and engineering. The results show that the adoption of deep-learning approaches is effective in human emotion classification when large number of sensors input is utilised (average accuracy 95% and F-Measure=%95) and the hybrid models outperform traditional fully connected deep neural network (average accuracy 73% and F-Measure=73%). Furthermore, the hybrid models outperform previously developed Ensemble algorithms that utilise feature engineering to train the model average accuracy 83% and F-Measure=82%) © 2018 Elsevier B.V., All rights reserved.","Convoltutional neural network; Deep learning; Emotion recognition; Long short-term memory mobile sensing","Biomedical signal processing; Brain; Classification (of information); Data mining; Deep learning; Deep neural networks; Human computer interaction; Human robot interaction; Iterative methods; Learning algorithms; Location; Smartphones; Convolutional neural network; Emotion classification; Emotion recognition; Feature engineerings; Mobile sensing; Multiple modalities; Physiological signals; Temporal relationships; Long short-term memory","","","","Plasqui, Guy, Physical activity assessment with accelerometers: An evaluation against doubly labeled water, Obesity, 15, 10, pp. 2371-2379, (2007); Lara, Óscar D., A survey on human activity recognition using wearable sensors, IEEE Communications Surveys and Tutorials, 15, 3, pp. 1192-1209, (2013); Tone, Erin B., Facial expression recognition in adolescents with mood and anxiety disorders, American Journal of Psychiatry, 160, 6, pp. 1172-1174, (2003); Pham, Trang, DeepCare: A deep dynamic memory model for predictive medicine, Lecture Notes in Computer Science, 9652 LNAI, pp. 30-41, (2016); Cowie, Roddy, Emotion recognition in human-computer interaction, IEEE Signal Processing Magazine, 18, 1, pp. 32-80, (2001); Kanjo, Eiman, Towards unravelling the relationship between on-body, environmental and emotion data using sensor information fusion approach, Information Fusion, 40, pp. 18-31, (2018); Kanjo, Eiman, NotiMind: Utilizing Responses to Smart Phone Notifications as Affective Sensors, IEEE Access, 5, pp. 22023-22035, (2017); Jerritta, S., Physiological signals based human emotion recognition: A review, pp. 410-415, (2011); Busso, Carlos, Analysis of emotion recognition using facial expressions, speech and multimodal information, pp. 205-211, (2004); Pers Ubiquitous Comput, (2015)","","Elsevier B.V.","","","","","","15662535","","","","English","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85054071151"
"Millar, L.; McConnachie, A.; Minnis, H.; Wilson, P.; Thompson, L.; Anzulewicz, A.; Sobota, K.; Philip J. Rowe, P.; Gillberg, C.; Delafield-Butt, J.","Millar, Lindsay Jane (57202988037); McConnachie, Alex M. (6701526254); Minnis, Helen J. (6602079258); Wilson, Philip Michael John (57203055015); Thompson, Lucy C. (28268123000); Anzulewicz, Anna (56059865400); Sobota, Krzysztof (57190870598); Philip J. Rowe, Philip John (35617880500); Gillberg, Christopher L. (7101634220); Delafield-Butt, Jonathan T. (8983874000)","57202988037; 6701526254; 6602079258; 57203055015; 28268123000; 56059865400; 57190870598; 35617880500; 7101634220; 8983874000","Phase 3 diagnostic evaluation of a smart tablet serious game to identify autism in 760 children 3-5 years old in Sweden and the United Kingdom","2019","BMJ Open","9","7","e026226","","","0","19","10.1136/bmjopen-2018-026226","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069521298&doi=10.1136%2Fbmjopen-2018-026226&partnerID=40&md5=3879259d32807dbc3f0ec8fac24ecff9","Laboratory for Innovation in Autism, University of Strathclyde, Glasgow, United Kingdom; Biomedical Engineering, University of Strathclyde, Glasgow, United Kingdom; University of Glasgow, Glasgow, United Kingdom; University of Glasgow, Glasgow, United Kingdom; Centre for Rural Health, University of Aberdeen, Aberdeen, United Kingdom; Gillberg Neuropsychiatry Centre, Sahlgrenska Akademin, Gothenburg, Sweden; SM Polska Sp. Z O.o. S.K., Krakow, Poland","Millar, Lindsay Jane, Laboratory for Innovation in Autism, University of Strathclyde, Glasgow, United Kingdom, Biomedical Engineering, University of Strathclyde, Glasgow, United Kingdom; McConnachie, Alex M., University of Glasgow, Glasgow, United Kingdom; Minnis, Helen J., University of Glasgow, Glasgow, United Kingdom; Wilson, Philip Michael John, Centre for Rural Health, University of Aberdeen, Aberdeen, United Kingdom; Thompson, Lucy C., Centre for Rural Health, University of Aberdeen, Aberdeen, United Kingdom, Gillberg Neuropsychiatry Centre, Sahlgrenska Akademin, Gothenburg, Sweden; Anzulewicz, Anna, SM Polska Sp. Z O.o. S.K., Krakow, Poland; Sobota, Krzysztof, SM Polska Sp. Z O.o. S.K., Krakow, Poland; Philip J. Rowe, Philip John, Laboratory for Innovation in Autism, University of Strathclyde, Glasgow, United Kingdom, Biomedical Engineering, University of Strathclyde, Glasgow, United Kingdom; Gillberg, Christopher L., University of Glasgow, Glasgow, United Kingdom, Gillberg Neuropsychiatry Centre, Sahlgrenska Akademin, Gothenburg, Sweden; Delafield-Butt, Jonathan T., Laboratory for Innovation in Autism, University of Strathclyde, Glasgow, United Kingdom","Introduction Recent evidence suggests an underlying movement disruption may be a core component of autism spectrum disorder (ASD) and a new, accessible early biomarker. Mobile smart technologies such as iPads contain inertial movement and touch screen sensors capable of recording subsecond movement patterns during gameplay. A previous pilot study employed machine learning analysis of motor patterns recorded from children 3-5 years old. It identified those with ASD from age-matched and gender-matched controls with 93% accuracy, presenting an attractive assessment method suitable for use in the home, clinic or classroom. Methods and analysis This is a phase III prospective, diagnostic classification study designed according to the Standards for Reporting Diagnostic Accuracy Studies guidelines. Three cohorts are investigated: children typically developing (TD); children with a clinical diagnosis of ASD and children with a diagnosis of another neurodevelopmental disorder (OND) that is not ASD. The study will be completed in Glasgow, UK and Gothenburg, Sweden. The recruitment target is 760 children (280 TD, 280 ASD and 200 OND). Children play two games on the iPad then a third party data acquisition and analysis algorithm (Play.Care, Harimata) will classify the data as positively or negatively associated with ASD. The results are blind until data collection is complete, when the algorithm's classification will be compared against medical diagnosis. Furthermore, parents of participants in the ASD and OND groups will complete three questionnaires: Strengths and Difficulties Questionnaire; Early Symptomatic Syndromes Eliciting Neurodevelopmental Clinical Examinations Questionnaire and the Adaptive Behavioural Assessment System-3 or Vineland Adaptive Behavior Scales-II. The primary outcome measure is sensitivity and specificity of Play.Care to differentiate ASD children from TD children. Secondary outcomes measures include the accuracy of Play.Care to differentiate ASD children from OND children. Ethics and dissemination This study was approved by the West of Scotland Research Ethics Service Committee 3 and the University of Strathclyde Ethics Committee. Results will be disseminated in peer-reviewed publications and at international scientific conferences. Trial registration number NCT03438994; Pre-results. © 2019 Elsevier B.V., All rights reserved.","autism; diagnosis; digital health; machine learning; motor control; smart technology","Adaptive Behavioural Assessment System 3; Article; autism; autism assessment; Autism Diagnostic Interview Revised; child; clinical assessment; clinical evaluation; cohort analysis; comparative study; controlled study; developmental disorder; diagnostic accuracy; diagnostic value; disease association; Early Symptomatic Syndrome Eliciting Neurodevelopmental Clinical Examinations Questionnaire; Glasgow coma scale; human; machine learning; major clinical study; mental function assessment; outcome assessment; phase 3 clinical trial; practice guideline; prospective study; questionnaire; recreational game; sensitivity and specificity; strengths and difficulties questionnaire; Sweden; United Kingdom; Vineland Adaptive Behavior Scale 2; female; male; pathophysiology; phase 3 clinical trial (topic); preschool child; video game; Autistic Disorder; Child, Preschool; Clinical Trials, Phase III as Topic; Female; Humans; Male; Prospective Studies; Video Games","","","Acknowledgements We are grateful to the children, their parents, teachers and clinicians who have worked so hard to make this study possible, both in its design and in its implementation. Contributors JD-B, CG, HM, PR, PW, LT, AM, KS and AA produced the study design. LM, JD-B, CG, HM, PR, AM, PW and LT developed the study protocol with input from KS and AA. Sample size calculations were carried out by AM. KS, AA and JD-B were responsible for technical development and pilot work. This paper was written by LM and JD-B with input from all coauthors. Funding This work was subcontracted to the University of Strathclyde by Harimata sp. z o.o. as an integral part of a Horizon 2020 SME Instrument, grant number 756079. Competing interests The academic authors LM, HM, PW, AM, PR, CG and JD-B are members of the trial steering and management committees and declare no financial interest in this product or the funding company, Harimata sp. z o.o. Coauthors AA and KS are board members of the Harimata sp. z o.o. that intends to commercialise the Play.Care assessment technology. AA and KS have options vesting in the company. AA is a voting member of the trial steering committee. Patient consent for publication Not required. Ethics approval This study was granted approval by the West of Scotland Research Ethics Service Committee 3 (reference number: 17-WS-0223 231435) for the National Health Service and by the University of Strathclyde Ethics Committee. Provenance and peer review Not commissioned; externally peer reviewed.","Baio, Jon, Prevalence of autism spectrum disorder among children aged 8 Years - Autism and developmental disabilities monitoring network, 11 Sites, United States, 2014, MMWR Surveillance Summaries, 67, 6, pp. 1-23, (2018); Brugha, Traolach Sean, Epidemiology of autism spectrum disorders in adults in the community in England, Archives of General Psychiatry, 68, 5, pp. 459-466, (2011); Knapp, Martin R.J., Economic cost of autism in the UK, Autism, 13, 3, pp. 317-336, (2009); Bradshaw, Jessica L., Feasibility and Effectiveness of Very Early Intervention for Infants At-Risk for Autism Spectrum Disorder: A Systematic Review, Journal of Autism and Developmental Disorders, 45, 3, pp. 778-794, (2015); Pickles, Andrew R., Parent-mediated social communication therapy for young children with autism (PACT): long-term follow-up of a randomised controlled trial, The Lancet, 388, 10059, pp. 2501-2509, (2016); Peters-Scheffer, Nienke C., Cost comparison of early intensive behavioral intervention and treatment as usual for children with autism spectrum disorder in the Netherlands, Research in Developmental Disabilities, 33, 6, pp. 1763-1772, (2012); Chasson, Gregory S., Cost comparison of early intensive behavioral intervention and special education for children with autism, Journal of Child and Family Studies, 16, 3, pp. 401-413, (2007); Concern as Children in York Face Long Waits for Autism Diagnosis the Press, (2019); Autism Assessment Delay Concerns Across Wales Bbc News, (2018); Trevarthen, Colwyn B., Autism as a developmental disorder in intentional movement and affective engagement, Frontiers in Integrative Neuroscience, JUN, pp. 1-31, (2013)","","BMJ Publishing Group subscriptions@bmjgroup.com","","","","","","20446055","","","31315858","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85069521298"
"Keshav, N.U.; Vogt-Lowell, K.; Vahabzadeh, A.; Sahin, N.T.","Keshav, Neha U. (57194451409); Vogt-Lowell, Kevin (57336711000); Vahabzadeh, Arshya B.N. (36085598300); Sahin, Ned T. (7005485925)","57194451409; 57336711000; 36085598300; 7005485925","Digital attention-related augmented-reality game: Significant correlation between student game performance and validated clinical measures of attention-deficit/hyperactivity disorder (ADHD)","2019","Children","6","6","72","","","0","32","10.3390/children6060072","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081172040&doi=10.3390%2Fchildren6060072&partnerID=40&md5=649155b05977d5ecf1e6a21b01dfd8b3","Brain Power, Cambridge, United States; Department of Psychiatry, Massachusetts General Hospital, Boston, United States; Harvard Faculty of Arts and Sciences, Cambridge, United States","Keshav, Neha U., Brain Power, Cambridge, United States; Vogt-Lowell, Kevin, Brain Power, Cambridge, United States; Vahabzadeh, Arshya B.N., Brain Power, Cambridge, United States, Department of Psychiatry, Massachusetts General Hospital, Boston, United States; Sahin, Ned T., Brain Power, Cambridge, United States, Harvard Faculty of Arts and Sciences, Cambridge, United States","As many as half of school children with autism spectrum disorder (ASD) exhibit symptoms of attention-deficit/hyperactivity disorder (ADHD), resulting in marked negative academic, social, and behavioral outcomes. The focus of the US Food and Drug Administration (FDA) on real-world data from novel digital sources, and the emergence of Current Procedural Terminology (CPT) codes to reimburse for digital monitoring and neurobehavioral testing suggest an increasing acceptance of the role of technology in augmenting clinical care and research. Empowered Brain is an augmented reality and artificial intelligence-based social-emotional communication aid for students with ASD. In this study, student performance on Empowered Brain is correlated to validated clinical measures of ADHD. Seven high school students with a diagnosis of ASD were recruited from a public high school. All students were assessed for severity of ADHD-related symptoms via three clinical gold-standard assessments, namely the Aberrant Behavioral Checklist (ABC), Social Responsiveness Scale 2 (SRS-2), and Teacher Report Form (TRF). Students used Empowered Brain over a one-week period. We measured the correlation of student in-game performance (as measured by point-and star-based rewards) relative to the hyperactivity subscale of the ABC (ABC-H), and the ADHD-subscale of the TRF. All seven students completed the study and managed to successfully use Empowered Brain. Students received a culminative total of 32 sessions, an average of 4.6 sessions per student (range 2–8). Student in-game performance demonstrated highly significant correlation relative to ABC-H (points: p = 0.0013; stars: p = 0.0013), and significant correlation to TRF ADHD scores (points: p = 0.012; stars: p = 0.012). No adverse effects were noted among students who used Empowered Brain. New technologies may herald novel ways of identifying and characterizing symptoms of ADHD in student populations. This study provides evidence that Empowered Brain in-game performance correlates with ADHD symptom severity in students with ASD. Larger samples are required to validate these findings, with more diverse participants that can also widen the generalizability of these findings to a broader range of brain conditions that manifest with inattention, impulsivity, and hyperactivity. Through further research, we may find that such technologies can help us to identify and longitudinally monitor such symptoms, and potentially aid in severity stratification and digital phenotyping. © 2021 Elsevier B.V., All rights reserved.","ADHD; Artificial intelligence; ASD; Assistive technology; Attention; Augmented reality; Autism; Autism; Digital phenotyping; Google Glass; Hyperactivity; Serious games; Social-emotional learning; Special education","","","","Funding text 1: This work has been supported by the Office of the Assistant Secretary of Defense for Health Affairs, through the Autism Research Program under Award No. W81XWH 17-1-0449. Early work to transform smartglasses into biomedical sensors was supported in part by the United States Army Medical Research and Materiel Command under Contract No. W81XWH-14-C-0007 (awarded to TIAX, LLC). Opinions, interpretations, conclusions, and recommendations are those of the authors and are not necessarily endorsed by the Department of Defense.; Funding text 2: Funding: This work has been supported by the Office of the Assistant Secretary of Defense for Health Affairs, through the Autism Research Program under Award No. W81XWH 17-1-0449. Early work to transform smartglasses into biomedical sensors was supported in part by the United States Army Medical Research and Materiel Command under Contract No. W81XWH-14-C-0007 (awarded to TIAX, LLC). Opinions, interpretations, conclusions, and recommendations are those of the authors and are not necessarily endorsed by the Department of Defense.","Diagnostic and Statistical Manual of Mental Disorders, (2000); Shattuck, Paul T., Postsecondary education and employment among youth with an autism spectrum disorder, Pediatrics, 129, 6, pp. 1042-1049, (2012); Taylor, Julie Lounds, A longitudinal examination of 10-year change in vocational and educational activities for adults with autism spectrum disorders, Developmental Psychology, 50, 3, pp. 699-708, (2014); Kohane, Isaac S., The co-morbidity burden of children and young adults with autism spectrum disorders., PLOS ONE, 7, 4, (2012); Eaves, Linda C., Young adult outcome of autism spectrum disorders, Journal of Autism and Developmental Disorders, 38, 4, pp. 739-747, (2008); Roux, Anne M., Postsecondary employment experiences among young adults with an autism spectrum disorder, Journal of the American Academy of Child and Adolescent Psychiatry, 52, 9, pp. 931-939, (2013); Leyfer, Ovsanna T., Comorbid psychiatric disorders in children with autism: Interview development and rates of disorders, Journal of Autism and Developmental Disorders, 36, 7, pp. 849-861, (2006); Simonoff, Emily A., Psychiatric disorders in children with autism spectrum disorders: Prevalence, comorbidity, and associated factors in a population-derived sample, Journal of the American Academy of Child and Adolescent Psychiatry, 47, 8, pp. 921-929, (2008); Chiang, Huey-Ling Ling, School dysfunction in youth with autistic spectrum disorder in Taiwan: The effect of subtype and ADHD, Autism Research, 11, 6, pp. 857-869, (2018); Loe, Irene Marilyn, Academic and educational outcomes of children with ADHD, Journal of Pediatric Psychology, 32, 6, pp. 643-654, (2007)","","MDPI","","","","","","22279067","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85081172040"
"Egger, M.; Ley, M.; Hanke, S.","Egger, Maria (57225451329); Ley, Matthias (57211684555); Hanke, Sten (56246089700)","57225451329; 57211684555; 56246089700","Emotion Recognition from Physiological Signal Analysis: A Review","2019","Electronic Notes in Theoretical Computer Science","343","","","35","55","0","449","10.1016/j.entcs.2019.04.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073162689&doi=10.1016%2Fj.entcs.2019.04.009&partnerID=40&md5=ac001d6cd4e9b8179f5523e487c77955","Austrian Institute of Technology, Vienna, Austria","Egger, Maria, Austrian Institute of Technology, Vienna, Austria; Ley, Matthias, Austrian Institute of Technology, Vienna, Austria; Hanke, Sten, Austrian Institute of Technology, Vienna, Austria","Human computer interaction is increasingly utilized in smart home, industry 4.0 and personal health. Communication between human and computer can benefit by a flawless exchange of emotions. As emotions have substantial influence on cognitive processes of the human brain such as learning, memory, perception and problem solving, emotional interactions benefit different applications. It can further be relevant in modern health care especially in interaction with patients suffering from stress or depression. Additionally rehabilitation applications, guiding patients through their rehabilitation training while adapting to the patients emotional state, would be highly motivating and might lead to a faster recovery. Depending on the application area, different systems for emotion recognition suit different purposes. The aim of this work is to give an overview of methods to recognize emotions and to compare their applicability based on existing studies. This review paper should enable practitioners, researchers and engineers to find a system most suitable for certain applications. An entirely contact-less method is to analyze facial features with the help of a video camera. This is useful when computers, smart-phones or tablets with integrated cameras are included in the task. Smart wearables provide contact with the skin and physiological parameters such as electro-dermal activity and heart related signals can be recorded unobtrusively also during dynamical tasks. Next to unimodal solutions, multimodal affective computing systems are analyzed since they promise higher classification accuracy. Accuracy varies based on the amount of detected emotions, extracted features, classification method and the quality of the database. Electroencephalography achieves 88.86 % accuracy for four emotions, multimodal measurements (Electrocardiography, Electromyography and bio-signals) 79.3 % for four emotive states, facial recognition 89 % for seven states and speech recognition 80.46 % for happiness and sadness. Looking forward, heart-related parameters might be an option to measure emotions accurately and unobtrusive with the help of smart wearables. This can be used in dynamic or outdoor tasks. Facial recognition on the other hand is a useful contact-less tool when it comes to emotion recognition during computer interaction. © 2019 Elsevier B.V., All rights reserved.","Affective computing; Emotion recognition; Emotional intelligence; Facial recognition; HRV; Review; State of the art","Ambient intelligence; Artificial intelligence; Automation; Biomedical signal processing; Classification (of information); Electroencephalography; Electrophysiology; Emotional intelligence; Face recognition; Human computer interaction; mHealth; Patient rehabilitation; Physiological models; Reviews; Signal analysis; Smartphones; Video cameras; Wearable technology; Affective Computing; Classification accuracy; Emotion recognition; Facial recognition; Multi-modal measurement; Physiological parameters; Rehabilitation training; State of the art; Speech recognition","","","This work has been funded by the AAL Joint Program project FollowMe (Project. No.: AAL-2015-2-108). The authors would like to thank the members of the projects consortium for their valuable inputs.","Agrafioti, Foteini, ECG pattern analysis for emotion detection, IEEE Transactions on Affective Computing, 3, 1, pp. 102-115, (2012); Axisa, Fabrice, Smart clothes for the monitoring in real time and conditions of physiological, emotional and sensorial reactions of human, Annual International Conference of the IEEE Engineering in Medicine and Biology - Proceedings, 4, pp. 3744-3747, (2003); Openface an Open Source Facial Behaviour Analysis Toolkit, (2018); Banse, Rainer, Acoustic Profiles in Vocal Emotion Expression, Journal of Personality and Social Psychology, 70, 3, pp. 614-636, (1996); Beale, Russell, The role of affect and emotion in HCI, Lecture Notes in Computer Science, 4868 LNCS, pp. 1-11, (2008); Benedek, Mathias, A continuous measure of phasic electrodermal activity, Journal of Neuroscience Methods, 190, 1, pp. 80-91, (2010); Bradley, Margaret M., Measuring emotion: The self-assessment manikin and the semantic differential, Journal of Behavior Therapy and Experimental Psychiatry, 25, 1, pp. 49-59, (1994); International Affective Digitized Sounds Iads Stimuli Instruction Manual and Affective Ratings, (1999); Brady, Sarah, Garment-based monitoring of respiration rate using a foam pressure sensor, Proceedings - International Symposium on Wearable Computers, ISWC, 2005, pp. 214-215, (2005); Breazeal, Cynthia L., Emotion and sociable humanoid robots, International Journal of Human Computer Studies, 59, 1-2, pp. 119-155, (2003)","Chatzigiannakis, I.; Sadri, F.; Hanke, S.; Garschall, M.; Himmelsbach, J.; Lalos, A.S.; Leligou, H.C.; Mylonas, G.","Elsevier B.V.","","European Conference on Ambient Intelligence, AMI 2018","","Larnaca","141468","15710661","","","","English","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85073162689"
"Guimarães, A.J.; Silva Araujo, V.J.S.; Araújo, V.S.; Batista, L.O.; De Campos Souza, P.V.C.","Guimarães, Augusto Junio (57204780479); Silva Araujo, Vinicius Jonathan (57208820754); Araújo, Vanessa Souza (57206288108); Batista, Lucas Oliveira (57208821513); De Campos Souza, Paulo Vitor (57218164676)","57204780479; 57208820754; 57206288108; 57208821513; 57218164676","A Hybrid Model Based on Fuzzy Rules to Act on the Diagnosed of Autism in Adults","2019","IFIP Advances in Information and Communication Technology","559","","","401","412","0","24","10.1007/978-3-030-19823-7_34","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065924304&doi=10.1007%2F978-3-030-19823-7_34&partnerID=40&md5=9d9d67e41890549416859a9636553281","Centro Federal de Educacao Tecnologica de Minas Gerais, Belo Horizonte, Brazil; Faculty UNA Betim, 640 - Centro, Brazil","Guimarães, Augusto Junio, Faculty UNA Betim, 640 - Centro, Brazil; Silva Araujo, Vinicius Jonathan, Faculty UNA Betim, 640 - Centro, Brazil; Araújo, Vanessa Souza, Faculty UNA Betim, 640 - Centro, Brazil; Batista, Lucas Oliveira, Faculty UNA Betim, 640 - Centro, Brazil; De Campos Souza, Paulo Vitor, Centro Federal de Educacao Tecnologica de Minas Gerais, Belo Horizonte, Brazil, Faculty UNA Betim, 640 - Centro, Brazil","Aspects of Autistic Spectrum Disorder (ASD) can be diagnosed, with rare frequency, in people already in adulthood. To aid in the diagnosed of autistic traits, a mobile system was developed with the objective of executing the techniques extracted from expert studies to determine the effective diagnosis of the disease. This type of system uses artificial intelligence capabilities and machine learning techniques to assign probabilities to people who pass the in-app test. According to the information provided by the authors of the mobile application, future research could address the use of other intelligent models to assist in predicting whether or not the patient has traits of autism. Therefore, this paper proposes the insertion of a hybrid interpretive technique based on the synergy of the concepts of artificial neural networks and fuzzy systems trained by the extreme learning machine to generate fuzzy rules to deal with questions provided by users seeking to obtain immediate answers on preliminary diagnoses of autism in adults. The tests performed achieved high levels of accuracy superior to the preliminary studies that inspired this research, making it a viable alternative for the efficient diagnosed of autism in adults. © 2019 Elsevier B.V., All rights reserved.","Autism in adults; Extreme learning machine; Fuzzy neural network; Fuzzy rules","Diagnosis; Diseases; Fuzzy logic; Fuzzy neural networks; Fuzzy rules; Knowledge acquisition; Learning systems; Autism in adults; Autistic spectrum disorders; Extreme learning machine; Hybrid model; Intelligent models; Machine learning techniques; Mobile applications; Mobile systems; Fuzzy inference","","","","International Congress on Engineering and Life Science, (2018); Int J Forensic Comput Sci, (2018); Tenth International Conference on Forensic Computer Science and Cyber Law Icofcs 2018, (2018); Buyukoflaz, Fatiha Nur, Early autism diagnosis of children with machine learning algorithms, pp. 1-4, (2018); De Campos Souza, Paulo Vitor, Pruning fuzzy neural networks based on unineuron for problems of classification of patterns, Journal of Intelligent and Fuzzy Systems, 35, 2, pp. 2597-2605, (2018); De Campos Souza, Paulo Vitor, Using fuzzy neural networks for improving the prediction of children with autism through mobile devices., Proceedings - International Symposium on Computers and Communications, 2018-June, pp. 1086-1089, (2018); De Campos Souza, Paulo Vitor, Fuzzy neural networks based on fuzzy logic neurons regularized by resampling techniques and regularization theory for regression problems, Inteligencia Artificial, 21, 62, pp. 114-133, (2018); De Campos Souza, Paulo Vitor, Regularized fuzzy neural networks based on nullneurons for problems of classification of patterns, pp. 25-30, (2018); De Campos Souza, Paulo Vitor, Uninorm based regularized fuzzy neural networks, pp. 1-8, (2018); De Campos Souza, Paulo Vitor, Regularized fuzzy neural network based on or neuron for time series forecasting, Communications in Computer and Information Science, 831, pp. 13-23, (2018)","Iliadis, L.; Maglogiannis, I.; MacIntyre, J.; Pimenidis, E.","Springer New York LLC barbara.b.bertram@gsk.com","","15th IFIP WG 12.5 International Conference on Artificial Intelligence Applications and Innovations, AIAI 2019","","Hersonissos","226189","1868422X; 18684238","9783032007766; 9783031962301; 9783031949234; 9783031971143; 9783031962387; 9783031965210; 9780387291215; 9783319900223; 9783319162737; 1402080697","","","English","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85065924304"
"Daniels, J.; Schwartz, J.; Voss, C.; Haber, N.; Fazel, A.; Kline, A.; Washington, P.; Feinstein, C.; Winograd, T.; Wall, D.P.","Daniels, Jena (56145159000); Schwartz, Jessey Nicole (57196152141); Voss, Catalin (57190141053); Haber, Nick (56353182800); Fazel, Azar (57190138415); Kline, Aaron (57191505004); Washington, Peter Yigitcan (57191498961); Feinstein, Carl B. (7005771772); Winograd, Terry (7003633896); Wall, Dennis Paul (7202196193)","56145159000; 57196152141; 57190141053; 56353182800; 57190138415; 57191505004; 57191498961; 7005771772; 7003633896; 7202196193","Exploratory study examining the at-home feasibility of a wearable tool for social-affective learning in children with autism","2018","npj Digital Medicine","1","1","32","","","0","66","10.1038/s41746-018-0035-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135599026&doi=10.1038%2Fs41746-018-0035-3&partnerID=40&md5=1fc75cda2d09fbbf977ea7630f66e7d2","Department of Pediatrics, Stanford University, Stanford, United States; Stanford Engineering, Stanford, United States; Department of Psychiatry and Behavioral Sciences, Stanford University, Stanford, United States; Department of Biomedical Data Science, Stanford University, Stanford, United States","Daniels, Jena, Department of Pediatrics, Stanford University, Stanford, United States; Schwartz, Jessey Nicole, Department of Pediatrics, Stanford University, Stanford, United States; Voss, Catalin, Stanford Engineering, Stanford, United States; Haber, Nick, Department of Pediatrics, Stanford University, Stanford, United States; Fazel, Azar, Department of Pediatrics, Stanford University, Stanford, United States; Kline, Aaron, Department of Pediatrics, Stanford University, Stanford, United States; Washington, Peter Yigitcan, Stanford Engineering, Stanford, United States; Feinstein, Carl B., Department of Psychiatry and Behavioral Sciences, Stanford University, Stanford, United States; Winograd, Terry, Stanford Engineering, Stanford, United States; Wall, Dennis Paul, Department of Pediatrics, Stanford University, Stanford, United States, Department of Psychiatry and Behavioral Sciences, Stanford University, Stanford, United States, Department of Biomedical Data Science, Stanford University, Stanford, United States","Although standard behavioral interventions for autism spectrum disorder (ASD) are effective therapies for social deficits, they face criticism for being time-intensive and overdependent on specialists. Earlier starting age of therapy is a strong predictor of later success, but waitlists for therapies can be 18 months long. To address these complications, we developed Superpower Glass, a machine-learning-assisted software system that runs on Google Glass and an Android smartphone, designed for use during social interactions. This pilot exploratory study examines our prototype tool’s potential for social-affective learning for children with autism. We sent our tool home with 14 families and assessed changes from intake to conclusion through the Social Responsiveness Scale (SRS-2), a facial affect recognition task (EGG), and qualitative parent reports. A repeated-measures one-way ANOVA demonstrated a decrease in SRS-2 total scores by an average 7.14 points (F(1,13) = 33.20, p = <.001, higher scores indicate higher ASD severity). EGG scores also increased by an average 9.55 correct responses (F(1,10) = 11.89, p = <.01). Parents reported increased eye contact and greater social acuity. This feasibility study supports using mobile technologies for potential therapeutic purposes. © 2022 Elsevier B.V., All rights reserved.","","Diseases; Economic and social effects; Wearable technology; Affective learning; Android smartphone; Autism spectrum disorders; Behavioral interventions; Children with autisms; Effective therapy; Exploratory studies; Google+; Machine-learning; Software-systems; Glass; Article; assessment of humans; autism; Autism Diagnostic Observation Schedule; child; Child Behavior Checklist; clinical article; comorbidity; controlled study; facial expression; feasibility study; female; human; machine learning; male; priority journal; race difference; school child; social cognition; social competence; social learning; Social Responsiveness Scale","","","","Christensen, Deborah L., Prevalence and characteristics of autism spectrum disorder among children aged 8 years - Autism and developmental disabilities monitoring network, 11 sites, United States, 2012, MMWR Surveillance Summaries, 65, 3, pp. 1-23, (2016); Christensen, Deborah L., Prevalence and characteristics of autism spectrum disorder among 4-year-old children in the autism and developmental disabilities monitoring network, Journal of Developmental and Behavioral Pediatrics, 37, 1, pp. 1-8, (2016); Buescher, Ariane V.S., Costs of autism spectrum disorders in the United Kingdom and the United States, JAMA Pediatrics, 168, 8, pp. 721-728, (2014); Nicholas, Joyce S., Prevalence and Characteristics of Children With Autism-Spectrum Disorders, Annals of Epidemiology, 18, 2, pp. 130-136, (2008); Robins, Diana L., Prevalence counts: Commentary on ""prevalence and characteristics of autism spectrum disorder among 4-year-old children in the autism and developmental disabilities monitoring network, Journal of Developmental and Behavioral Pediatrics, 37, 1, pp. 80-82, (2016); Fridenson-Hayo, Shimrit, Basic and complex emotion recognition in children with autism: Cross-cultural findings, Molecular Autism, 7, 1, (2016); Harms, Madeline B., Facial emotion recognition in autism spectrum disorders: A review of behavioral and neuroimaging studies, Neuropsychology Review, 20, 3, pp. 290-322, (2010); Ozonoff, Sally J., Are there Emotion Perception Deficits in Young Autistic Children?, Journal of Child Psychology and Psychiatry and Allied Disciplines, 31, 3, pp. 343-361, (1990); Daniels, Jena, Feasibility Testing of a Wearable Behavioral Aid for Social Learning in Children with Autism, Applied Clinical Informatics, 9, 1, pp. 129-140, (2018); Baron-Cohen, Simon B., Can emotion recognition be taught to children with autism spectrum conditions?, Philosophical Transactions of the Royal Society B: Biological Sciences, 364, 1535, pp. 3567-3574, (2009)","","Nature Publishing Group","","","","","","23986352","","","","English","Article","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85135599026"
"Espinosa-Aranda, J.L.; Vállez, N.; Rico-Saavedra, J.M.; Parra-Patino, J.; Bueno, G.; Sorci, M.; Moloney, D.; Peña, D.; Deniz, O.","Espinosa-Aranda, Jose Luis (55901889500); Vállez, Noelia (36172704800); Rico-Saavedra, Jose Maria (57194455871); Parra-Patino, Javier (56544727300); Bueno, M. G. (7003988757); Sorci, Matteo (23398702000); Moloney, David (7003753857); Peña, Dexmont (56581545800); Deniz, O. (8562422200)","55901889500; 36172704800; 57194455871; 56544727300; 7003988757; 23398702000; 7003753857; 56581545800; 8562422200","Smart doll: Emotion recognition using embedded deep learning","2018","Symmetry","10","9","387","","","0","12","10.3390/sym10090387","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054401573&doi=10.3390%2Fsym10090387&partnerID=40&md5=c9864ea17793a5c55f95f3cbd90438dc","Universidad de Castilla-La Mancha, Ciudad Real, Spain; Site EPFL, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; Collinstown Industrial Park, Leixlip House Hotel, Leixlip, Ireland","Espinosa-Aranda, Jose Luis, Universidad de Castilla-La Mancha, Ciudad Real, Spain; Vállez, Noelia, Universidad de Castilla-La Mancha, Ciudad Real, Spain; Rico-Saavedra, Jose Maria, Universidad de Castilla-La Mancha, Ciudad Real, Spain; Parra-Patino, Javier, Universidad de Castilla-La Mancha, Ciudad Real, Spain; Bueno, M. G., Universidad de Castilla-La Mancha, Ciudad Real, Spain; Sorci, Matteo, Site EPFL, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; Moloney, David, Collinstown Industrial Park, Leixlip House Hotel, Leixlip, Ireland; Peña, Dexmont, Collinstown Industrial Park, Leixlip House Hotel, Leixlip, Ireland; Deniz, O., Universidad de Castilla-La Mancha, Ciudad Real, Spain","Computer vision and deep learning are clearly demonstrating a capability to create engaging cognitive applications and services. However, these applications have been mostly confined to powerful Graphic Processing Units (GPUs) or the cloud due to their demanding computational requirements. Cloud processing has obvious bandwidth, energy consumption and privacy issues. The Eyes of Things (EoT) is a powerful and versatile embedded computer vision platform which allows the user to develop artificial vision and deep learning applications that analyse images locally. In this article, we use the deep learning capabilities of an EoT device for a real-life facial informatics application: a doll capable of recognizing emotions, using deep learning techniques, and acting accordingly. The main impact and significance of the presented application is in showing that a toy can now do advanced processing locally, without the need of further computation in the cloud, thus reducing latency and removing most of the ethical issues involved. Finally, the performance of the convolutional neural network developed for that purpose is studied and a pilot was conducted on a panel of 12 children aged between four and ten years old to test the doll. © 2018 Elsevier B.V., All rights reserved.","Computer vision; Deep learning; Facial informatics; Mobile applications; Real-time and embedded systems","","","","Funding: This work has been supported by the European Union’s Horizon 2020 Research and Innovation Programme under grant agreement No. 643924 [25].","Espinosa-Aranda, Jose Luis, Pulga, a tiny open-source MQTT broker for flexible and secure IoT deployments, pp. 690-694, (2015); Satyanarayanan, Mahadev, The case for VM-based cloudlets in mobile computing, IEEE Pervasive Computing, 8, 4, pp. 14-23, (2009); 2nd International Workshop on Computing and Networking for Internet of Things Comnet Iot Held in Conjunction with 14th International Conference on Distributed Computing and Networking Icdcn 2013, (2013); Deniz, O., Eyes of things, Sensors, 17, 5, (2017); Movidius Sup TM Sup Myriad Sup TM Sup Vpu 2 A Class Defining Processor; Barry, Brendan C., Always-on vision processing unit for mobile applications, IEEE Micro, 35, 2, pp. 56-66, (2015); Moloney, David, A Vision for the Future [Soapbox], IEEE Consumer Electronics Magazine, 4, 2, pp. 40-45, (2015); Zhang, Yu, Multi-kernel extreme learning machine for EEG classification in brain-computer interfaces, Expert Systems with Applications, 96, pp. 302-310, (2018); Jiao, Yong, A novel multilayer correlation maximization model for improving CCA-Based frequency recognition in ssvep brain-computer interface, International Journal of Neural Systems, 28, 4, (2018); Zhang, Yu, Sparse Bayesian Classification of EEG for Brain-Computer Interface, IEEE Transactions on Neural Networks and Learning Systems, 27, 11, pp. 2256-2267, (2016)","","MDPI AG indexing@mdpi.com Postfach Basel CH-4005","","","","","","20738994","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85054401573"
"Psaltis, A.; Apostolakis, K.C.; Dimitropoulos, K.; Daras, P.","Psaltis, Athanasios (57190073285); Apostolakis, Konstantinos Cornelis (55339270200); Dimitropoulos, Kosmas (22950021000); Daras, Petros (6602644373)","57190073285; 55339270200; 22950021000; 6602644373","Multimodal student engagement recognition in prosocial games","2018","IEEE Transactions on Games","10","3","8015151","292","303","0","49","10.1109/TCIAIG.2017.2743341","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064375011&doi=10.1109%2FTCIAIG.2017.2743341&partnerID=40&md5=b2a9c705fecc6184e54dbddff41efa7e","Centre for Research and Technology-Hellas, Thessaloniki, Greece","Psaltis, Athanasios, Centre for Research and Technology-Hellas, Thessaloniki, Greece; Apostolakis, Konstantinos Cornelis, Centre for Research and Technology-Hellas, Thessaloniki, Greece; Dimitropoulos, Kosmas, Centre for Research and Technology-Hellas, Thessaloniki, Greece; Daras, Petros, Centre for Research and Technology-Hellas, Thessaloniki, Greece","In this paper, we address the problem of recognizing student engagement in prosocial games by exploiting engagement cues from different input modalities. Since engagement is a multifaceted phenomenon with different dimensions, i.e., behavioral, cognitive, and affective, we propose the modeling of student engagement using real-time data from both the students and the game. More specifically, we apply body motion and facial expression analysis to identify the affective state of students, while we extract features related to their cognitive and behavioral engagement based on the analysis of their interaction with the game. For the automatic recognition of engagement, we adopt a machine learning approach based on artificial neural networks, while for the annotation of the engagement data, we introduce a novel approach based on the use of games with different degrees of challenge in conjunction with a retrospective self-reporting method. To evaluate the proposed methodology, we conducted real-life experiments in 4 classes, in 3 primary schools, with 72 students and 144 gameplay recordings in total. Experimental results show the great potential of the proposed methodology, which improves the classification accuracy of the three distinct dimensions with a detection rate of 85%. A detailed analysis of the role of each component of the Game Engagement Questionnaire, i.e., immersion, presence, flow, and absorption, in the classification process is also presented in this paper. © 2020 Elsevier B.V., All rights reserved.","Emotion recognition; Engagement recognition; Human–computer interaction; Serious games; Student engagement","Affective state; Automatic recognition; Classification accuracy; Classification process; Facial expression analysis; Input modalities; Machine learning approaches; Student engagement; Students","","","Manuscript received April 28, 2017; revised August 8, 2017; accepted August 16, 2017. Date of publication August 23, 2017; date of current version September 13, 2018. This work was supported by the EU Horizon 2020 Frame-work Programme under Grant 644204 (ProsocialLearn project). (Corresponding author: Athanasios Psaltis.) The authors are with the Information Technologies Institute, ITI - CERTH, Thessaloniki 57001, Greece (e-mail: at.psaltis@iti.gr; kapostol@iti.gr; dimitrop@iti.gr; daras@iti.gr).","Wiebe, Eric N., Measuring engagement in video game-based environments: Investigation of the User Engagement Scale, Computers in Human Behavior, 32, pp. 123-132, (2014); American Journal of Education, (1991); Student Engagement and Achievement in American Secondary Schools, (1992); Sullivan, Peter, Junior secondary students' perceptions of influences on their engagement with schooling, Australian Journal of Education, 53, 2, pp. 176-191, (2009); Fredricks, Jennifer A., School engagement: Potential of the concept, state of the evidence, Review of Educational Research, 74, 1, pp. 59-109, (2004); Taylor, Leah, Improving student engagement, Current Issues in Education, 14, 1, (2011); Reeve, Johnmarshall, Enhancing students' engagement by increasing teachers' autonomy support, Motivation and Emotion, 28, 2, pp. 147-169, (2004); Wellington Teaching and Learning Research Initiative, (2010); School Context Student Attitudes and Behavior and Academic Achievement an Exploratory Analysis, (2006); Journal of Education and Learning, (2012)","","Institute of Electrical and Electronics Engineers Inc.","","","","","","24751502; 24751510","","","","English","Article","Final","All Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85064375011"
"Abbas, H.; Garberson, F.; Glover, E.; Wall, D.P.","Abbas, Halim (57202307500); Garberson, Ford (57202300220); Glover, Eric (57202308543); Wall, Dennis Paul (7202196193)","57202307500; 57202300220; 57202308543; 7202196193","Machine learning approach for early detection of autism by combining questionnaire and home video screening","2018","Journal of the American Medical Informatics Association","25","8","","1000","1007","0","144","10.1093/jamia/ocy039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055023394&doi=10.1093%2Fjamia%2Focy039&partnerID=40&md5=5ce69d2ee9b7294a205e2ceaa283a11c","Cognoa Inc., Palo Alto, United States; Department of Pediatrics, Stanford University, Stanford, United States; Department of Biomedical Data Science, Stanford University, Stanford, United States","Abbas, Halim, Cognoa Inc., Palo Alto, United States; Garberson, Ford, Cognoa Inc., Palo Alto, United States; Glover, Eric, Cognoa Inc., Palo Alto, United States; Wall, Dennis Paul, Cognoa Inc., Palo Alto, United States, Department of Pediatrics, Stanford University, Stanford, United States, Department of Biomedical Data Science, Stanford University, Stanford, United States","Background: Existing screening tools for early detection of autism are expensive, cumbersome, time- intensive, and sometimes fall short in predictive value. In this work, we sought to apply Machine Learning (ML) to gold standard clinical data obtained across thousands of children at-risk for autism spectrum disorder to create a low-cost, quick, and easy to apply autism screening tool. Methods: Two algorithms are trained to identify autism, one based on short, structured parent-reported questionnaires and the other on tagging key behaviors from short, semi-structured home videos of children. A combination algorithm is then used to combine the results into a single assessment of higher accuracy. To overcome the scarcity, sparsity, and imbalance of training data, we apply novel feature selection, feature engineering, and feature encoding techniques. We allow for inconclusive determination where appropriate in order to boost screening accuracy when conclusive. The performance is then validated in a controlled clinical study. Results: A multi-center clinical study of n=162 children is performed to ascertain the performance of these algorithms and their combination. We demonstrate a significant accuracy improvement over standard screening tools in measurements of AUC, sensitivity, and specificity. Conclusion: These findings suggest that a mobile, machine learning process is a reliable method for detection of autism outside of clinical settings. A variety of confounding factors in the clinical analysis are discussed along with the solutions engineered into the algorithms. Final results are statistically limited and will benefit from future clinical studies to extend the sample size. © 2021 Elsevier B.V., All rights reserved.","Autism spectrum disorder; Diagnostic techniques and procedures; Mobile applications; Supervised machine learning","Article; autism; child; childhood disease; clinical examination; human; learning algorithm; major clinical study; medical informatics; mobile application; random forest; sensitivity and specificity; supervised machine learning; videorecording; algorithm; clinical trial; controlled clinical trial; controlled study; early diagnosis; machine learning; multicenter study; preschool child; procedures; questionnaire; receiver operating characteristic; Algorithms; Autistic Disorder; Child, Preschool; Early Diagnosis; Humans; Machine Learning; Methods; ROC Curve; Surveys and Questionnaires; Videotape Recording","","","","Durkin, Maureen S., Socioeconomic inequality in the prevalence of autism spectrum disorder: Evidence from a U.S. cross-sectional study, PLOS ONE, 5, 7, (2010); Christensen, Deborah L., Prevalence and characteristics of autism spectrum disorder among children aged 8 years - Autism and developmental disabilities monitoring network, 11 sites, United States, 2012, MMWR Surveillance Summaries, 65, 3, pp. 1-23, (2016); Zwaigenbaum, Lonnie, Clinical assessment and management of toddlers with suspected autism spectrum disorder: Insights from studies of high-risk infants, Pediatrics, 123, 5, pp. 1383-1391, (2009); Allely, Clare Sarah, Diagnosing autism spectrum disorders in primary care, Practitioner, 255, 1745, pp. 27-30, (2011); Manual for the Aseba Preschool Forms and Profiles, (2001); Lord, Catherine E., Autism Diagnostic Interview-Revised: A revised version of a diagnostic interview for caregivers of individuals with possible pervasive developmental disorders, Journal of Autism and Developmental Disorders, 24, 5, pp. 659-685, (1994); Lord, Catherine E., Austism diagnostic observation schedule: A standardized observation of communicative and social behavior, Journal of Autism and Developmental Disorders, 19, 2, pp. 185-212, (1989); Lord, Catherine E., A multisite study of the clinical diagnosis of different autism spectrum disorders, Archives of General Psychiatry, 69, 3, pp. 306-313, (2012); Wall, Dennis Paul, Use of artificial intelligence to shorten the behavioral diagnosis of autism, PLOS ONE, 7, 8, (2012); Duda, Marlena, Testing the accuracy of an observation-based classifier for rapid detection of autism risk, Translational Psychiatry, 4, (2011)","","Oxford University Press","","","","","","1527974X; 10675027","","JAMAF","29741630","English","Article","Final","All Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85055023394"
"Heraz, A.; Clynes, M.","Heraz, Alicia (24472834300); Clynes, Manfred (57225808045)","24472834300; 57225808045","Recognition of emotions conveyed by touch through force-sensitive screens: Observational study of humans and machine learning techniques","2018","JMIR Mental Health","5","8","e10104","","","0","18","10.2196/10104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052876305&doi=10.2196%2F10104&partnerID=40&md5=adb851a5f00b15d1f19ab0914c2f00f7","Institute of Amity and Emotion Research, Nyack, United States; The Brain Mining Lab, Montreal, Canada","Heraz, Alicia, The Brain Mining Lab, Montreal, Canada; Clynes, Manfred, Institute of Amity and Emotion Research, Nyack, United States","Background: Emotions affect our mental health: they influence our perception, alter our physical strength, and interfere with our reason. Emotions modulate our face, voice, and movements. When emotions are expressed through the voice or face, they are difficult to measure because cameras and microphones are not often used in real life in the same laboratory conditions where emotion detection algorithms perform well. With the increasing use of smartphones, the fact that we touch our phones, on average, thousands of times a day, and that emotions modulate our movements, we have an opportunity to explore emotional patterns in passive expressive touches and detect emotions, enabling us to empower smartphone apps with emotional intelligence. Objective: In this study, we asked 2 questions. (1) As emotions modulate our finger movements, will humans be able to recognize emotions by only looking at passive expressive touches? (2) Can we teach machines how to accurately recognize emotions from passive expressive touches? Methods: We were interested in 8 emotions: anger, awe, desire, fear, hate, grief, laughter, love (and no emotion). We conducted 2 experiments with 2 groups of participants: good imagers and emotionally aware participants formed group A, with the remainder forming group B. In the first experiment, we video recorded, for a few seconds, the expressive touches of group A, and we asked group B to guess the emotion of every expressive touch. In the second experiment, we trained group A to express every emotion on a force-sensitive smartphone. We then collected hundreds of thousands of their touches, and applied feature selection and machine learning techniques to detect emotions from the coordinates of participant' finger touches, amount of force, and skin area, all as functions of time. Results: We recruited 117 volunteers: 15 were good imagers and emotionally aware (group A); the other 102 participants formed group B. In the first experiment, group B was able to successfully recognize all emotions (and no emotion) with a high 83.8% (769/918) accuracy: 49.0% (50/102) of them were 100% (450/450) correct and 25.5% (26/102) were 77.8% (182/234) correct. In the second experiment, we achieved a high 91.11% (2110/2316) classification accuracy in detecting all emotions (and no emotion) from 9 spatiotemporal features of group A touches. Conclusions: Emotions modulate our touches on force-sensitive screens, and humans have a natural ability to recognize other people's emotions by watching prerecorded videos of their expressive touches. Machines can learn the same emotion recognition ability and do better than humans if they are allowed to continue learning on new data. It is possible to enable force-sensitive screens to recognize users' emotions and share this emotional insight with users, increasing users' emotional awareness and allowing researchers to design better technologies for well-being. © 2021 Elsevier B.V., All rights reserved.","Artificial intelligence; Emotional artificial intelligence; Emotional intelligence; Emotions; Force-sensitive screens; Human-computer interaction; Mental health; Positive computing; Smartphone","","","","","Sentics the Touch of Emotions, (1977); Ethics, (1996); Ardiel, Evan L., The importance of touch in development, Paediatrics and Child Health (Canada), 15, 3, pp. 153-156, (2010); Fairhurst, Merle T., Physiological and Behavioral Responses Reveal 9-Month-Old Infants' Sensitivity to Pleasant Touch, Psychological Science, 25, 5, pp. 1124-1131, (2014); Feldman, Ruth, Maternal-preterm skin-to-skin contact enhances child physiologic organization and cognitive control across the first 10 years of life, Biological Psychiatry, 75, 1, pp. 56-64, (2014); Touch, (2001); Field, Tiffany Martini, Touch for socioemotional and physical well-being: A review, Developmental Review, 30, 4, pp. 367-383, (2010); Hertenstein, Matthew J., Gender and the Communication of Emotion Via Touch, Sex Roles, 64, 1, pp. 70-80, (2011); Hertenstein, Matthew J., Touch communicates distinct emotions, Emotion, 6, 3, pp. 528-533, (2006); Positive Computing Technology for Wellbeing and Human Potential, (2014)","","","","","","","","23687959","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85052876305"
"Lind, M.N.; Byrne, M.L.; Wicks, G.; Smidt, A.M.; Allen, N.B.","Lind, Monika N. (57218595839); Byrne, Michelle L. (36629953700); Wicks, Geordie (57203763533); Smidt, Alec M. (57186752300); Allen, Nicholas B. (26034055700)","57218595839; 36629953700; 57203763533; 57186752300; 26034055700","The effortless assessment of risk states (EARS) Tool: An interpersonal approach to mobile sensing","2018","JMIR Mental Health","5","3","e10334","","","0","65","10.2196/10334","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068608187&doi=10.2196%2F10334&partnerID=40&md5=60f480e0d32f4e06a509e47bb5783a6b","Center for Digital Mental Health, University of Oregon, Eugene, United States","Lind, Monika N., Center for Digital Mental Health, University of Oregon, Eugene, United States; Byrne, Michelle L., Center for Digital Mental Health, University of Oregon, Eugene, United States; Wicks, Geordie, Center for Digital Mental Health, University of Oregon, Eugene, United States; Smidt, Alec M., Center for Digital Mental Health, University of Oregon, Eugene, United States; Allen, Nicholas B., Center for Digital Mental Health, University of Oregon, Eugene, United States","Background: To predict and prevent mental health crises, we must develop new approaches that can provide a dramatic advance in the effectiveness, timeliness, and scalability of our interventions. However, current methods of predicting mental health crises (eg, clinical monitoring, screening) usually fail on most, if not all, of these criteria. Luckily for us, 77% of Americans carry with them an unprecedented opportunity to detect risk states and provide precise life-saving interventions. Smartphones present an opportunity to empower individuals to leverage the data they generate through their normal phone use to predict and prevent mental health crises. Objective: To facilitate the collection of high-quality, passive mobile sensing data, we built the Effortless Assessment of Risk States (EARS) tool to enable the generation of predictive machine learning algorithms to solve previously intractable problems and identify risk states before they become crises. Methods: The EARS tool captures multiple indices of a person's social and affective behavior via their naturalistic use of a smartphone. Although other mobile data collection tools exist, the EARS tool places a unique emphasis on capturing the content as well as the form of social communication on the phone. Signals collected include facial expressions, acoustic vocal quality, natural language use, physical activity, music choice, and geographical location. Critically, the EARS tool collects these data passively, with almost no burden on the user. We programmed the EARS tool in Java for the Android mobile platform. In building the EARS tool, we concentrated on two main considerations: (1) privacy and encryption and (2) phone use impact. Results: In a pilot study (N=24), participants tolerated the EARS tool well, reporting minimal burden. None of the participants who completed the study reported needing to use the provided battery packs. Current testing on a range of phones indicated that the tool consumed approximately 15% of the battery over a 16-hour period. Installation of the EARS tool caused minimal change in the user interface and user experience. Once installation is completed, the only difference the user notices is the custom keyboard. Conclusions: The EARS tool offers an innovative approach to passive mobile sensing by emphasizing the centrality of a person's social life to their well-being. We built the EARS tool to power cutting-edge research, with the ultimate goal of leveraging individual big data to empower people and enhance mental health. © 2021 Elsevier B.V., All rights reserved.","Cell phone; Crisis prevention; Depression; Individual big data; Mental health; Mobile apps; Mobile sensing; Passive mobile sensing; Personal sensing; Risk assessment; Telemedicine","","","","","Kessler, Ronald C., The Epidemiology of Major Depressive Disorder: Results from the National Comorbidity Survey Replication (NCS-R), JAMA, 289, 23, pp. 3095-3105, (2003); Mojtabai, Ramin, Trends in psychological distress, depressive episodes and mental health treatment-seeking in the United States: 2001-2012, Journal of Affective Disorders, 174, pp. 556-561, (2015); Reavley, N. J., Mental health reform: Increased resources but limited gains, Medical Journal of Australia, 201, 7, pp. 375-376, (2014); Murray, Christopher J.L., Disability-adjusted life years (DALYs) for 291 diseases and injuries in 21 regions, 1990-2010: A systematic analysis for the Global Burden of Disease Study 2010, The Lancet, 380, 9859, pp. 2197-2223, (2012); World Health Statistics 2016 Monitoring Health for the Sdgs, (2016); Curtin, Sally C., QuickStats: Age-adjusted rate* for suicide,† by sex — National vital statistics system, United States, 1975–2015, Morbidity and Mortality Weekly Report, 66, 10, (2017); Yip, Paul Siu Fai, Means restriction for suicide prevention, The Lancet, 379, 9834, pp. 2393-2399, (2012); Johnson, Eric J., Do Defaults Save Lives?, Science, 302, 5649, pp. 1338-1339, (2003); Nahum-Shani, Inbal, Just-in-time adaptive interventions (JITAIs) in mobile health: Key components and design principles for ongoing health behavior support, Annals of Behavioral Medicine, 52, 6, pp. 446-462, (2018); Thornicroft, Graham, Undertreatment of people with major depressive disorder in 21 countries, British Journal of Psychiatry, 210, 2, pp. 119-124, (2017)","","JMIR Publications Inc.","","","","","","23687959","","","","English","Review","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85068608187"
"Lee, T.K.; Baddar, W.J.; Kim, S.T.; Ro, Y.M.","Lee, Tae-kwan (57200645739); Baddar, Wissam J. (56347927600); Kim, Seong Tae (56333626800); Ro, Yong Man (7102329309)","57200645739; 56347927600; 56333626800; 7102329309","Convolution with Logarithmic Filter Groups for Efficient Shallow CNN","2018","Lecture Notes in Computer Science","10704 LNCS","","","117","129","0","10","10.1007/978-3-319-73603-7_10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042098653&doi=10.1007%2F978-3-319-73603-7_10&partnerID=40&md5=3a31088ee2f4b4c46e62b6c0a4624ca5","Image and Video Systems Lab., Korea Advanced Institute of Science and Technology, Daejeon, South Korea","Lee, Tae-kwan, Image and Video Systems Lab., Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Baddar, Wissam J., Image and Video Systems Lab., Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Kim, Seong Tae, Image and Video Systems Lab., Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Ro, Yong Man, Image and Video Systems Lab., Korea Advanced Institute of Science and Technology, Daejeon, South Korea","In convolutional neural networks (CNNs), the filter grouping in convolution layers is known to be useful to reduce the network parameter size. In this paper, we propose a new logarithmic filter grouping which can capture the nonlinearity of filter distribution in CNNs. The proposed logarithmic filter grouping is installed in shallow CNNs applicable in a mobile application. Experiments were performed with the shallow CNNs for classification tasks. Our classification results on Multi-PIE dataset for facial expression recognition and CIFAR-10 dataset for object classification reveal that the compact CNN with the proposed logarithmic filter grouping scheme outperforms the same network with the uniform filter grouping in terms of accuracy and parameter efficiency. Our results indicate that the efficiency of shallow CNNs can be improved by the proposed logarithmic filter grouping. © 2018 Elsevier B.V., All rights reserved.","Classification tasks; CNN parameter efficiency; Nonlinear logarithmic filter groups; Shallow convolutional neural network (CNN)","Bandpass filters; Convolution; Efficiency; Neural networks; Classification results; Classification tasks; Convolutional neural network; Facial expression recognition; Logarithmic filters; Mobile applications; Network parameters; Object classification; Classification (of information)","","","Acknowledgment. This work was supported by Institute for Information & communications Technology Promotion(IITP) grant funded by the Korea government(MSIT) (No.2017-0-00111, Practical technology development of high performing emotion recognition and facial expression based authentication using deep network).","He, Kaiming, Deep residual learning for image recognition, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2016-December, pp. 770-778, (2016); Taigman, Yaniv, DeepFace: Closing the gap to human-level performance in face verification, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 1701-1708, (2014); Szegedy, Christian, Going deeper with convolutions, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 07-12-June-2015, pp. 1-9, (2015); Szegedy, Christian, Inception-v4, inception-ResNet and the impact of residual connections on learning, pp. 4278-4284, (2017); Baddar, Wissam J., Learning features robust to image variations with siamese networks for facial expression recognition, Lecture Notes in Computer Science, 10132 LNCS, pp. 189-200, (2017); IEEE Transactions on Affective Computing, (2017); Mobilenets Efficient Convolutional Neural Networks for Mobile Vision Applications, (2017); Ba, Jimmy Lei, Do deep nets really need to be deep?, Advances in Neural Information Processing Systems, 3, January, pp. 2654-2662, (2014); McDonnell, Mark D., Enhanced image classification with a fast-learning shallow convolutional neural network, Proceedings of the International Joint Conference on Neural Networks, 2015-September, (2015); Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications, (2015)","Schoeffmann, K.; Gabbouj, M.; O'Connor, N.E.; Elgammal, A.; Chalidabhongse, T.H.; Aramvith, S.; Ngo, C.W.; Ho, Y.-S.","Springer Verlag service@springer.de","Natawut Nupairoj; Widhayakorn Asdornwised","24th International Conference on MultiMedia Modeling, MMM 2018","","Bangkok","210629","16113349; 03029743","9789819698936; 9789819698042; 9789819698110; 9789819698905; 9789819512324; 9783032026019; 9783032008909; 9783031915802; 9789819698141; 9783031984136","","","English","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85042098653"
"Alshamsi, H.; Këpuska, V.; Meng, H.","Alshamsi, Humaid (57194263837); Këpuska, Veton Z. (56613984100); Meng, Hongying (57200611749)","57194263837; 56613984100; 57200611749","Real time automated facial expression recognition app development on smart phones","2017","","","","8117150","384","392","0","33","10.1109/IEMCON.2017.8117150","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045846866&doi=10.1109%2FIEMCON.2017.8117150&partnerID=40&md5=34bef7efea73f26041691525a7cf3e87","College of Engineering and Science, Melbourne, United States; Department of Electrical & Computer Engineering, Brunel University London, Uxbridge, United Kingdom","Alshamsi, Humaid, College of Engineering and Science, Melbourne, United States; Këpuska, Veton Z., College of Engineering and Science, Melbourne, United States; Meng, Hongying, Department of Electrical & Computer Engineering, Brunel University London, Uxbridge, United Kingdom","Automated facial expression recognition (AFER) is a crucial technology to and a challenging task for human computer interaction. Previous methods of AFER have incorporated different features and classification methods and use basic testing approaches. In this paper, we employ the best feature descriptor for AFER by empirically evaluating the feature descriptors named the Facial Landmarks descriptor and the Center of Gravity descriptor. We examine each feature descriptor by considering one classification method, such as the Support Vector Machine (SVM) method, with three unique facial expression recognition (FER) datasets. In addition to test accuracies, we present confusion matrices of AFER. We also analyze the effect of using these feature and image resolutions on AFER performance. Our study indicates that the Facial Landmarks descriptor is the best choice to run AFER on mobile phones. The results of our study demonstrate that the proposed facial expression recognition on a mobile phone application is successful and provides up to 96.3% recognition accuracy. © 2018 Elsevier B.V., All rights reserved.","Automated Facial Expression Recognition; Facial Landmarks; Machine Learning; Mobile Computing; Support Vector Machine","Automation; Cellular telephones; Classification (of information); Human computer interaction; Image resolution; Learning systems; Mobile computing; Mobile phones; Mobile telecommunication systems; Smartphones; Support vector machines; Telephone sets; Classification methods; Confusion matrices; Crucial technology; Facial expression recognition; Facial landmark; Feature descriptors; Mobile phone applications; Recognition accuracy; Face recognition","","","","Pantic, Maja, Human computing and machine understanding of human behavior: A survey, Lecture Notes in Computer Science, 4451 LNAI, pp. 47-71, (2007); Handbook of Face Recognition, (2005); Ekman, Paul, Strong evidence for universais in facial expressions: A reply to Russell's mistaken critique, Psychological Bulletin, 115, 2, pp. 268-287, (1994); Black, Michael Julian, Recognizing Facial Expressions in Image Sequences Using Local Parameterized Models of Image Motion, International Journal of Computer Vision, 25, 1, pp. 23-48, (1997); Chang, Ya, Manifold based analysis of facial expression, Image and Vision Computing, 24, 6, pp. 605-614, (2006); Valstar, Michel F., Fully automatic facial action unit detection and temporal analysis, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2006, (2006); Asian Conf Computer Vision, (2007); Bartlett, Marian Stewart, Fully automatic facial action recognition in spontaneous behavior, 2006, pp. 223-230, (2006); De-La-Torre, Fernando, Temporal segmentation of facial behavior, Proceedings of the IEEE International Conference on Computer Vision, (2007); Zeng, Zhihong, Spontaneous emotional facial expression detection, Journal of Multimedia, 1, 5, pp. 1-8, (2006)","Saha, N.S.; Chakrabarti, S.","Institute of Electrical and Electronics Engineers Inc.","IEEE Vancouver Section; Institute of Engineering and Management (IEM); University of Engineering and Management (UEM)","8th IEEE Annual Information Technology, Electronics and Mobile Communication Conference, IEMCON 2017","","Vancouver; BC; University of British Columbia","132665","","9781538633717","","","English","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85045846866"
"Beltrán-Prieto, L.A.; Komínková Oplatková, Z.","Beltrán-Prieto, Luis Antonio (56695408700); Komínková Oplatková, Zuzana Komínková (15043128400)","56695408700; 15043128400","A performance comparison of two emotion-recognition implementations using OpenCV and Cognitive Services API","2017","MATEC Web of Conferences","125","","02067","","","0","5","10.1051/matecconf/201712502067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032874818&doi=10.1051%2Fmatecconf%2F201712502067&partnerID=40&md5=d5e157a0835d0a4f662f2996f81ee802","Department of Informatics and Artificial Intelligence, Tomas Bata University in Zlin, Zlin, Czech Republic","Beltrán-Prieto, Luis Antonio, Department of Informatics and Artificial Intelligence, Tomas Bata University in Zlin, Zlin, Czech Republic; Komínková Oplatková, Zuzana Komínková, Department of Informatics and Artificial Intelligence, Tomas Bata University in Zlin, Zlin, Czech Republic","Emotions represent feelings about people in several situations. Various machine learning algorithms have been developed for emotion detection in a multimedia element, such as an image or a video. These techniques can be measured by comparing their accuracy with a given dataset in order to determine which algorithm can be selected among others. This paper deals with the comparison of two implementations of emotion recognition in faces, each implemented with specific technology. OpenCV is an open-source library of functions and packages mostly used for computer-vision analysis and applications. Cognitive services is a set of APIs containing artificial intelligence algorithms for computer-vision, speech, knowledge, and language processing. Two Android mobile applications were developed in order to test the performance between an OpenCV algorithm for emotion recognition and an implementation of Emotion cognitive service. For this research, one thousand tests were carried out per experiment. Our findings show that the OpenCV implementation got a better performance than the Cognitive services application. In both cases, performance can be improved by increasing the sample size per emotion during the training step. © 2017 Elsevier B.V., All rights reserved.","","Artificial intelligence; Computer circuits; Learning algorithms; Learning systems; Speech recognition; Artificial intelligence algorithms; Emotion recognition; Language processing; Mobile applications; Multimedia elements; Open-source libraries; Performance comparison; Services applications; Computer vision","","","This work was supported by the Ministry of Education, Youth and Sports of the Czech Republic within the National Sustainability Programme project No. LO1303 (MSMT-7778/2014) and also by the European Regional Development Fund under the project CEBIA-Tech No. CZ.1.05/2.1.00/03.0089, further it was supported by Grant Agency of the Czech Republic—GACR 588 P103/15/06700S and by Internal Grant Agency of Tomas Bata University in Zlin under the project No. IGA/CebiaTech/2017/004. L.A.B.P author also thanks the doctoral scholarship provided by the National Council for Science and Technology (CONACYT) and the Council for Science and Technology of the State of Guanajuato (CONCYTEG) in Mexico.","Garn, C. Alex, Predicting changes in student engagement in university physical education: Application of control-value theory of achievement emotions, Psychology of Sport and Exercise, 29, pp. 93-102, (2017); Fernández-Caballero, Antonio, Smart environment architecture for emotion detection and regulation, Journal of Biomedical Informatics, 64, pp. 55-73, (2016); Felbermayr, Armin, The Role of Emotions for the Perceived Usefulness in Online Customer Reviews, Journal of Interactive Marketing, 36, pp. 60-76, (2016); Gennari, Rosella, Children's emotions and quality of products in participatory game design, International Journal of Human Computer Studies, 101, pp. 45-61, (2017); Expert Systems with Applications, (2016); Matos, Francisco A., Deep learning for plasma tomography using the bolometer system at JET, Fusion Engineering and Design, 114, pp. 18-25, (2017); Liu, Hao, Group-aware deep feature learning for facial age estimation, Pattern Recognition, 66, pp. 82-94, (2017); Vieira, Sandra, Using deep learning to investigate the neuroimaging correlates of psychiatric and neurological disorders: Methods and applications, Neuroscience and Biobehavioral Reviews, 74, pp. 58-75, (2017); Proc Brit Mach Vis Conf, (2015); Appl Soft Comput, (2017)","Mladenov, V.; Mastorakis, N.; Bulucea, A.","EDP Sciences edps@edpsciences.com","","21st International Conference on Circuits, Systems, Communications and Computers, CSCC 2017","","Heraklion, Crete; Agia Pelagia Beach","130915","2261236X; 22747214","9782759812745; 9782759816590; 9782759890125; 9782759890224; 9782759811007; 9782759890323; 9782759810741; 9782759816606; 9782759890521; 9782759810802","","","English","Conference paper","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85032874818"
"Schwan, J.; Ghaleb, E.; Hortal, E.; Asteriadis, S.","Schwan, Justus (57196025310); Ghaleb, Esam (56246908200); Hortal, Enrique (55834426400); Asteriadis, Stylianos (55936774500)","57196025310; 56246908200; 55834426400; 55936774500","High-performance and lightweight real-time deep face emotion recognition","2017","","","","8022671","76","79","0","11","10.1109/SMAP.2017.8022671","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030975090&doi=10.1109%2FSMAP.2017.8022671&partnerID=40&md5=f672551e1a5a700e863578acc6a91b90","Department of Data Science and Knowledge Engineering, Universiteit Maastricht, Maastricht, Netherlands","Schwan, Justus, Department of Data Science and Knowledge Engineering, Universiteit Maastricht, Maastricht, Netherlands; Ghaleb, Esam, Department of Data Science and Knowledge Engineering, Universiteit Maastricht, Maastricht, Netherlands; Hortal, Enrique, Department of Data Science and Knowledge Engineering, Universiteit Maastricht, Maastricht, Netherlands; Asteriadis, Stylianos, Department of Data Science and Knowledge Engineering, Universiteit Maastricht, Maastricht, Netherlands","Deep learning is used for all kinds of tasks which require human-like performance, such as voice and image recognition in smartphones, smart home technology, and self-driving cars. While great advances have been made in the field, results are often not satisfactory when compared to human performance. In the field of facial emotion recognition, especially in the wild, Convolutional Neural Networks (CNN) are employed because of their excellent generalization properties. However, while CNNs can learn a representation for certain object classes, an amount of (annotated) training data roughly proportional to the class's complexity is needed and seldom available. This work describes an advanced pre-processing algorithm for facial images and a transfer learning mechanism, two potential candidates for relaxing this requirement. Using these algorithms, a lightweight face emotion recognition application for Human-Computer Interaction with TurtleBot units was developed. © 2017 Elsevier B.V., All rights reserved.","","Automation; Human computer interaction; Image recognition; Intelligent buildings; Neural networks; Semantics; Social networking (online); Speech recognition; Convolutional neural network; Face emotion recognition; Facial emotions; Generalization properties; Human performance; Pre-processing algorithms; Smart Home Technology; Transfer learning; Face recognition","","","This work was supported by the Horizon 2020 funded project MaTHiSiS (Managing Affective-learning THrough Intelligent atoms and Smart InteractionS) nr. 687772 (http://www.mathisis-project.eu/). We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Nvidia GeForce GTX TITAN X GPU used for this research.","Schmidhuber, Jürgen U., Deep Learning in neural networks: An overview, Neural Networks, 61, pp. 85-117, (2015); Proceedings of the British Machine Vision, (2015); Goodfellow, Ian J., Challenges in representation learning: A report on three machine learning contests, Lecture Notes in Computer Science, 8228 LNCS, PART 3, pp. 117-124, (2013); Corr, (2013); Dynamic Programming, (1957); Dhall, Abhinav, Collecting large, richly annotated facial-expression databases from movies, IEEE Multimedia, 19, 3, pp. 34-41, (2012); Kanade, Takeo, Comprehensive database for facial expression analysis, pp. 46-53, (2000); Workshop on Cvpr, (2010); Facial Emotion Recognition in Real Time, (2016); O Reilly Media Incorporated, (2008)","Bielikova, M.; Simko, M.","Institute of Electrical and Electronics Engineers Inc.","","12th International Workshop on Semantic and Social Media Adaptation and Personalization, SMAP 2017","","Bratislava","130300","","9781538607565","","","English","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85030975090"
"Monteith, S.; Glenn, T.; Geddes, J.; Whybrow, P.C.; Bauer, M.","Monteith, Scott (56227233500); Glenn, Tasha (7005672247); Geddes, John Richard (56883972800); Whybrow, Peter C. (7006776377); Bauer, Michael (56431978500)","56227233500; 7005672247; 56883972800; 7006776377; 56431978500","Big data for bipolar disorder","2016","International Journal of Bipolar Disorders","4","1","10","","","0","36","10.1186/s40345-016-0051-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992657975&doi=10.1186%2Fs40345-016-0051-7&partnerID=40&md5=00c1d7ca60f7fc4f778f5d6b6d081553","MSU College of Human Medicine, Grand Rapids, United States; ChronoRecord Association, Inc., Fullerton, United States; Warneford Hospital, Oxford, United Kingdom; Jane & Terry Semel Institute for Neuroscience & Human Behavior, Los Angeles, United States; Department of Psychiatry and Psychotherapy, Universitätsklinikum Carl Gustav Carus Dresden, Dresden, Germany","Monteith, Scott, MSU College of Human Medicine, Grand Rapids, United States; Glenn, Tasha, ChronoRecord Association, Inc., Fullerton, United States; Geddes, John Richard, Warneford Hospital, Oxford, United Kingdom; Whybrow, Peter C., Jane & Terry Semel Institute for Neuroscience & Human Behavior, Los Angeles, United States; Bauer, Michael, Department of Psychiatry and Psychotherapy, Universitätsklinikum Carl Gustav Carus Dresden, Dresden, Germany","The delivery of psychiatric care is changing with a new emphasis on integrated care, preventative measures, population health, and the biological basis of disease. Fundamental to this transformation are big data and advances in the ability to analyze these data. The impact of big data on the routine treatment of bipolar disorder today and in the near future is discussed, with examples that relate to health policy, the discovery of new associations, and the study of rare events. The primary sources of big data today are electronic medical records (EMR), claims, and registry data from providers and payers. In the near future, data created by patients from active monitoring, passive monitoring of Internet and smartphone activities, and from sensors may be integrated with the EMR. Diverse data sources from outside of medicine, such as government financial data, will be linked for research. Over the long term, genetic and imaging data will be integrated with the EMR, and there will be more emphasis on predictive models. Many technical challenges remain when analyzing big data that relates to size, heterogeneity, complexity, and unstructured text data in the EMR. Human judgement and subject matter expertise are critical parts of big data analysis, and the active participation of psychiatrists is needed throughout the analytical process. © 2018 Elsevier B.V., All rights reserved.","Big data; Bipolar disorder; Claims; EMR; Patient monitoring; Registries","C reactive protein; lithium; neuroleptic agent; artificial intelligence; autism; bipolar disorder; bipolar I disorder; bipolar II disorder; confidentiality; electronic medical record; epilepsy; genetic heterogeneity; health care policy; human; Internet; life expectancy; lung embolism; major depression; physical disease; pneumonia; prevalence; priority journal; psychiatrist; randomized controlled trial (topic); Review; smartphone; urinary tract cancer","","C reactive protein, 9007-41-4; lithium, 7439-93-2","","Abrams, Thad E., Variations in the associations between psychiatric comorbidity and hospital mortality according to the method of identifying psychiatric diagnoses, Journal of General Internal Medicine, 23, 3, pp. 317-322, (2008); Aiff, Harald, Effects of 10 to 30 years of lithium treatment on kidney function, Journal of Psychopharmacology, 29, 5, pp. 608-614, (2015); Allebeck, Peter, The use of population based registers in psychiatric research, Acta Psychiatrica Scandinavica, 120, 5, pp. 386-391, (2009); Alvarez-Lozano, Jorge, Tell me your apps and i will tell you your mood: Correlation of apps usage with Bipolar Disorder State, ACM International Conference Proceeding Series, 2014-May, (2014); undefined; Bagalman, Erin, Health Care Resource Utilization and Costs in a Commercially Insured Population of Patients With Bipolar Disorder Type I and Frequent Psychiatric Interventions, Clinical Therapeutics, 33, 10, pp. 1381-e4, (2011); Baldessarini, Ross John, Patterns of psychotropic drug prescription for U.S. patients with diagnoses of bipolar disorders, Psychiatric Services, 58, 1, pp. 85-91, (2007); Banaee, Hadi, Data mining for wearable sensors in health monitoring systems: A review of recent trends and challenges, Sensors, 13, 12, pp. 17472-17500, (2013); Bauer, Mark S., Independent Assessment of Manic and Depressive Symptoms by Self-rating: Scale Characteristics and Implications for the Study of Mania, Archives of General Psychiatry, 48, 9, pp. 807-812, (1991); Bauer, Michael, Using technology to improve longitudinal studies: Self-reporting with Chrono Record in bipolar disorder, Bipolar Disorders, 6, 1, pp. 67-74, (2004)","","SpringerOpen","","","","","","21947511","","","","English","Review","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-84992657975"
"Dai, D.; Liu, Q.; Meng, H.","Dai, Daxiang (57192117377); Liu, Qun (59282438400); Meng, Hongying (57200611749)","57192117377; 59282438400; 57200611749","Can your smartphone detect your emotion?","2016","","","","7603434","1704","1709","0","13","10.1109/FSKD.2016.7603434","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997795687&doi=10.1109%2FFSKD.2016.7603434&partnerID=40&md5=f70bad9ecac225ac806ba8e3eb54be0c","Chongqing Key Laboratory of Computational Intelligence, Chongqing, China; Department of Electrical & Computer Engineering, Brunel University London, Uxbridge, United Kingdom","Dai, Daxiang, Chongqing Key Laboratory of Computational Intelligence, Chongqing, China; Liu, Qun, Chongqing Key Laboratory of Computational Intelligence, Chongqing, China; Meng, Hongying, Department of Electrical & Computer Engineering, Brunel University London, Uxbridge, United Kingdom","The smartphone has become an indispensable part in people's life. Identifying the user's emotional state according to the usage of smartphone is a new way to improve the human-computer interaction and user experience. In this paper, we present an attempt to recognize emotional states by using finger-stroke pattern. Firstly, International Affective Picture System (IAPS) were used to design the emotion inducing experiment. Then finger-stroke features under different emotional categories were extracted and analyzed. Ultimately, we used machine learning algorithms to identify three basic emotional states including positive, neutral, and negative. The experiment results show that the stroke exists some specific behavioral patterns between different people. For all 24 subjects, the average classification accuracy rate reached 85.1%, and we got the average recognition accuracy at 72.3%, 74.6% and 69.6% for male, female and all subjects. © 2017 Elsevier B.V., All rights reserved.","Emotion recognition; Human-computer interaction; Machine learning; Smartphone","Artificial intelligence; Fuzzy systems; Learning algorithms; Learning systems; Signal encoding; Smartphones; Average classification accuracy rates; Behavioral patterns; Emotion recognition; Emotional state; Picture system; Recognition accuracy; Stroke features; User experience; Human computer interaction","","","","Hertenstein, Matthew J., Touch communicates distinct emotions, Emotion, 6, 3, pp. 528-533, (2006); Hertenstein, Matthew J., The Communication of Emotion via Touch, Emotion, 9, 4, pp. 566-573, (2009); Thompson, Erin Hope, The effect of relationship status on communicating emotions through touch, Cognition and Emotion, 25, 2, pp. 295-306, (2011); Yohanan, Steve, The Role of Affective Touch in Human-Robot Interaction: Human Intent and Expectations in Touching the Haptic Creature, International Journal of Social Robotics, 4, 2, pp. 163-180, (2012); Flagg, Anna, Affective touch gesture recognition for a furry zoomorphic machine, pp. 25-32, (2013); Lin, Zheng, Make it possible: Multilingual sentiment analysis without much prior knowledge, 2, pp. 79-86, (2014); Lin, Zheng, Language-independent sentiment classification using three common words, International Conference on Information and Knowledge Management, Proceedings, pp. 1041-1045, (2011); Liu, Shenghua, Co-training and visualizing sentiment evolvement for tweet events, pp. 105-106, (2013); Liu, Shenghua, Adaptive co-training SVM for sentiment classification on tweets, International Conference on Information and Knowledge Management, Proceedings, pp. 2079-2088, (2013); Jang, Eun Hye, Emotion classification based on bio-signals emotion recognition using machine learning algorithms, 3, pp. 1373-1376, (2014)","Du, J.; Liu, C.; Li, K.; Wang, L.; Tong, Z.; Li, M.; Xiong, N.","Institute of Electrical and Electronics Engineers Inc.","","12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery, ICNC-FSKD 2016","","Changsha","124487","","9781509040933","","","English","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-84997795687"
"Alshamsi, H.; Meng, H.; Li, M.","Alshamsi, Humaid (57194263837); Meng, Hongying (57200611749); Li, Maozhen (7405263968)","57194263837; 57200611749; 7405263968","Real time facial expression recognition app development on mobile phones","2016","","","","7603442","1750","1755","0","24","10.1109/FSKD.2016.7603442","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997682853&doi=10.1109%2FFSKD.2016.7603442&partnerID=40&md5=e319c75e2afed323edc9459ae828f4b1","Electronic and Computer Engineering, Brunel University London, Uxbridge, United Kingdom; Key Laboratory of Embedded System and Service Computing, Tongji University, Shanghai, China","Alshamsi, Humaid, Electronic and Computer Engineering, Brunel University London, Uxbridge, United Kingdom; Meng, Hongying, Electronic and Computer Engineering, Brunel University London, Uxbridge, United Kingdom; Li, Maozhen, Electronic and Computer Engineering, Brunel University London, Uxbridge, United Kingdom, Key Laboratory of Embedded System and Service Computing, Tongji University, Shanghai, China","Facial expression has made significant progress in recent years with many commercial systems are available for real-world applications. It gains strong interest to implement a facial expression system on a portable device such as tablet and smart phone device using the camera already integrated in the devices. It is very common to see face recognition phone unlocking app in new smart phones which are proven to be hassle free way to unlock a phone. Implementation a facial expression system in a smart phone would provide fun applications that can be used to measure the mood of the user in their daily life or as a tool for their daily monitoring of the motion in phycology studies. However, traditional facial expression algorithms are normally computing extensive and can only be implemented offline at a computer. In this paper, a novel automatic system has been proposed to recognize emotions from face images on a smart phone in real-time. In our system, the camera of the smart phone is used to capture the face image, BRIEF features are extracted and k-nearest neighbor algorithm is implemented for the classification. The experimental results demonstrate that the proposed facial expression recognition on mobile phone is successful and it gives up to 89.5% recognition accuracy. © 2017 Elsevier B.V., All rights reserved.","facial expression; image processing; machine learning; mobile computing","Artificial intelligence; Cameras; Cellular telephones; Fuzzy systems; Human computer interaction; Image processing; Learning algorithms; Learning systems; Mobile computing; Mobile phones; Nearest neighbor search; Pattern recognition; Smartphones; Telephone sets; Automatic systems; Commercial systems; Face images; Facial expression recognition; Facial Expressions; K nearest neighbor algorithm; Portable device; Recognition accuracy; Face recognition","","","","Pantic, Maja, Human computing and machine understanding of human behavior: A survey, Lecture Notes in Computer Science, 4451 LNAI, pp. 47-71, (2007); Handbook of Face Recognition, (2005); Ekman, Paul, Strong evidence for universais in facial expressions: A reply to Russell's mistaken critique, Psychological Bulletin, 115, 2, pp. 268-287, (1994); Black, Michael Julian, Recognizing Facial Expressions in Image Sequences Using Local Parameterized Models of Image Motion, International Journal of Computer Vision, 25, 1, pp. 23-48, (1997); Chang, Ya, Manifold based analysis of facial expression, Image and Vision Computing, 24, 6, pp. 605-614, (2006); Valstar, Michel F., Fully automatic facial action unit detection and temporal analysis, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2006, (2006); Asian Conf Computer Vision, (2007); Bartlett, Marian Stewart, Fully automatic facial action recognition in spontaneous behavior, 2006, pp. 223-230, (2006); De-La-Torre, Fernando, Temporal segmentation of facial behavior, Proceedings of the IEEE International Conference on Computer Vision, (2007); Zeng, Zhihong, Spontaneous emotional facial expression detection, Journal of Multimedia, 1, 5, pp. 1-8, (2006)","Du, J.; Liu, C.; Li, K.; Wang, L.; Tong, Z.; Li, M.; Xiong, N.","Institute of Electrical and Electronics Engineers Inc.","","12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery, ICNC-FSKD 2016","","Changsha","124487","","9781509040933","","","English","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-84997682853"
"Pohjalainen, J.; Ringeval, F.; Zhang, Z.; Schuller, B.W.","Pohjalainen, Jouni (6505777912); Ringeval, Fabien (25960335900); Zhang, Zixing (24282033300); Schuller, Björn W. (6603767415)","6505777912; 25960335900; 24282033300; 6603767415","Spectral and cepstral audio noise reduction techniques in speech emotion recognition","2016","","","","","670","674","0","39","10.1145/2964284.2967306","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994652671&doi=10.1145%2F2964284.2967306&partnerID=40&md5=37e5d48bcc736baa83998af60be2066e","Department of Complex and Intelligent Systems, Universität Passau, Passau, Germany; Université Grenoble Alpes, Saint Martin d'Heres, France; Department of Computing, Imperial College London, London, United Kingdom","Pohjalainen, Jouni, Department of Complex and Intelligent Systems, Universität Passau, Passau, Germany; Ringeval, Fabien, Department of Complex and Intelligent Systems, Universität Passau, Passau, Germany, Université Grenoble Alpes, Saint Martin d'Heres, France; Zhang, Zixing, Department of Complex and Intelligent Systems, Universität Passau, Passau, Germany; Schuller, Björn W., Department of Complex and Intelligent Systems, Universität Passau, Passau, Germany, Department of Computing, Imperial College London, London, United Kingdom","Signal noise reduction can improve the performance of machine learning systems dealing with time signals such as audio. Real-life applicability of these recognition technologies requires the system to uphold its performance level in variable, challenging conditions such as noisy environments. In this contribution, we investigate audio signal denoising methods in cepstral and log-spectral domains and compare them with common implementations of standard techniques. The different approaches are first compared generally using averaged acoustic distance metrics. They are then applied to automatic recognition of spontaneous and natural emotions under simulated smartphone-recorded noisy conditions. Emotion recognition is implemented as support vector regression for continuous-valued prediction of arousal and valence on a realistic multimodal database. In the experiments, the proposed methods are found to generally outperform standard noise reduction algorithms. © 2016 Elsevier B.V., All rights reserved.","Denoising; Noise reduction; Speech emotion recognition","Acoustic noise; Artificial intelligence; Audio acoustics; Audio systems; Learning systems; Noise abatement; Signal denoising; Automatic recognition; De-noising; Emotion recognition; Log-spectral domain; Noise reduction algorithms; Noise reduction technique; Speech emotion recognition; Support vector regression (SVR); Speech recognition","","","","Barker, Jon P., The PASCAL CHiME speech separation and recognition challenge, Computer Speech and Language, 27, 3, pp. 621-633, (2013); Berouti, Michael G., ENHANCEMENT OF SPEECH CORRUPTED BY ACOUSTIC NOISE., pp. 208-211, (1979); Boll, Steven F., Suppression of Acoustic Noise in Speech Using Spectral Subtraction, IEEE Transactions on Acoustics, Speech, and Signal Processing, 27, 2, pp. 113-120, (1979); Voicebox Speech Processing Toolbox for Matlab Speech Enhancement Functions; Cappé, Olivier, Elimination of the Musical Noise Phenomenon with the Ephraim and Malah Noise Suppressor, IEEE Transactions on Speech and Audio Processing, 2, 2, pp. 345-349, (1994); Discrete Time Processing of Speech Signals, (1993); Ephraim, Yariv, Speech Enhancement Using a Minimum Mean-Square Error Short-Time Spectral Amplitude Estimator, IEEE Transactions on Acoustics, Speech, and Signal Processing, 32, 6, pp. 1109-1121, (1984); Ephraim, Yariv, Speech enhancement using a minimum mean-square error-log-spectral amplitude estimator, IEEE Transactions on Acoustics, Speech, and Signal Processing, 33, 2, pp. 443-445, (1985); Godin, Keith W., Impact of noise reduction and spectrum estimation on noise robust speaker identification, Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH, pp. 3656-3660, (2013); Goudbeek, Martijn B., Beyond arousal: Valence and potency/control cues in the vocal expression of emotion, Journal of the Acoustical Society of America, 128, 3, pp. 1322-1336, (2010)","","Association for Computing Machinery, Inc acmhelp@acm.org","ACM SIGMM","24th ACM Multimedia Conference, MM 2016","","Amsterdam","124107","","9781450336031","","","English","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-84994652671"
"Kaza, K.; Psaltis, A.; Stefanidis, K.; Apostolakis, K.C.; Thermos, S.; Dimitropoulos, K.; Daras, P.","Kaza, Kyriaki (57190066799); Psaltis, Athanasios (57190073285); Stefanidis, Kiriakos (57190065039); Apostolakis, Konstantinos Cornelis (55339270200); Thermos, Spyridon (57190068868); Dimitropoulos, Kosmas (22950021000); Daras, Petros (6602644373)","57190066799; 57190073285; 57190065039; 55339270200; 57190068868; 22950021000; 6602644373","Body motion analysis for emotion recognition in serious games","2016","Lecture Notes in Computer Science","9738","","","33","42","0","21","10.1007/978-3-319-40244-4_4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978907394&doi=10.1007%2F978-3-319-40244-4_4&partnerID=40&md5=d531d232f59dbc81a66d5920ab9577ac","Centre for Research and Technology-Hellas, Thessaloniki, Greece","Kaza, Kyriaki, Centre for Research and Technology-Hellas, Thessaloniki, Greece; Psaltis, Athanasios, Centre for Research and Technology-Hellas, Thessaloniki, Greece; Stefanidis, Kiriakos, Centre for Research and Technology-Hellas, Thessaloniki, Greece; Apostolakis, Konstantinos Cornelis, Centre for Research and Technology-Hellas, Thessaloniki, Greece; Thermos, Spyridon, Centre for Research and Technology-Hellas, Thessaloniki, Greece; Dimitropoulos, Kosmas, Centre for Research and Technology-Hellas, Thessaloniki, Greece; Daras, Petros, Centre for Research and Technology-Hellas, Thessaloniki, Greece","In this paper, we present an emotion recognition methodology that utilizes information extracted from body motion analysis to assess affective state during gameplay scenarios. A set of kinematic and geometrical features are extracted from joint-oriented skeleton tracking and are fed to a deep learning network classifier. In order to evaluate the performance of our methodology, we created a dataset with Microsoft Kinect recordings of body motions expressing the five basic emotions (anger, happiness, fear, sadness and surprise) which are likely to appear in a gameplay scenario. In this five emotions recognition problem, our methodology outperformed all other classifiers, achieving an overall recognition rate of 93 %. Furthermore, we conducted a second series of experiments to perform a qualitative analysis of the features and assess the descriptive power of different groups of features. © 2017 Elsevier B.V., All rights reserved.","3D body movement features; Body motion analysism; Emotion recognition; RBM; Serious games","Motion analysis; Speech recognition; Body motion analysis; Body motions; Body movements; Emotion recognition; Emotions recognition; Geometrical features; Qualitative analysis; Serious games; Human computer interaction","","","The research leading to this work has received funding from the EU Horizon 2020 Framework Programme under grant agreement no. 644204 (ProsocialLearn project).","Ekman, Paul, Differential communication of affect by head and body cues, Journal of Personality and Social Psychology, 2, 5, pp. 726-735, (1965); Wallbott, Harald G., Bodily expression of emotion, European Journal of Social Psychology, 28, 6, pp. 879-896, (1998); Boone, R. Thomas, Children's decoding of emotion in expressive body movement: the development of cue attunement., Developmental Psychology, 34, 5, pp. 1007-1016, (1998); de Meijer, Marco, The contribution of general features of body movement to the attribution of emotions, Journal of Nonverbal Behavior, 13, 4, pp. 247-268, (1989); de Gelder, Beatrice, Why bodies? Twelve reasons for including bodily expressions in affective neuroscience, Philosophical Transactions of the Royal Society B: Biological Sciences, 364, 1535, pp. 3475-3484, (2009); Gunes, Hatice, Bodily Expression for Automatic Affect Recognition, pp. 343-377, (2015); Castellano, Ginevra, Recognising human emotions from body movement and gesture dynamics, Lecture Notes in Computer Science, 4738 LNCS, pp. 71-82, (2007); Coulson, Mark C., Attributing emotion to static body postures: Recognition accuracy, confusions, and viewpoint dependence, Journal of Nonverbal Behavior, 28, 2, pp. 117-139, (2004); Kleinsmith, Andrea L., Cross-cultural differences in recognizing affect from body posture, Interacting with Computers, 18, 6, pp. 1371-1389, (2006); Camurri, Antonio, Multimodal analysis of expressive gesture in music and dance performances, Lecture Notes in Computer Science, 2915, pp. 20-39, (2004)","Antona, M.; Stephanidis, C.","Springer Verlag service@springer.de","","10th International Conference on Universal Access in Human-Computer Interaction, UAHCI 2016 and Held as Part of 18th International Conference on Human-Computer Interaction, HCI International 2016","","Toronto; ON","177999","16113349; 03029743","9789819698936; 9789819698042; 9789819698110; 9789819698905; 9789819512324; 9783032026019; 9783032008909; 9783031915802; 9789819698141; 9783031984136","","","English","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-84978907394"
"Douiji, D.; Mousannif, M.; Hassan, A.M.","Douiji, Yasmina (56512145800); Mousannif, Hajar (36802403100); Hassan, Al Moatassime (57189456795)","56512145800; 36802403100; 57189456795","Using YouTube Comments for Text-based Emotion Recognition","2016","Procedia Computer Science","83","","","292","299","0","68","10.1016/j.procs.2016.04.128","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971287189&doi=10.1016%2Fj.procs.2016.04.128&partnerID=40&md5=b0ead973e44079e4f01ae7c64f244e5b","Faculté des Sciences et Techniques Marrakech, Marakech, Morocco; Faculty of Semlalia, Morocco","Douiji, Yasmina, Faculté des Sciences et Techniques Marrakech, Marakech, Morocco; Mousannif, Hajar, Faculty of Semlalia, Morocco; Hassan, Al Moatassime, Faculté des Sciences et Techniques Marrakech, Marakech, Morocco","With the increase of Smartphone use, there is a growing need for advanced features that offer to Smartphone users a smarter interaction. We aim through the presented system to detect users' emotions from their textual exchanges, dealing with the complexity of chat writing style and the evolution of languages. We consider that such a system is a start for interesting applications that exploit users' emotional states. Our system uses an unsupervised machine learning algorithm that performs emotion classification, based on a data corpus built from YouTube comments. The reason behind such a choice is the similarity between YouTube comments and instant messages writing style. To classify a text entry into a particular emotion category, we compute its similarity to each target emotion, using the Pointwise Mutual Information measure. Our method yields a global precision of 92.75%, which reflects the feasibility of our approach. © 2016 Elsevier B.V., All rights reserved.","affective computing; emotion recognition; machine learning; text","Artificial intelligence; Classification (of information); Complex networks; Energy conservation; Learning algorithms; Learning systems; Signal encoding; Smartphones; Speech recognition; Affective Computing; Emotion classification; Emotion recognition; Evolution of languages; Instant messages; Pointwise mutual information; text; Unsupervised machine learning; Character recognition","","","","Maglogiannis, Ilias G., Face detection and recognition of natural human emotion using Markov random fields, Personal and Ubiquitous Computing, 13, 1, pp. 95-101, (2009); Busso, Carlos, Iterative feature normalization for emotional speech detection, Proceedings - ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 5692-5695, (2011); Gunes, Hatice, Bi-modal emotion recognition from expressive face and body gestures, Journal of Network and Computer Applications, 30, 4, pp. 1334-1345, (2007); Agrawal, Ameeta, Unsupervised emotion detection from text using semantic and syntactic relations, pp. 346-353, (2012); Nebraska Symposium on Motivation, (1972); Proceedings of the Biennial Gscl Conference, (2009); Kao, Edward Chao Chun, Towards text-based emotion detection: A survey and possible improvements, pp. 70-74, (2009); Strapparava, Carlo, WordNet-Affect: An affective extension of WordNet, pp. 1083-1086, (2004); Journal of Regional Science, (1968); Chaumartin, François Régis, UPAR7: A knowledge-based system for headline sentiment tagging, pp. 422-425, (2007)","Shakshuki, E.","Elsevier","","7th International Conference on Ambient Systems, Networks and Technologies, ANT 2016 and the 6th International Conference on Sustainable Energy Information Technology, SEIT 2016","","Madrid; Technical School of Telecommunications Engineering (ETSIT), Universidad Politecnica de Madrid (UPM)","121607","18770509","9781510849914","","","English","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-84971287189"
"Cano, A.R.; García-Tejedor, Á.J.; Fernández-Manjón, B.","Cano, Ana Rus (56911857600); García-Tejedor, Álvaro Josè (27567618300); Fernández-Manjón, Baltasar (55884700700)","56911857600; 27567618300; 55884700700","A literature Review of Serious games for intellectual Disabilities","2015","Lecture Notes in Computer Science","9307","","","560","563","0","9","10.1007/978-3-319-24258-3_59","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944707689&doi=10.1007%2F978-3-319-24258-3_59&partnerID=40&md5=9e91dced2076e17e9ea994b6024bdf22","Universidad Complutense de Madrid, Madrid, Spain; Universidad Francisco de Vitoria, Pozuelo de Alarcon, Spain","Cano, Ana Rus, Universidad Complutense de Madrid, Madrid, Spain; García-Tejedor, Álvaro Josè, Universidad Francisco de Vitoria, Pozuelo de Alarcon, Spain; Fernández-Manjón, Baltasar, Universidad Complutense de Madrid, Madrid, Spain","Our review examines the literature on Serious Games used as learning tools for people with intellectual disabilities. Although intellectual disabilities are a very broad field where each individual has very specific characteristics, it would be beneficial to have general evidence-based recommendations about how to design videogames adapted to their cognitive requirements. Thus, the first step of our investigation is to identify and review the available literature on Serious Games for intellectual disabilities classifying them according to the learning outcomes associated. Search terms identified 43 papers covering this topic and our review presents the initial results. A second aim is to understand the mechanics designed, the methods used in the investigation and the data obtained. The final goal is to identify what is working in this kind of games and how this can be generalized into a methodology to simplify the creation of more effective games for people with intellectual disabilities. © 2015 Elsevier B.V., All rights reserved.","Autism spectrum disorder; Cognitive disabilities; Down syndrome; Educational games; Intellectual disabilities; Serious games","Artificial intelligence; Computers; Autism spectrum disorders; Cognitive disability; Down syndrome; Educational game; Intellectual disability; Serious games; Engineering education","","","The e-UCM research group has been partially funded by Regional Government of Madrid (eMadrid S2013/ICE-2715), by the Complutense University of Madrid (GR3/14-921340), by the Ministry of Education (TIN2013-46149-C2-1-R), by the RIURE Network (CYTED 513RT0471) and by the European Commission (RAGE H2020-ICT-2014-1-644187).","Alves, Samanta, LifeisGame prototype: A serious game about emotions for children with autism spectrum disorders, PsychNology Journal, 11, 3, pp. 191-211, (2013); Karal, Hasan, Educational computer games for developing psychomotor ability in children with mild mental impairment, Procedia - Social and Behavioral Sciences, 9, pp. 996-1000, (2010); Davis, Andrew S., Children With Down Syndrome: Implications for Assessment and Intervention in the School, School Psychology Quarterly, 23, 2, pp. 271-281, (2008); International Handbook of Autism and Pervasive Developmental Disorders, (2011); Connolly, Thomas M., A systematic literature review of empirical evidence on computer games and serious games, Computers and Education, 59, 2, pp. 661-686, (2012); Wouters, Pieter, Current practices in serious game research: A review from a learning outcomes perspective, pp. 232-250, (2009)","Konert, J.; Klobučar, T.; Rensing, Christoph; Conole, G.; Lavoue, É.","Springer Verlag service@springer.de","","10th European Conference on Technology Enhanced Learning, EC-TEL 2015","","Toledo","141279","16113349; 03029743","9789819698936; 9789819698042; 9789819698110; 9789819698905; 9789819512324; 9783032026019; 9783032008909; 9783031915802; 9789819698141; 9783031984136","","","English","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-84944707689"
