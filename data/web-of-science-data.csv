"Publication Type","Authors","Book Authors","Book Editors","Book Group Authors","Author Full Names","Book Author Full Names","Group Authors","Article Title","Source Title","Book Series Title","Book Series Subtitle","Language","Document Type","Conference Title","Conference Date","Conference Location","Conference Sponsor","Conference Host","Author Keywords","Keywords Plus","Abstract","Addresses","Affiliations","Reprint Addresses","Email Addresses","Researcher Ids","ORCIDs","Funding Orgs","Funding Name Preferred","Funding Text","Cited References","Cited Reference Count","Times Cited, WoS Core","Times Cited, All Databases","180 Day Usage Count","Since 2013 Usage Count","Publisher","Publisher City","Publisher Address","ISSN","eISSN","ISBN","Journal Abbreviation","Journal ISO Abbreviation","Publication Date","Publication Year","Volume","Issue","Part Number","Supplement","Special Issue","Meeting Abstract","Start Page","End Page","Article Number","DOI","DOI Link","Book DOI","Early Access Date","Number of Pages","WoS Categories","Web of Science Index","Research Areas","IDS Number","Pubmed Id","Open Access Designations","Highly Cited Status","Hot Paper Status","Date of Export","UT (Unique WOS ID)","Web of Science Record"
"J","Swadi, MR; Croock, MS","","","","Swadi, Mazin R.; Croock, Muayad S.","","","Intelligent Mobile Application for Autism Detection and Level Identification System Using Deep-Learning Model","TRAITEMENT DU SIGNAL","","","English","Article","","","","","","","","The early detection and level assignment are very important in autism spectrum disorder (ASD) cases. In this paper, we introduce an autism diagnosing and level identification mobile application based on a deep-learning model. The application is designed with two stages; the first stage classifies children as either ASD children or potentially normal, while the second stage identifies the ASD level. For feature extraction and image classification, a convolutional neural network (CNN) is proposed. The 2,122 photos that were removed from the 3000 original dataset because of poor quality and racial imbalances made up the dataset used to develop and evaluate this model. Results show that the accuracy is 97.3% and the area under the curve is 99.8%. The identification of ASD level is performed using two welldefined scales in the literature that are adopted in the traditional examinations. Depending on the child's ASD level, psychiatrists provide specific and standard learning support. The application provides caregivers with fast decisions (about 20 minutes) to get an idea of the learning strategy to be followed with their child. Early intervention is very beneficial for children diagnosed with ASD, and these applications assist children in underfunded nations or those without access to healthcare.","[Swadi, Mazin R.; Croock, Muayad S.] Univ Technol Iraq, Control & Syst Engn Dept, Baghdad 10066, Iraq","","Swadi, MR (corresponding author), Univ Technol Iraq, Control & Syst Engn Dept, Baghdad 10066, Iraq.","cse.22.08@grad.uotechnology.edu.iq","","","","","","","","0","0","","","INT INFORMATION & ENGINEERING TECHNOLOGY ASSOC","EDMONTON","#2020, SCOTIA PLACE TOWER ONE, 10060 JASPER AVE, EDMONTON, AB T5J 3R8, CANADA","","","","TRAIT SIGNAL","Trait. Signal","OCT","2024","41","5","","","","","2539","2548","","10.18280/ts.410527","http://dx.doi.org/10.18280/ts.410527","","","10","","","","M8H7S","","","","","2025-10-20","WOS:001359895400027","View Full Record in Web of Science"
"J","Washington, P; Kalantarian, H; Kent, J; Husic, A; Kline, A; Leblanc, E; Hou, C; Mutlu, OC; Dunlap, K; Penev, Y; Varma, M; Stockham, NT; Chrisman, B; Paskov, K; Sun, MW; Jung, JY; Voss, C; Haber, N; Wall, DP","","","","Washington, Peter; Kalantarian, Haik; Kent, John; Husic, Arman; Kline, Aaron; Leblanc, Emilie; Hou, Cathy; Mutlu, Onur Cezmi; Dunlap, Kaitlyn; Penev, Yordan; Varma, Maya; Stockham, Nate Tyler; Chrisman, Brianna; Paskov, Kelley; Sun, Min Woo; Jung, Jae-Yoon; Voss, Catalin; Haber, Nick; Wall, Dennis Paul","","","Improved Digital Therapy for Developmental Pediatrics Using Domain-Specific Artificial Intelligence: Machine Learning Study","JMIR PEDIATRICS AND PARENTING","","","English","Article","","","","","","","","Background: Automated emotion classification could aid those who struggle to recognize emotions, including children with developmental behavioral conditions such as autism. However, most computer vision emotion recognition models are trained on adult emotion and therefore underperform when applied to child faces. Objective: We designed a strategy to gamify the collection and labeling of child emotion-enriched images to boost the performance of automatic child emotion recognition models to a level closer to what will be needed for digital health care approaches. Methods: We leveraged our prototype therapeutic smartphone game, GuessWhat, which was designed in large part for children with developmental and behavioral conditions, to gamify the secure collection of video data of children expressing a variety of emotions prompted by the game. Independently, we created a secure web interface to gamify the human labeling effort, called HollywoodSquares, tailored for use by any qualified labeler. We gathered and labeled 2155 videos, 39,968 emotion frames, and 106,001 labels on all images. With this drastically expanded pediatric emotion-centric database (>30 times larger than existing public pediatric emotion data sets), we trained a convolutional neural network (CNN) computer vision classifier of happy, sad, surprised, fearful, angry, disgust, and neutral expressions evoked by children. Results: The classifier achieved a 66.9% balanced accuracy and 67.4% Fl-score on the entirety of the Child Affective Facial Expression (CAFE) as well as a 79.1% balanced accuracy and 78% Fl-score on CAFE Subset A, a subset containing at least 60% human agreement on emotions labels. This performance is at least 10% higher than all previously developed classifiers evaluated against CAFE, the best of which reached a 56% balanced accuracy even when combining anger and disgust into a single class. Conclusions: This work validates that mobile games designed for pediatric therapies can generate high volumes of domain-relevant data sets to train state-of-the-art classifiers to perform tasks helpful to precision health efforts.","[Washington, Peter] Stanford Univ, Dept Pediat Syst Med, Stanford, CA 94305 USA; [Washington, Peter] Stanford Univ, Dept Biomed Data Sci, Stanford, CA 94305 USA","","Washington, P (corresponding author), Stanford Univ, Dept Pediat Syst Med, Stanford, CA 94305 USA.;Washington, P (corresponding author), Stanford Univ, Dept Biomed Data Sci, Stanford, CA 94305 USA.","peterwashington@stanford.edu","","","National Institutes of Health [1R01EB025025-01, 1R01LM013364-01, 1R21HD091500-01, 1R01LM013083]; National Science Foundation [2014232]; Hartwell Foundation; Bill and Melinda Gates Foundation; Coulter Foundation; Lucile Packard Foundation; Auxiliaries Endowment; Islamic Development Bank Transform Fund; Weston Havens Foundation; Stanford's Human-Centered Artificial Intelligence Program; Precision Health and Integrated Diagnostics Center; Beckman Center; Bio-X Center; Predictives and Diagnostics Accelerator; Spark Program in Translational Research; Wu Tsai Neurosciences Institute's Neuroscience:Translate Program; Stanford Interdisciplinary Graduate Fellowship (SIGF); Spectrum; MediaX; Direct For Computer & Info Scie & Enginr; Div Of Information & Intelligent Systems [2014232] Funding Source: National Science Foundation; National Library of Medicine [R01LM013083] Funding Source: NIH RePORTER","National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); National Science Foundation(National Science Foundation (NSF)); Hartwell Foundation; Bill and Melinda Gates Foundation(Bill & Melinda Gates Foundation); Coulter Foundation; Lucile Packard Foundation(The David & Lucile Packard Foundation); Auxiliaries Endowment; Islamic Development Bank Transform Fund; Weston Havens Foundation; Stanford's Human-Centered Artificial Intelligence Program; Precision Health and Integrated Diagnostics Center; Beckman Center; Bio-X Center; Predictives and Diagnostics Accelerator; Spark Program in Translational Research; Wu Tsai Neurosciences Institute's Neuroscience:Translate Program; Stanford Interdisciplinary Graduate Fellowship (SIGF); Spectrum; MediaX; Direct For Computer & Info Scie & Enginr; Div Of Information & Intelligent Systems(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE)NSF - Division of Information & Intelligent Systems (IIS)); National Library of Medicine(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Library of Medicine (NLM))","We would like to acknowledge all the nine high school and undergraduate emotion annotators: Natalie Park, Chris Harjadi, Meagan Tsou, Belle Bankston, Hadley Daniels, Sky Ng-Thow-Hing, Bess Olshen, Courtney McCormick, and Jennifer Yu. The work was supported in part by funds to DPW from the National Institutes of Health (grants 1R01EB025025-01, 1R01LM013364-01, 1R21HD091500-01, and 1R01LM013083) , the National Science Foundation (Award 2014232) , The Hartwell Foundation, Bill and Melinda Gates Foundation, Coulter Foundation, Lucile Packard Foundation, Auxiliaries Endowment, the Islamic Development Bank Transform Fund, the Weston Havens Foundation, and program grants from Stanford's Human-Centered Artificial Intelligence Program, Precision Health and Integrated Diagnostics Center, Beckman Center, Bio-X Center, Predictives and Diagnostics Accelerator, Spectrum, Spark Program in Translational Research, MediaX, and from the Wu Tsai Neurosciences Institute's Neuroscience:Translate Program. We also acknowledge generous support from David Orr, Imma Calvo, Bobby Dekesyer, and Peter Sullivan. PW would like to acknowledge support from Mr. Schroeder and the Stanford Interdisciplinary Graduate Fellowship (SIGF) as the Schroeder Family Goldman Sachs Graduate Fellow.","","","19","22","","","JMIR PUBLICATIONS, INC","TORONTO","130 QUEENS QUAY East, Unit 1100, TORONTO, ON M5A 0P6, CANADA","","","","JMIR PEDIATR PARENT","JMIR Pediatr. Parent.","APR-JUN","2022","5","2","","","","","","","e26760","10.2196/26760","http://dx.doi.org/10.2196/26760","","","14","","","","2U6UN","","Green Submitted, Green Published, gold","","","2025-10-20","WOS:000823293300008","View Full Record in Web of Science"
"J","Wang, HP; Tobon, VDP; Hossain, MS; El Saddik, A","","","","Wang, Haopeng; Tobon, Diana P., V; Hossain, M. Shamim; El Saddik, Abdulmotaleb","","","Deep Learning (DL)-Enabled System for Emotional Big Data","IEEE ACCESS","","","English","Article","","","","","","","","Emotion care for human well-being is important for all ages. In this paper, we propose an emotion care system based on big data analysis for autism disorder patient training, where emotion is detected in terms of facial expression. The expression can be captured through a camera as well as Internet of Things (IoT)-enabled devices. The system works with deep learning techniques on emotional big data to extract emotional features and recognize six kinds of facial expressions in real-time and offline. A convolutional neural network (CNN) model based on MobileNet V1 structure is trained with two emotional datasets, FER-2013 dataset and a new proposed dataset named MCFER. The experiments on three strategies showed that the proposed system with deep learning model obtained an accuracy of 95.89%. The system can also detect and track multiple faces as well as recognize facial expressions with high performance on mobile devices with a speed of up to 12 frames per second.","[Wang, Haopeng; Tobon, Diana P., V; El Saddik, Abdulmotaleb] Univ Ottawa, Sch Elect Engn & Comp Sci, MCRLAB, Ottawa, ON K1N 6N5, Canada; [Hossain, M. Shamim] King Saud Univ, Dept Software Engn, Coll Comp & Informat Sci, Riyadh 11543, Saudi Arabia","","Hossain, MS (corresponding author), King Saud Univ, Dept Software Engn, Coll Comp & Informat Sci, Riyadh 11543, Saudi Arabia.","mshossain@ksu.edu.sa","","","King Saud University, Riyadh, Saudi Arabia [RSP-2021/32]","King Saud University, Riyadh, Saudi Arabia(King Saud University)","This work was supported by the Researchers Supporting Project number (RSP-2021/32), King Saud University, Riyadh, Saudi Arabia.","","","6","10","","","IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","PISCATAWAY","445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","","","","IEEE ACCESS","IEEE Access","","2021","9","","","","","","116073","116082","","10.1109/ACCESS.2021.3103501","http://dx.doi.org/10.1109/ACCESS.2021.3103501","","","10","","","","UI2AY","","gold","","","2025-10-20","WOS:000690418100001","View Full Record in Web of Science"
"J","Banerjee, A; Mutlu, OC; Kline, A; Surabhi, S; Washington, P; Wall, DP","","","","Banerjee, Agnik; Mutlu, Onur Cezmi; Kline, Aaron; Surabhi, Saimourya; Washington, Peter; Wall, Dennis Paul","","","Training and Profiling a Pediatric Facial Expression Classifier for Children on Mobile Devices: Machine Learning Study","JMIR FORMATIVE RESEARCH","","","English","Article","","","","","","","","Background: Implementing automated facial expression recognition on mobile devices could provide an accessible diagnostic and therapeutic tool for those who struggle to recognize facial expressions, including children with developmental behavioral conditions such as autism. Despite recent advances in facial expression classifiers for children, existing models are too computationally expensive for smartphone use. Objective: We explored several state-of-the-art facial expression classifiers designed for mobile devices, used posttraining optimization techniques for both classification performance and efficiency on a Motorola Moto G6 phone, evaluated the importance of training our classifiers on children versus adults, and evaluated the models' performance against different ethnic groups. Methods: We collected images from 12 public data sets and used video frames crowdsourced from the GuessWhat app to train our classifiers. All images were annotated for 7 expressions: neutral, fear, happiness, sadness, surprise, anger, and disgust. We tested 3 copies for each of 5 different convolutional neural network architectures: MobileNetV3-Small 1.0x, MobileNetV2 1.0x, EfficientNetB0, MobileNetV3-Large 1.0x, and NASNetMobile. We trained the first copy on images of children, second copy on images of adults, and third copy on all data sets. We evaluated each model against the entire Child Affective Facial Expression (CAFE) set and by ethnicity. We performed weight pruning, weight clustering, and quantize-aware training when possible and profiled each model's performance on the Moto G6. Results: Our best model, a MobileNetV3-Large network pretrained on ImageNet, achieved 65.78% accuracy and 65.31% F1-score on the CAFE and a 90-millisecond inference latency on a Moto G6 phone when trained on all data. This accuracy is only 1.12% lower than the current state of the art for CAFE, a model with 13.91x more parameters that was unable to run on the Moto G6 due to its size, even when fully optimized. When trained solely on children, this model achieved 60.57% accuracy and 60.29% F1-score. When trained only on adults, the model received 53.36% accuracy and 53.10% F1-score. Although the MobileNetV3-Large trained on all data sets achieved nearly a 60% F1-score across all ethnicities, the data sets for South Asian and African American children achieved lower accuracy (as much as 11.56%) and F1-score (as much as 11.25%) than other groups. Conclusions: With specialized design and optimization techniques, facial expression classifiers can become lightweight enough to run on mobile devices and achieve state-of-the-art performance. There is potentially a data shift phenomenon between facial expressions of children compared with adults; our classifiers performed much better when trained on children. Certain underrepresented ethnic groups (e.g., South Asian and African American) also perform significantly worse than groups such as","[Banerjee, Agnik; Kline, Aaron; Surabhi, Saimourya; Wall, Dennis Paul] Stanford Univ, Dept Pediat Syst Med, Stanford, CA USA; [Mutlu, Onur Cezmi] Stanford Univ, Dept Elect Engn, Stanford, CA USA; [Washington, Peter] Univ Hawaii Manoa, Dept Informat & Comp Sci, Honolulu, HI USA; [Wall, Dennis Paul] Stanford Univ, Dept Biomed Data Sci, Stanford, CA USA; [Wall, Dennis Paul] Stanford Univ, Dept Psychiat & Behav Sci, Stanford, CA USA; [Wall, Dennis Paul] Stanford Univ, Dept Pediat Syst Med, 3145 Porter Dr, Stanford, CA 94304 USA","","Wall, DP (corresponding author), Stanford Univ, Dept Pediat Syst Med, 3145 Porter Dr, Stanford, CA 94304 USA.","dpwall@stanford.edu","","","National Institutes of Health [1R01EB025025-01, 1R01LM013364-01, 1R21HD091500-01, 1R01LM013083]; National Science Foundation [2014232]; Hartwell Foundation; Bill and Melinda Gates Foundation; Coulter Foundation; Lucile Packard Foundation; Auxiliaries Endowment; Islamic Development Bank Transform Fund; Weston Havens Foundation; Stanford's Human Centered Artificial Intelligence Program, Precision Health and Integrated Diagnostics Center, Beckman Center, Bio-X Center, Predictives and Diagnostics Accelerator, Spectrum; Wu Tsai Neurosciences Institute's Neuroscience:Translate Program; Spark Program in Translational Research; Mr. Schroeder and the Stanford Interdisciplinary Graduate Fellowship (SIGF); MediaX","National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); National Science Foundation(National Science Foundation (NSF)); Hartwell Foundation; Bill and Melinda Gates Foundation(Bill & Melinda Gates Foundation); Coulter Foundation; Lucile Packard Foundation(The David & Lucile Packard Foundation); Auxiliaries Endowment; Islamic Development Bank Transform Fund; Weston Havens Foundation; Stanford's Human Centered Artificial Intelligence Program, Precision Health and Integrated Diagnostics Center, Beckman Center, Bio-X Center, Predictives and Diagnostics Accelerator, Spectrum; Wu Tsai Neurosciences Institute's Neuroscience:Translate Program; Spark Program in Translational Research; Mr. Schroeder and the Stanford Interdisciplinary Graduate Fellowship (SIGF); MediaX","The work was supported in part by funds to DPW from the National Institutes of Health (1R01EB025025-01, 1R01LM013364-01, 1R21HD091500-01, 1R01LM013083) ; the National Science Foundation (Award 2014232) ; The Hartwell Foundation; the Bill and Melinda Gates Foundation; the Coulter Foundation; the Lucile Packard Foundation; the Auxiliaries Endowment; the Islamic Development Bank Transform Fund; the Weston Havens Foundation; program grants from Stanford's Human Centered Artificial Intelligence Program, Precision Health and Integrated Diagnostics Center, Beckman Center, Bio-X Center, Predictives and Diagnostics Accelerator, Spectrum, Spark Program in Translational Research, and MediaX; and the Wu Tsai Neurosciences Institute's Neuroscience:Translate Program. We also acknowledge generous support from David Orr, Imma Calvo, Bobby Dekesyer, and Peter Sullivan. PW would like to acknowledge support from Mr. Schroeder and the Stanford Interdisciplinary Graduate Fellowship (SIGF) as the Schroeder Family Goldman Sachs Graduate Fellow.","","","9","10","","","JMIR PUBLICATIONS, INC","TORONTO","130 QUEENS QUAY East, Unit 1100, TORONTO, ON M5A 0P6, CANADA","","","","JMIR FORM RES","JMIR Form. Res.","","2023","7","","","","","","","","e39917","10.2196/39917","http://dx.doi.org/10.2196/39917","","","15","","","","H8ON2","","Green Submitted, gold","","","2025-10-20","WOS:000998490100003","View Full Record in Web of Science"
"J","Voss, C; Schwartz, J; Daniels, J; Kline, A; Haber, N; Washington, P; Tariq, Q; Robinson, TN; Desai, M; Phillips, JM; Feinstein, C; Winograd, T; Wall, DP","","","","Voss, Catalin; Schwartz, Jessey; Daniels, Jena; Kline, Aaron; Haber, Nick; Washington, Peter; Tariq, Qandeel; Robinson, Thomas N.; Desai, Manisha; Phillips, Jennifer M.; Feinstein, Carl; Winograd, Terry; Wall, Dennis P.","","","Effect of Wearable Digital Intervention for Improving Socialization in Children With Autism Spectrum Disorder A Randomized Clinical Trial","JAMA PEDIATRICS","","","English","Article","","","","","","","","IMPORTANCE Autism behavioral therapy is effective but expensive and difficult to access. While mobile technology-based therapy can alleviate wait-lists and scale for increasing demand, few clinical trials exist to support its use for autism spectrum disorder (ASD) care. OBJECTIVE To evaluate the efficacy of Superpower Glass, an artificial intelligence-driven wearable behavioral intervention for improving social outcomes of children with ASD. DESIGN, SETTING, AND PARTICIPANTS A randomized clinical trial in which participants received the Superpower Glass intervention plus standard of care applied behavioral analysis therapy and control participants received only applied behavioral analysis therapy. Assessments were completed at the Stanford University Medical School, and enrolled participants used the Superpower Glass intervention in their homes. Children aged 6 to 12 years with a formal ASD diagnosis who were currently receiving applied behavioral analysis therapy were included. Families were recruited between June 2016 and December 2017. The first participant was enrolled on November 1, 2016, and the last appointment was completed on April 11, 2018. Data analysis was conducted between April and October 2018. INTERVENTIONS The Superpower Glass intervention, deployed via Google Glass (worn by the child) and a smartphone app, promotes facial engagement and emotion recognition by detecting facial expressions and providing reinforcing social cues. Families were asked to conduct 20-minute sessions at home 4 times per week for 6 weeks. MAIN OUTCOMES AND MEASURES Four socialization measures were assessed using an intention-to-treat analysis with a Bonferroni test correction. RESULTS Overall, 71children (63 boys [89%]; mean [SD] age, 8.38 [2.46] years) diagnosed with ASD were enrolled (40 [56.3%] were randomized to treatment, and 31(43.7%) were randomized to control). Children receiving the intervention showed significant improvements on the Vineland Adaptive Behaviors Scale socialization subscale compared with treatment as usual controls (mean [SD] treatment impact, 4.58 [1.62]; P =.005). Positive mean treatment effects were also found for the other 3 primary measures but not to a significance threshold of P =.0125. CONCLUSIONS AND RELEVANCE The observed 4.58-point average gain on the Vineland Adaptive Behaviors Scale socialization subscale is comparable with gains observed with standard of care therapy. To our knowledge, this is the first randomized clinical trial to demonstrate efficacy of a wearable digital intervention to improve social behavior of children with ASD. The intervention reinforces facial engagement and emotion recognition, suggesting either or both could be a mechanism of action driving the observed improvement. This study underscores the potential of digital home therapy to augment the standard of care.","[Voss, Catalin; Winograd, Terry] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA; [Schwartz, Jessey; Daniels, Jena; Kline, Aaron; Haber, Nick; Washington, Peter; Tariq, Qandeel; Robinson, Thomas N.; Desai, Manisha; Wall, Dennis P.] Stanford Univ, Dept Pediat Syst Med, Stanford, CA 94305 USA; [Wall, Dennis P.] Stanford Univ, Dept Biomed Data Sci, Stanford, CA 94305 USA; [Robinson, Thomas N.; Phillips, Jennifer M.; Feinstein, Carl] Stanford Univ, Dept Pediat, Stanford Solut Sci Lab, Stanford, CA 94305 USA; [Robinson, Thomas N.; Phillips, Jennifer M.; Feinstein, Carl] Stanford Univ, Dept Med, Stanford, CA 94305 USA; [Wall, Dennis P.] Stanford Univ, Dept Psychiat & Behav Sci, Stanford, CA 94305 USA","","Wall, DP (corresponding author), Stanford Univ, Dept Pediat, Div Syst Med, 1265 Welch Rd,Med Sch Off Bldg X14, Stanford, CA 94305 USA.","dpwall@stanford.edu","","","National Institutes of Health [1R01EB02502501, 1R21HD091500-01]; Lucile Packard Foundation for Children's Health; Hartwell Foundation; Wallace H. Coulter Foundation; Stanford Precision Health and Integrated Diagnostics Center (PHIND); Stanford Predictives and Diagnostics Accelerator (SPADA); Stanford Bio-X Center Program; Stanford Beckman Center; Walter V. and Idun Berry Postdoctoral Fellowship Program; Bill and Melinda Gates Foundation","National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Lucile Packard Foundation for Children's Health; Hartwell Foundation; Wallace H. Coulter Foundation; Stanford Precision Health and Integrated Diagnostics Center (PHIND); Stanford Predictives and Diagnostics Accelerator (SPADA); Stanford Bio-X Center Program; Stanford Beckman Center; Walter V. and Idun Berry Postdoctoral Fellowship Program; Bill and Melinda Gates Foundation(Bill & Melinda Gates Foundation)","The work was supported by material support (35 units of Google Glass) from Google Inc and grants from the National Institutes of Health (grants 1R01EB02502501 and 1R21HD091500-01), the Lucile Packard Foundation for Children's Health, the Hartwell Foundation, the Wallace H. Coulter Foundation, the Stanford Precision Health and Integrated Diagnostics Center (PHIND), the Stanford Predictives and Diagnostics Accelerator (SPADA), the Stanford Bio-X Center Program, the Stanford Beckman Center, the Walter V. and Idun Berry Postdoctoral Fellowship Program, the Bill and Melinda Gates Foundation, Peter Sullivan, and Bobby Dekeyser. We thank David Orr and Imma Calvo for their generous support in obtaining funding.","","","123","145","","","AMER MEDICAL ASSOC","CHICAGO","330 N WABASH AVE, STE 39300, CHICAGO, IL 60611-5885 USA","","","","JAMA PEDIATR","JAMA Pediatr.","MAY","2019","173","5","","","","","446","454","","10.1001/jamapediatrics.2019.0285","http://dx.doi.org/10.1001/jamapediatrics.2019.0285","","","9","","","","HX6HR","","hybrid","","","2025-10-20","WOS:000467505200012","View Full Record in Web of Science"
"J","Daniels, J; Schwartz, JN; Voss, C; Haber, N; Fazel, A; Kline, A; Washington, P; Feinstein, C; Winograd, T; Wall, DP","","","","Daniels, Jena; Schwartz, Jessey N.; Voss, Catalin; Haber, Nick; Fazel, Azar; Kline, Aaron; Washington, Peter; Feinstein, Carl; Winograd, Terry; Wall, Dennis P.","","","Exploratory study examining the at-home feasibility of a wearable tool for social-affective learning in children with autism","NPJ DIGITAL MEDICINE","","","English","Article","","","","","","","","Although standard behavioral interventions for autism spectrum disorder (ASD) are effective therapies for social deficits, they face criticism for being time-intensive and overdependent on specialists. Earlier starting age of therapy is a strong predictor of later success, but waitlists for therapies can be 18 months long. To address these complications, we developed Superpower Glass, a machine-learning-assisted software system that runs on Google Glass and an Android smartphone, designed for use during social interactions. This pilot exploratory study examines our prototype tool's potential for social-affective learning for children with autism. We sent our tool home with 14 families and assessed changes from intake to conclusion through the Social Responsiveness Scale (SRS-2), a facial affect recognition task (EGG), and qualitative parent reports. A repeated-measures one-way ANOVA demonstrated a decrease in SRS-2 total scores by an average 7.14 points (F(1,13) = 33.20, p = <. 001, higher scores indicate higher ASD severity). EGG scores also increased by an average 9.55 correct responses (F(1,10) = 11.89, p = <. 01). Parents reported increased eye contact and greater social acuity. This feasibility study supports using mobile technologies for potential therapeutic purposes.","[Daniels, Jena; Schwartz, Jessey N.; Haber, Nick; Fazel, Azar; Kline, Aaron; Wall, Dennis P.] Stanford Univ, Dept Pediat, Div Syst Med, Palo Alto, CA 94304 USA; [Voss, Catalin; Washington, Peter; Winograd, Terry] Stanford Univ, Dept Comp Sci, Palo Alto, CA 94304 USA; [Feinstein, Carl; Wall, Dennis P.] Stanford Univ, Dept Psychiat & Behav Sci, Palo Alto, CA 94304 USA; [Wall, Dennis P.] Stanford Univ, Dept Biomed Data Sci, Palo Alto, CA 94304 USA","","Wall, DP (corresponding author), Stanford Univ, Dept Pediat, Div Syst Med, Palo Alto, CA 94304 USA.;Wall, DP (corresponding author), Stanford Univ, Dept Psychiat & Behav Sci, Palo Alto, CA 94304 USA.;Wall, DP (corresponding author), Stanford Univ, Dept Biomed Data Sci, Palo Alto, CA 94304 USA.","dpwall@stanford.edu","","","NIH [1R01EB025025-01, 1R21HD091500-01]; Hartwell Foundation; Bill and Melinda Gates Foundation; Coulter Foundation; Lucile Packard Foundation; Stanford's Precision Health and Integrated Diagnostics Center (PHIND); Beckman Center; Bio-X Center; Predictives and Diagnostics Accelerator Program; Child Health Research Institute (CHRI); Human-Centered AI (HAI); Google","NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Hartwell Foundation; Bill and Melinda Gates Foundation(Bill & Melinda Gates Foundation); Coulter Foundation; Lucile Packard Foundation(The David & Lucile Packard Foundation); Stanford's Precision Health and Integrated Diagnostics Center (PHIND); Beckman Center; Bio-X Center; Predictives and Diagnostics Accelerator Program; Child Health Research Institute (CHRI); Human-Centered AI (HAI); Google(Google Incorporated)","We would like to thank the participating families for their important contributions. The work was supported in part by funds to D. P. W. from NIH (1R01EB025025-01 and 1R21HD091500-01), the Hartwell Foundation, Bill and Melinda Gates Foundation, Coulter Foundation, Lucile Packard Foundation, and program grants from Stanford's Precision Health and Integrated Diagnostics Center (PHIND), Beckman Center, Bio-X Center, the Predictives and Diagnostics Accelerator Program, the Child Health Research Institute (CHRI), and Human-Centered AI (HAI). We also acknowledge generous support from David Orr, Imma Calvo, Bobby Dekesyer, and Peter Sullivan. In-kind material grants included a gift from Google (35 units of Google Glass version 1) and Amazon Web Services Founder Support.","","","64","74","","","NATURE PORTFOLIO","BERLIN","HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY","","","","NPJ DIGIT MED","npj Digit. Med.","AUG 2","2018","1","","","","","","","","32","10.1038/s41746-018-0035-3","http://dx.doi.org/10.1038/s41746-018-0035-3","","","10","","","","GT1CA","","Green Published, gold","","","2025-10-20","WOS:000444193800001","View Full Record in Web of Science"
"J","Wan, GB; Deng, FH; Jiang, ZJ; Song, SF; Hu, D; Chen, LF; Wang, HB; Li, MC; Chen, G; Yan, T; Su, JL; Zhang, JM","","","","Wan, Guobin; Deng, Fuhao; Jiang, Zijian; Song, Sifan; Hu, Di; Chen, Lifu; Wang, Haibo; Li, Miaochun; Chen, Gong; Yan, Ting; Su, Jionglong; Zhang, Jiaming","","","FECTS: A Facial Emotion Cognition and Training System for Chinese Children with Autism Spectrum Disorder","COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE","","","English","Article","","","","","","","","Traditional training methods such as card teaching, assistive technologies (e.g., augmented reality/virtual reality games and smartphone apps), DVDs, human-computer interactions, and human-robot interactions are widely applied in autistic rehabilitation training in recent years. In this article, we propose a novel framework for human-computer/robot interaction and introduce a preliminary intervention study for improving the emotion recognition of Chinese children with an autism spectrum disorder. The core of the framework is the Facial Emotion Cognition and Training System (FECTS, including six tasks to train children with ASD to match, infer, and imitate the facial expressions of happiness, sadness, fear, and anger) based on Simon Baron-Cohen's E-S (empathizing-systemizing) theory. Our system may be implemented on PCs, smartphones, mobile devices such as PADs, and robots. The training record (e.g., a tracked record of emotion imitation) of the Chinese autistic children interacting with the device implemented using our FECTS will be uploaded and stored in the database of a cloud-based evaluation system. Therapists and parents can access the analysis of the emotion learning progress of these autistic children using the cloud-based evaluation system. Deep-learning algorithms of facial expressions recognition and attention analysis will be deployed in the back end (e.g., devices such as a PC, a robotic system, or a cloud system) implementing our FECTS, which can perform real-time tracking of the imitation quality and attention of the autistic children during the expression imitation phase. In this preliminary clinical study, a total of 10 Chinese autistic children aged 3-8 are recruited, and each of them received a single 20-minute training session every day for four consecutive days. Our preliminary results validated the feasibility of the developed FECTS and the effectiveness of our algorithms based on Chinese children with an autism spectrum disorder. To verify that our FECTS can be further adapted to children from other countries, children with different cultural/sociological/linguistic contexts should be recruited in future studies.","[Wan, Guobin] Shenzhen Maternal & Child Hlth Hosp, Shenzhen 518000, Peoples R China; [Deng, Fuhao; Jiang, Zijian; Zhang, Jiaming] Shenzhen Inst Artificial Intelligence & Robot Soc, Shenzhen 518172, Peoples R China; [Song, Sifan] Xian Jiaotong Liverpool Univ, Dept Math Sci, Suzhou 215123, Peoples R China; [Hu, Di] Univ Maryland, Robert H Smith Sch Business, College Pk, MA USA; [Chen, Lifu] DoGoodly Int Educ Ctr Shenzhen Co Ltd, Shenzhen 518219, Peoples R China; [Chen, Lifu] Smart Children Educ Ctr, Shenzhen 518219, Peoples R China; [Wang, Haibo] China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221116, Jiangsu, Peoples R China; [Li, Miaochun] Guangdong Pharmaceut Univ, Dept Informat Management & Informat Syst, Zhongshan 511436, Peoples R China; [Chen, Gong] Sunwoda Elect Co Ltd, Shiyan St, Shenzhen 518000, Guangdong, Peoples R China; [Yan, Ting] Chinese Acad Sci, Brain Cognit & Brain Dis Inst,CAS Key Lab Brain C, Shenzhen Inst Adv Technol,Guangdong Prov Key Lab, Shenzhen Hong Kong Inst Brain Sci,Shenzhen Key La, Shenzhen 518055, Guangdong, Peoples R China; [Su, Jionglong] Xian Jiaotong Liverpool Univ, XJTLU Entrepreneur Coll Taicang, Sch AI & Adv Comp, Suzhou 215123, Jiangsu, Peoples R China; [Zhang, Jiaming] Chinese Univ Hong Kong Shenzhen, Inst Robot & Intelligent Mfg, Shenzhen 518172, Peoples R China","","Zhang, JM (corresponding author), Shenzhen Inst Artificial Intelligence & Robot Soc, Shenzhen 518172, Peoples R China.;Yan, T (corresponding author), Chinese Acad Sci, Brain Cognit & Brain Dis Inst,CAS Key Lab Brain C, Shenzhen Inst Adv Technol,Guangdong Prov Key Lab, Shenzhen Hong Kong Inst Brain Sci,Shenzhen Key La, Shenzhen 518055, Guangdong, Peoples R China.;Su, JL (corresponding author), Xian Jiaotong Liverpool Univ, XJTLU Entrepreneur Coll Taicang, Sch AI & Adv Comp, Suzhou 215123, Jiangsu, Peoples R China.;Zhang, JM (corresponding author), Chinese Univ Hong Kong Shenzhen, Inst Robot & Intelligent Mfg, Shenzhen 518172, Peoples R China.","ting.yan@siat.ac.cn; jionglong.su@xjtlu.edu.cn; zhangjiaming@cuhk.edu.cn","","","Shenzhen Science and Technology Innovation Commission [JCYJ20180508152240368]; National Natural Science Foundation of China [31800900]; Key Realm R&D Program of Guangdong Province [2019B030335001]; Sanming Project of Medicine in Shenzhen [SZSM201512009]; Open Program of Neusoft Corporation [SKLSAOP1702]","Shenzhen Science and Technology Innovation Commission; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Key Realm R&D Program of Guangdong Province; Sanming Project of Medicine in Shenzhen; Open Program of Neusoft Corporation","THe authors would like to thank Dr. Guobin Wan and his colleagues in Shenzhen Maternal and Child Health Hospital and Mr. Lifu Chen and his colleagues in DoGoodly International Education Center (Shenzhen) Co. Ltd., and Smart Children Education Center (Shenzhen), for giving many constructive suggestions in designing the Facial Emotion Cognition and Training System. 1is work was supported by the Shenzhen Science and Technology Innovation Commission (Grant no. JCYJ20180508152240368). 1is work was also supported by the National Natural Science Foundation of China (Grant no. 31800900), the Key Realm R&D Program of Guangdong Province (2019B030335001), the Sanming Project of Medicine in Shenzhen (SZSM201512009), and the Open Program of Neusoft Corporation (item number SKLSAOP1702).","","","19","21","","","HINDAWI LTD","LONDON","ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND","","","","COMPUT INTEL NEUROSC","Comput. Intell. Neurosci.","APR 27","2022","2022","","","","","","","","9213526","10.1155/2022/9213526","http://dx.doi.org/10.1155/2022/9213526","","","21","","","","2S1VE","","Green Published, hybrid","","","2025-10-20","WOS:000821586800009","View Full Record in Web of Science"
"J","Lee, CC; Chaspari, T; Provost, EM; Narayanan, SS","","","","Lee, Chi-Chun; Chaspari, Theodora; Provost, Emily Mower; Narayanan, Shrikanth S.","","","An Engineering View on Emotions and Speech: From Analysis and Predictive Models to Responsible Human-Centered Applications","PROCEEDINGS OF THE IEEE","","","English","Article","","","","","","","","The substantial growth of Internet-of-Things technology and the ubiquity of smartphone devices has increased the public and industry focus on speech emotion recognition (SER) technologies. Yet, conceptual, technical, and societal challenges restrict the wide adoption of these technologies in various domains, including, healthcare, and education. These challenges are amplified when automated emotion recognition systems are called to function in-the-wild due to the inherent complexity and subjectivity of human emotion, the difficulty of obtaining reliable labels at high temporal resolution, and the diverse contextual and environmental factors that confound the expression of emotion in real life. In addition, societal and ethical challenges hamper the wide acceptance and adoption of these technologies, with the public raising questions about user privacy, fairness, and explainability. This article briefly reviews the history of affective speech processing, provides an overview of current state-of-the-art approaches to SER, and discusses algorithmic approaches to render these technologies accessible to all, maximizing their benefits and leading to responsible human-centered computing applications.","[Lee, Chi-Chun] Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 300044, Taiwan; [Chaspari, Theodora] Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77843 USA; [Provost, Emily Mower] Univ Michigan, Dept Comp Sci & Engn, Ann Arbor, MI 48109 USA; [Narayanan, Shrikanth S.] Univ Southern Calif, Dept Elect & Comp Engn, Los Angeles, CA 90007 USA","","Chaspari, T (corresponding author), Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77843 USA.","cclee@ee.nthu.edu.tw; chaspari@tamu.edu; emilykmp@umich.edu; shri@ee.usc.edu","","","National Science and Technology Counsel (NSTC) Taiwan [110-2221-E-007-067-MY3]; National Science Foundation (NSF) [IIS-2046118]; NSF [RI-2006618, IIS-2204942]","National Science and Technology Counsel (NSTC) Taiwan; National Science Foundation (NSF)(National Science Foundation (NSF)); NSF(National Science Foundation (NSF))","The work of Chi-Chun Lee was supported by the National Science and Technology Counsel (NSTC) Taiwan under Grant 110-2221-E-007-067-MY3. The work of Theodora Chaspari was supported by the National Science Foundation (NSF) under Grant IIS-2046118. The work of Emily Mower Provost was supported by NSF under Grant RI-2006618. The work of Shrikanth S. Narayanan was supported by NSF under Grant IIS-2204942.(Chi-Chun Lee and Theodora Chaspari contributed equally to this work.)","","","8","9","","","IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","PISCATAWAY","445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA","","","","P IEEE","Proc. IEEE","OCT","2023","111","10","","","","","1142","1158","","10.1109/JPROC.2023.3276209","http://dx.doi.org/10.1109/JPROC.2023.3276209","","JUN 2023","17","","","","HJ5U5","","","","","2025-10-20","WOS:001019428100001","View Full Record in Web of Science"
"J","Hwang, SH; Yu, Y; Kim, J; Lee, T; Park, YR; Kim, HW","","","","Hwang, Sang Ho; Yu, Yeonsoo; Kim, Jichul; Lee, Taeyeop; Park, Yu Rang; Kim, Hyo-Won","","","A Study on the Screening of Children at Risk for Developmental Disabilities Using Facial Landmarks Derived From a Mobile-Based Application","PSYCHIATRY INVESTIGATION","","","English","Article","","","","","","","","Objective Early detection and intervention of developmental disabilities (DDs) are critical to improving the long-term outcomes of afflicted children. In this study, our objective was to utilize facial landmark features from mobile application to distinguish between children with DDs and typically developing (TD) children. Methods The present study recruited 89 children, including 33 diagnosed with DD, and 56 TD children. The aim was to examine the effectiveness of a deep learning classification model using facial video collected from children through mobile-based application. The study participants underwent comprehensive developmental assessments, which included the child completion of the Korean Psychoeducational Profile-Revised and caregiver completing the Korean versions of Vineland Adaptive Behavior Scale, Korean version of the Childhood Autism Rating Scale, Social Responsiveness Scale, and Child Behavior Checklist. We extracted facial landmarks from recorded videos using mobile application and performed DDs classification using long short-term memory with stratified 5-fold cross-validation. Results The classification model shows an average accuracy of 0.88 (range: 0.78-1.00), an average precision of 0.91 (range: 0.75-1.00), and an average F1-score of 0.80 (range: 0.60-1.00). Upon interpreting prediction results using SHapley Additive exPlanations (SHAP), we verified that the most crucial variable was the nodding head angle variable, with a median SHAP score of 2.6. All the top 10 contributing variables exhibited significant differences in distribution between children with DD and TD (p<0.05). Conclusion The results of this study provide evidence that facial landmarks, utilizing readily available mobile-based video data, can be used to detect DD at an early stage.","[Hwang, Sang Ho; Park, Yu Rang] Yonsei Univ, Coll Med, Dept Biomed Syst Informat, 50-1 Yonsei Ro, Seoul 03722, South Korea; [Yu, Yeonsoo] Univ Ulsan, Coll Med, Seoul, South Korea; [Kim, Jichul; Lee, Taeyeop; Kim, Hyo-Won] Univ Ulsan, Coll Med, Asan Med Ctr, Dept Psychiat, 88 Olymp Ro 43 Gil, Seoul 05505, South Korea","","Park, YR (corresponding author), Yonsei Univ, Coll Med, Dept Biomed Syst Informat, 50-1 Yonsei Ro, Seoul 03722, South Korea.;Kim, HW (corresponding author), Univ Ulsan, Coll Med, Asan Med Ctr, Dept Psychiat, 88 Olymp Ro 43 Gil, Seoul 05505, South Korea.","yurangpark@yuhs.ac; shingubi@amc.seoul.kr","","","National Research Foundation of Korea (NRF) - South Korean government (Ministry of Science and ICT) [NRF-2020R1A5A8017671]","National Research Foundation of Korea (NRF) - South Korean government (Ministry of Science and ICT)(National Research Foundation of Korea)","This work was supported by the National Research Foundation of Korea (NRF) grant funded by the South Korean government (Ministry of Science and ICT) (NRF-2020R1A5A8017671) .","","","0","0","","","KOREAN NEUROPSYCHIATRIC ASSOC","SEOUL","RN 522, G-FIVE CENTRAL PLAZA 1685-8 SEOCHO 4-DONG, SEOCHO-GU, SEOUL, 137-882, SOUTH KOREA","","","","PSYCHIAT INVEST","Psychiatry Investig.","MAY","2024","21","5","","","","","496","505","","10.30773/pi.2023.0315","http://dx.doi.org/10.30773/pi.2023.0315","","","10","","","","SB5M8","","Green Submitted, gold","","","2025-10-20","WOS:001232013000006","View Full Record in Web of Science"
